<!doctype html>
<html class="theme-next   use-motion ">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="google-site-verification" content="SxtUWZeCB1kdMgucSmFlZL9HwwJmgQHGcp9w_p4Xtgc" />
<meta name="baidu-site-verification" content="Sil1fR2zak" />



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"/>




  <link href="//fonts.googleapis.com/css?family=Lato:300,400,700,400italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">



<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=0.4.5.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Mxnet," />





  <link rel="alternate" href="/atom.xml" title="做最好的自己" type="application/atom+xml" />




      <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=0.4.5.2" />






<meta name="description" content="1.1.前言资料详见动手学深度学习">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习之Mxnet--李沐视频">
<meta property="og:url" content="http://zhangjin4415.github.io/2017/10/13/deeplearn/index.html">
<meta property="og:site_name" content="做最好的自己">
<meta property="og:description" content="1.1.前言资料详见动手学深度学习">
<meta property="og:image" content="http://zhangjin4415.github.io/img/deeplearn/img1.png">
<meta property="og:image" content="http://zhangjin4415.github.io/img/deeplearn/img2.png">
<meta property="og:image" content="http://zhangjin4415.github.io/img/deeplearn/img3.png">
<meta property="og:image" content="http://zhangjin4415.github.io/img/deeplearn/img4.png">
<meta property="og:image" content="http://zhangjin4415.github.io/img/deeplearn/img4.gif">
<meta property="og:image" content="http://zhangjin4415.github.io/img/deeplearn/img5.gif">
<meta property="og:image" content="http://zhangjin4415.github.io/../img/deeplearn/dropout.png">
<meta property="og:image" content="http://zhangjin4415.github.io/img/deeplearn/nin.svg">
<meta property="og:image" content="http://zhangjin4415.github.io/img/deeplearn/googlenet.png">
<meta property="og:image" content="http://zhangjin4415.github.io/img/deeplearn/inception.svg">
<meta property="og:image" content="http://zhangjin4415.github.io/img/deeplearn/residual.svg">
<meta property="og:image" content="http://zhangjin4415.github.io/img/deeplearn/residual2.svg">
<meta property="og:image" content="http://zhangjin4415.github.io/img/deeplearn/densenet.svg">
<meta property="og:image" content="http://zhangjin4415.github.io/img/deeplearn/img5.png">
<meta property="og:image" content="http://zhangjin4415.github.io/img/deeplearn/img6.png">
<meta property="og:image" content="http://zhangjin4415.github.io/img/deeplearn/img7.png">
<meta property="og:image" content="http://zhangjin4415.github.io/img/deeplearn/img8.png">
<meta property="og:image" content="http://zhangjin4415.github.io/img/deeplearn/img9.png">
<meta property="og:image" content="http://zhangjin4415.github.io/img/deeplearn/img10.png">
<meta property="og:image" content="http://zhangjin4415.github.io/img/deeplearn/img11.png">
<meta property="og:image" content="http://zhangjin4415.github.io/img/deeplearn/pikachu.png">
<meta property="og:updated_time" content="2018-02-25T11:58:23.530Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深度学习之Mxnet--李沐视频">
<meta name="twitter:description" content="1.1.前言资料详见动手学深度学习">



<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: 'Mist',
    sidebar: 'post',
    motion: true
  };
</script>

  <title> 深度学习之Mxnet--李沐视频 | 做最好的自己 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  <!--[if lte IE 8]>
  <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'>
    <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode">
      <img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820"
           alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari."
           style='margin-left:auto;margin-right:auto;display: block;'/>
    </a>
  </div>
<![endif]-->
  






  <div class="container one-column page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">做最好的自己</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu ">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-home fa-fw"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-th fa-fw"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-archive fa-fw"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-tags fa-fw"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-user fa-fw"></i> <br />
            
            关于
          </a>
        </li>
      

      
      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div id="content" class="content">
          

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                深度学习之Mxnet--李沐视频
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            发表于
            <time itemprop="dateCreated" datetime="2017-10-13T19:44:22+08:00" content="2017-10-13">
              2017-10-13
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp; 分类于
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2017/10/13/deeplearn/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2017/10/13/deeplearn/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

        </div>
      </header>
    


    <div class="post-body">

      
      

      
        <span itemprop="articleBody"><h2 id="1-1-_u524D_u8A00"><a href="#1-1-_u524D_u8A00" class="headerlink" title="1.1.前言"></a>1.1.前言</h2><p>资料详见<a href="https://zh.gluon.ai/" target="_blank" rel="external">动手学深度学习</a><br><a id="more"></a></p>
<h2 id="1-2-_u4F7F_u7528NDArray_u6765_u5904_u7406_u6570_u636E"><a href="#1-2-_u4F7F_u7528NDArray_u6765_u5904_u7406_u6570_u636E" class="headerlink" title="1.2.使用NDArray来处理数据"></a>1.2.使用NDArray来处理数据</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># coding=utf-<span class="number">8</span></span></span><br><span class="line"></span><br><span class="line">from mxnet import ndarray as nd</span><br><span class="line">import numpy as np</span><br><span class="line"><span class="preprocessor">##具体看NDArray API</span></span><br><span class="line"></span><br><span class="line">nd.zeros((<span class="number">3</span>, <span class="number">4</span>))  <span class="preprocessor">#<span class="number">3</span>行和<span class="number">4</span>列的<span class="number">2</span>D数组 全<span class="number">0</span></span></span><br><span class="line">x=nd.ones((<span class="number">3</span>,<span class="number">4</span>)) <span class="preprocessor">#全<span class="number">1</span></span></span><br><span class="line"><span class="preprocessor">#或者从python的数组直接构造</span></span><br><span class="line">nd.<span class="built_in">array</span>([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">3</span>]]) <span class="preprocessor">#[[ <span class="number">1.</span>  <span class="number">2.</span>] [ <span class="number">2.</span>  <span class="number">3.</span>]]</span></span><br><span class="line"><span class="preprocessor">## 创建随机数 - 深度学习常用</span></span><br><span class="line">y = nd.random_normal(<span class="number">0</span>, <span class="number">1</span>, shape=(<span class="number">3</span>, <span class="number">4</span>)) <span class="preprocessor">#均值<span class="number">0</span>方差<span class="number">1</span>的正态分布<span class="number">3</span>x4矩阵</span></span><br><span class="line">print y.shape <span class="preprocessor"># (<span class="number">3L</span>, <span class="number">4L</span>)</span></span><br><span class="line">print y.size <span class="preprocessor"># <span class="number">12</span></span></span><br><span class="line"></span><br><span class="line"><span class="preprocessor">##数学操作  nd.dot(x,y)为矩阵乘法 区别于*对应元素相乘</span></span><br><span class="line">x+y</span><br><span class="line">x*y</span><br><span class="line">print nd.<span class="built_in">exp</span>(y) <span class="preprocessor">##指数</span></span><br><span class="line">print nd.dot(x,y.T) <span class="preprocessor">#x与y的转置进行矩阵乘法</span></span><br><span class="line"></span><br><span class="line"><span class="preprocessor">##Numpy与NDArray转换</span></span><br><span class="line">x = np.ones((<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">y = nd.<span class="built_in">array</span>(x)  <span class="preprocessor"># numpy -&gt; mxnet</span></span><br><span class="line">z = y.asnumpy()  <span class="preprocessor"># mxnet -&gt; numpy</span></span><br><span class="line">print([z, y])</span><br></pre></td></tr></table></figure>
<h2 id="1-3-_u4F7F_u7528autograd_u81EA_u52A8_u6C42_u5BFC"><a href="#1-3-_u4F7F_u7528autograd_u81EA_u52A8_u6C42_u5BFC" class="headerlink" title="1.3.使用autograd自动求导"></a>1.3.使用autograd自动求导</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</span><br><span class="line"><span class="keyword">import</span> mxnet.autograd <span class="keyword">as</span> ag <span class="comment">##自动求导</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = nd.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="comment">#对需要求导的变量需要通过NDArray的方法`attach_grad()`来要求系统申请对应的空间</span></span><br><span class="line">x.attach_grad()</span><br><span class="line"></span><br><span class="line"><span class="comment">#默认条件下，MXNet不会自动记录和构建用于求导的计算图，</span></span><br><span class="line"><span class="comment"># 我们需要使用autograd里的`record()`函数来显式的要求MXNet记录我们需要求导的程序。</span></span><br><span class="line"><span class="keyword">with</span> ag.record():</span><br><span class="line">    y = x * <span class="number">2</span></span><br><span class="line">    z = y * x  <span class="comment">##z=2乘以x的平方 [[  2.   8.] [ 18.  32.]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#接下来我们可以通过z.backward()来进行求导。</span></span><br><span class="line"><span class="comment"># 如果z不是一个标量，那么z.backward()等价于nd.sum(z).backward()</span></span><br><span class="line">z.backward() <span class="comment">## None</span></span><br><span class="line"><span class="keyword">print</span> x.grad  <span class="comment"># z针对x求导的结果 [[  4.   8.] [ 12.  16.]]</span></span><br><span class="line"><span class="comment">## x.grad == 4*x</span></span><br><span class="line"><span class="keyword">print</span> x.grad == <span class="number">4</span>*x  <span class="comment">## [[ 1.  1.] [ 1.  1.]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 对控制流求导</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(a)</span>:</span></span><br><span class="line">    b = a * <span class="number">2</span></span><br><span class="line">    <span class="comment">## nd.norm(b)-b中所有数据的平方之和取根号 asscalar()--转换成标量</span></span><br><span class="line">    <span class="keyword">while</span> nd.norm(b).asscalar() &lt; <span class="number">1000</span>:</span><br><span class="line">        b = b * <span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> nd.sum(b).asscalar() &gt; <span class="number">0</span>:</span><br><span class="line">        c = b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        c = <span class="number">100</span> * b</span><br><span class="line">    <span class="keyword">return</span> c</span><br><span class="line"></span><br><span class="line">a = nd.random_normal(shape=<span class="number">3</span>)</span><br><span class="line">a.attach_grad()</span><br><span class="line"><span class="keyword">with</span> ag.record():</span><br><span class="line">    c = f(a)</span><br><span class="line">c.backward()</span><br><span class="line"><span class="keyword">print</span> a.grad <span class="comment"># [ 512.  512.  512.]</span></span><br><span class="line"><span class="keyword">print</span> a.grad==c/a <span class="comment"># [ 1.  1.  1.]</span></span><br></pre></td></tr></table></figure>
<h2 id="1-4-_u4ECE0_u5F00_u59CB_u7EBF_u6027_u56DE_u5F52"><a href="#1-4-_u4ECE0_u5F00_u59CB_u7EBF_u6027_u56DE_u5F52" class="headerlink" title="1.4.从0开始线性回归"></a>1.4.从0开始线性回归</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">num_inputs = <span class="number">2</span></span><br><span class="line">num_examples = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">true_w = [<span class="number">2</span>, -<span class="number">3.4</span>]</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line"></span><br><span class="line">X = nd.random_normal(shape=(num_examples, num_inputs))</span><br><span class="line">y = true_w[<span class="number">0</span>] * X[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * X[:, <span class="number">1</span>] + true_b</span><br><span class="line">y += <span class="number">.01</span> * nd.random_normal(shape=y.shape) <span class="comment">##加噪声</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意到`X`的每一行是一个长度为2的向量，而`y`的每一行是一个长度为1的向量（标量）。</span></span><br><span class="line"></span><br><span class="line">print(X[<span class="number">0</span>], y[<span class="number">0</span>]) <span class="comment"># [[ 2.21220636  1.16307867],[ 4.6620779]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 当我们开始训练神经网络的时候，我们需要不断读取数据块。</span></span><br><span class="line"><span class="comment"># 这里我们定义一个函数它每次返回batch_size个随机的样本和对应的目标。</span></span><br><span class="line"><span class="comment"># 我们通过python的yield来构造一个迭代器。</span></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 产生一个随机索引</span></span><br><span class="line">    idx = list(range(num_examples))</span><br><span class="line">    random.shuffle(idx) <span class="comment">##索引打乱顺序</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        j = nd.array(idx[i:min(i+batch_size,num_examples)]) <span class="comment">#随机的batch个索引</span></span><br><span class="line">        <span class="keyword">yield</span> nd.take(X, j), nd.take(y, j) <span class="comment">#根据索引拿到数据</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data, label <span class="keyword">in</span> data_iter():</span><br><span class="line">    print(data, label)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">n=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> data, label <span class="keyword">in</span> data_iter():</span><br><span class="line">    n=n+<span class="number">1</span></span><br><span class="line"><span class="keyword">print</span> n <span class="comment">## 100 每次拿10组数据 拿100次拿完</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#下面我们随机初始化模型参数</span></span><br><span class="line">w = nd.random_normal(shape=(num_inputs, <span class="number">1</span>))</span><br><span class="line">b = nd.zeros((<span class="number">1</span>,))</span><br><span class="line">params = [w, b]</span><br><span class="line"><span class="keyword">print</span> params <span class="comment"># [[ 0.72455114] [ 0.13263007]], [ 0.]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 之后训练时我们需要对这些参数求导来更新它们的值，所以我们需要创建它们的梯度</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">    param.attach_grad()</span><br><span class="line"></span><br><span class="line"><span class="comment">## 定义模型</span></span><br><span class="line"><span class="comment"># 线性模型就是将输入和模型做乘法再加上偏移：</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> nd.dot(X, w) + b</span><br><span class="line"></span><br><span class="line"><span class="comment">## 损失函数</span></span><br><span class="line"><span class="comment">#我们使用常见的平方误差来衡量预测目标和真实目标之间的差距。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">square_loss</span><span class="params">(yhat, y)</span>:</span></span><br><span class="line">    <span class="comment"># 注意这里我们把y变形成yhat的形状来避免自动广播</span></span><br><span class="line">    <span class="keyword">return</span> (yhat - y.reshape(yhat.shape)) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#虽然线性回归有显试解，但绝大部分模型并没有。</span></span><br><span class="line"><span class="comment"># 所以我们这里通过随机梯度下降来求解。每一步，</span></span><br><span class="line"><span class="comment"># 我们将模型参数沿着梯度的反方向走特定距离，这个距离一般叫学习率。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(params, lr)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param[:] = param - lr * param.grad</span><br><span class="line"></span><br><span class="line"><span class="comment">## 训练</span></span><br><span class="line"><span class="comment"># 现在我们可以开始训练了。训练通常需要迭代数据数次，</span></span><br><span class="line"><span class="comment"># 一次迭代里，我们每次随机读取固定数个数据点，计算梯度并更新模型参数。</span></span><br><span class="line">epochs = <span class="number">10</span></span><br><span class="line">learning_rate = <span class="number">.001</span></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> range(epochs):</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> data_iter():</span><br><span class="line">        <span class="keyword">with</span> autograd.record():</span><br><span class="line">            output = net(data)</span><br><span class="line">            loss = square_loss(output, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        SGD(params, learning_rate)</span><br><span class="line">        total_loss += nd.sum(loss).asscalar()</span><br><span class="line">    print(<span class="string">"Epoch %d, average loss: %f"</span> % (e, total_loss/num_examples))</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> true_w,w  <span class="comment">#[2, -3.4]  [[ 1.99984801] [-3.40017033]]</span></span><br><span class="line"><span class="keyword">print</span> true_b,b  <span class="comment">#4.2  [ 4.19996023]</span></span><br></pre></td></tr></table></figure>
<h2 id="1-5-_u4F7F_u7528Gluon_u5B9E_u73B0_u7EBF_u6027_u56DE_u5F52"><a href="#1-5-_u4F7F_u7528Gluon_u5B9E_u73B0_u7EBF_u6027_u56DE_u5F52" class="headerlink" title="1.5.使用Gluon实现线性回归"></a>1.5.使用Gluon实现线性回归</h2><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line">from mxnet import ndarray as nd</span><br><span class="line">from mxnet import autograd</span><br><span class="line">from mxnet import gluon</span><br><span class="line"></span><br><span class="line"><span class="preprocessor">## 创建数据集</span></span><br><span class="line">num_inputs = <span class="number">2</span></span><br><span class="line">num_examples = <span class="number">1000</span></span><br><span class="line">true_w = [<span class="number">2</span>, -<span class="number">3.4</span>]</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line"></span><br><span class="line">X = nd.random_normal(shape=(num_examples, num_inputs))</span><br><span class="line">y = true_w[<span class="number">0</span>] * X[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * X[:, <span class="number">1</span>] + true_b</span><br><span class="line">y += <span class="number">.01</span> * nd.random_normal(shape=y.shape)</span><br><span class="line"></span><br><span class="line"><span class="preprocessor">## 数据读取</span></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">dataset = gluon.data.ArrayDataset(X, y)</span><br><span class="line">data_iter = gluon.data.DataLoader(dataset, batch_size, shuffle=True)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data, label in data_iter:</span><br><span class="line">    print(data, label)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="preprocessor">## 定义模型</span></span><br><span class="line"><span class="preprocessor"># 当我们手写模型的时候，我们需要先声明模型参数，然后再使用它们来构建模型。</span></span><br><span class="line"><span class="preprocessor"># 但gluon提供大量提前定制好的层，使得我们只需要主要关注使用哪些层来构建模型。</span></span><br><span class="line"><span class="preprocessor"># 例如线性模型就是使用对应的Dense层。</span></span><br><span class="line"><span class="preprocessor">#虽然我们之后会介绍如何构造任意结构的神经网络，</span></span><br><span class="line"><span class="preprocessor"># 构建模型最简单的办法是利用Sequential来所有层串起来。</span></span><br><span class="line"><span class="preprocessor"># 首先我们定义一个空的模型：</span></span><br><span class="line">net = gluon.nn.Sequential()</span><br><span class="line"></span><br><span class="line"><span class="preprocessor">#然后我们加入一个Dense层，它唯一必须要定义的参数就是输出节点的个数，在线性模型里面是1.</span></span><br><span class="line"><span class="preprocessor">#这里我们并没有定义说这个层的输入节点是多少，这个在之后真正给数据的时候系统会自动赋值。</span></span><br><span class="line">net.add(gluon.nn.Dense(<span class="number">1</span>))</span><br><span class="line">print net  #Sequential((<span class="number">0</span>): Dense(<span class="number">1</span>, linear))</span><br><span class="line"></span><br><span class="line"><span class="preprocessor"># 在使用前net我们必须要初始化模型权重，这里我们使用默认随机初始化方法</span></span><br><span class="line"><span class="preprocessor"># （之后我们会介绍更多的初始化方法）。</span></span><br><span class="line">net.initialize()</span><br><span class="line"></span><br><span class="line"><span class="preprocessor">## 损失函数</span></span><br><span class="line"><span class="preprocessor"># gluon提供了平方误差函数：</span></span><br><span class="line">square_loss = gluon.loss.L2Loss()</span><br><span class="line"></span><br><span class="line"><span class="preprocessor">## 优化</span></span><br><span class="line"><span class="preprocessor">#同样我们无需手动实现随机梯度下降，我们可以用创建一个Trainer的实例，</span></span><br><span class="line"><span class="preprocessor"># 并且将模型参数传递给它就行。</span></span><br><span class="line">trainer = gluon.Trainer(</span><br><span class="line">    net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.1</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="preprocessor">## 训练</span></span><br><span class="line"><span class="preprocessor"># 这里的训练跟前面没有太多区别，唯一的就是我们不再是调用SGD，</span></span><br><span class="line"><span class="preprocessor"># 而是trainer.step来更新模型。</span></span><br><span class="line">epochs = <span class="number">5</span></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"><span class="keyword">for</span> e in range(epochs):</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> data, label in data_iter:</span><br><span class="line">        with autograd.record():</span><br><span class="line">            output = net(data)</span><br><span class="line">            loss = square_loss(output, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        trainer.step(batch_size) ##往前走一步</span><br><span class="line">        total_loss += nd.sum(loss).asscalar()</span><br><span class="line">    print(<span class="string">"Epoch %d, average loss: %f"</span> % (e, total_loss/num_examples))</span><br><span class="line"></span><br><span class="line"><span class="preprocessor"># 比较学到的和真实模型。我们先从`net`拿到需要的层，然后访问其权重和位移。</span></span><br><span class="line">dense = net[<span class="number">0</span>]</span><br><span class="line">print true_w, dense.weight.data() ##[<span class="number">2</span>, -<span class="number">3.4</span>]  [[ <span class="number">1.99912584</span> -<span class="number">3.39986587</span>]]</span><br><span class="line">print true_b, dense.bias.data() ##<span class="number">4.2</span>  [ <span class="number">4.19950867</span>]</span><br></pre></td></tr></table></figure>
<h2 id="1-6-_u4ECE0_u5F00_u59CB_u591A_u7C7B_u903B_u8F91_u56DE_u5F52"><a href="#1-6-_u4ECE0_u5F00_u59CB_u591A_u7C7B_u903B_u8F91_u56DE_u5F52" class="headerlink" title="1.6.从0开始多类逻辑回归"></a>1.6.从0开始多类逻辑回归</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</span><br><span class="line"></span><br><span class="line"><span class="comment">#这里我们用MNIST分类数字</span></span><br><span class="line"><span class="comment"># 我们通过gluon的data.vision模块自动下载这个数据。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(data, label)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> data.astype(<span class="string">'float32'</span>)/<span class="number">255</span>, label.astype(<span class="string">'float32'</span>)</span><br><span class="line">mnist_train = gluon.data.vision.MNIST(train=<span class="keyword">True</span>, transform=transform)</span><br><span class="line">mnist_test = gluon.data.vision.MNIST(train=<span class="keyword">False</span>, transform=transform)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印一个样本的形状和它的标号</span></span><br><span class="line">data, label = mnist_train[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'example shape: '</span>, data.shape, <span class="string">'label:'</span>, label) <span class="comment">##('example shape: ', (28L, 28L, 1L), 'label:', 5.0)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 数据读取</span></span><br><span class="line"><span class="comment">#虽然我们可以像前面那样通过yield来定义获取批量数据函数，</span></span><br><span class="line"><span class="comment"># 这里我们直接使用gluon.data的DataLoader函数，它每次yield一个批量。</span></span><br><span class="line"><span class="comment"># 注意到这里我们要求每次从训练数据里读取一个由随机样本组成的批量，</span></span><br><span class="line"><span class="comment"># 但测试数据则不需要这个要求。</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_data = gluon.data.DataLoader(mnist_train, batch_size, shuffle=<span class="keyword">True</span>)</span><br><span class="line">test_data = gluon.data.DataLoader(mnist_test, batch_size, shuffle=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 初始化模型参数</span></span><br><span class="line"><span class="comment"># 跟线性模型一样，每个样本会表示成一个向量。我们这里数据是 28 * 28 大小的图片，</span></span><br><span class="line"><span class="comment"># 所以输入向量的长度是 28 * 28 = 784。</span></span><br><span class="line"><span class="comment"># 因为我们要做多类分类，我们需要对每一个类预测这个样本属于此类的概率。</span></span><br><span class="line"><span class="comment"># 因为这个数据集有10个类型，所以输出应该是长为10的向量。</span></span><br><span class="line"><span class="comment"># 这样，我们需要的权重将是一个 784 * 10 的矩阵：</span></span><br><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line">W = nd.random_normal(shape=(num_inputs, num_outputs))</span><br><span class="line">b = nd.random_normal(shape=num_outputs)</span><br><span class="line">params = [W, b]</span><br><span class="line"></span><br><span class="line"><span class="comment">#同之前一样，我们要对模型参数附上梯度：</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">    param.attach_grad()</span><br><span class="line"></span><br><span class="line"><span class="comment">## 定义模型</span></span><br><span class="line"><span class="comment"># 在线性回归教程里，我们只需要输出一个标量yhat使得尽可能的靠近目标值。</span></span><br><span class="line"><span class="comment"># 但在这里的分类里，我们需要属于每个类别的概率。</span></span><br><span class="line"><span class="comment"># 这些概率需要值为正，而且加起来等于1.</span></span><br><span class="line"><span class="comment"># 而如果简单的使用 Y=WX,我们不能保证这一点。</span></span><br><span class="line"><span class="comment"># 一个通常的做法是通过softmax函数来将任意的输入归一化成合法的概率值。</span></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(X)</span>:</span></span><br><span class="line">    exp = nd.exp(X)  <span class="comment">##全部变为正的</span></span><br><span class="line">    <span class="comment"># 假设exp是矩阵，这里对行进行求和，并要求保留axis 1，</span></span><br><span class="line">    <span class="comment"># 就是返回 (nrows, 1) 形状的矩阵</span></span><br><span class="line">    partition = exp.sum(axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> exp / partition  <span class="comment">##每个行除以它的行的和</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#可以看到，对于随机输入，我们将每个元素变成了非负数，而且每一行加起来为1。</span></span><br><span class="line">X = nd.random_normal(shape=(<span class="number">2</span>,<span class="number">5</span>))</span><br><span class="line"><span class="keyword">print</span> X</span><br><span class="line"><span class="comment"># [[ 0.79687113  0.85240501  0.61860603  0.47654876  0.74863517]</span></span><br><span class="line"><span class="comment"># [ 0.55032933 -0.22566749 -2.11320662 -0.95748073  0.32560727]]</span></span><br><span class="line">X_prob = softmax(X)</span><br><span class="line">print(X_prob)</span><br><span class="line"><span class="comment"># [[ 0.21869159  0.23117994  0.18298376  0.1587515   0.20839317]</span></span><br><span class="line"><span class="comment"># [ 0.39214477  0.1804826   0.02733301  0.08681861  0.31322101]]</span></span><br><span class="line">print(X_prob.sum(axis=<span class="number">1</span>)) <span class="comment"># [ 0.99999994  1.        ] ([1,1])</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#现在我们可以定义模型了：</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> softmax(nd.dot(X.reshape((-<span class="number">1</span>,num_inputs)), W) + b)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 交叉熵损失函数</span></span><br><span class="line"><span class="comment"># 我们需要定义一个针对预测为概率值的损失函数。其中最常见的是交叉熵损失函数，</span></span><br><span class="line"><span class="comment"># 它将两个概率分布的负交叉熵作为目标值，最小化这个值等价于最大化这两个概率的相似度。</span></span><br><span class="line"><span class="comment"># 具体来说，我们先将真实标号表示成一个概率分布，例如如果y=1，</span></span><br><span class="line"><span class="comment"># 那么其对应的分布就是一个除了第二个元素为1其他全为0的长为10的向量，</span></span><br><span class="line"><span class="comment"># 也就是 yvec=[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]。</span></span><br><span class="line"><span class="comment"># 那么交叉熵就是yvec[0]*log(yhat[0])+...+yvec[n]*log(yhat[n])。</span></span><br><span class="line"><span class="comment"># 注意到yvec里面只有一个1，那么前面等价于log(yhat[y])。</span></span><br><span class="line"><span class="comment"># 所以我们可以定义这个损失函数了</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy</span><span class="params">(yhat, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> - nd.pick(nd.log(yhat), y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#给定一个概率输出，我们将预测概率最高的那个类作为预测的类，</span></span><br><span class="line"><span class="comment"># 然后通过比较真实标号我们可以计算精度：</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(output, label)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> nd.mean(output.argmax(axis=<span class="number">1</span>)==label).asscalar()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#我们可以评估一个模型在这个数据上的精度。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span><span class="params">(data_iterator, net)</span>:</span></span><br><span class="line">    acc = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> data_iterator:</span><br><span class="line">        output = net(data)</span><br><span class="line">        acc += accuracy(output, label)</span><br><span class="line">    <span class="keyword">return</span> acc / len(data_iterator)</span><br><span class="line"></span><br><span class="line"><span class="comment">#因为我们随机初始化了模型，所以这个模型的精度应该大概是1/num_outputs = 0.1.</span></span><br><span class="line"><span class="keyword">print</span> evaluate_accuracy(test_data, net) <span class="comment">#0.09814453125</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(params, lr)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param[:] = param - lr * param.grad</span><br><span class="line"></span><br><span class="line"><span class="comment">## 训练</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">'..'</span>)</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">.1</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    train_loss = <span class="number">0.</span></span><br><span class="line">    train_acc = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_data:</span><br><span class="line">        <span class="keyword">with</span> autograd.record():</span><br><span class="line">            output = net(data)</span><br><span class="line">            loss = cross_entropy(output, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># 将梯度做平均，这样学习率会对batch size不那么敏感</span></span><br><span class="line">        SGD(params, learning_rate/batch_size)</span><br><span class="line"></span><br><span class="line">        train_loss += nd.mean(loss).asscalar()</span><br><span class="line">        train_acc += accuracy(output, label)</span><br><span class="line"></span><br><span class="line">    test_acc = evaluate_accuracy(test_data, net)</span><br><span class="line">    print(<span class="string">"Epoch %d. Loss: %f, Train acc %f, Test acc %f"</span> % (</span><br><span class="line">        epoch, train_loss/len(train_data), train_acc/len(train_data), test_acc))</span><br><span class="line"></span><br><span class="line"><span class="comment">## 预测</span></span><br><span class="line"><span class="comment">#训练完成后，现在我们可以演示对输入图片的标号的预测</span></span><br><span class="line"></span><br><span class="line">data, label = mnist_test[<span class="number">0</span>:<span class="number">9</span>]</span><br><span class="line">print(<span class="string">'true labels'</span>)</span><br><span class="line">print(label)</span><br><span class="line"></span><br><span class="line">predicted_labels = net(data).argmax(axis=<span class="number">1</span>)</span><br><span class="line">print(<span class="string">'predicted labels'</span>)</span><br><span class="line">print(predicted_labels.asnumpy())</span><br></pre></td></tr></table></figure>
<p>尝试增大学习率，你会发现结果马上回变成很糟糕，精度基本徘徊在随机的0.1左右。这是为什么呢？提示：</p>
<ul>
<li>打印下output看看是不是有有什么异常</li>
<li>前面线性回归还好好的，这里我们在net()里加了什么呢？</li>
<li>如果给exp输入个很大的数会怎么样？</li>
<li>即使解决exp的问题，求出来的导数是不是还是不稳定？</li>
</ul>
<p>请仔细想想再去对比下小伙伴之一@<a href="https://github.com/pluskid" target="_blank" rel="external">pluskid</a>早年写的一篇<a href="http://freemind.pluskid.org/machine-learning/softmax-vs-softmax-loss-numerical-stability/" target="_blank" rel="external">blog解释这个问题</a>，看看你想的是不是不一样。</p>
<h2 id="1-7-Gluon_u7248_u591A_u7C7B_u903B_u8F91_u56DE_u5F52"><a href="#1-7-Gluon_u7248_u591A_u7C7B_u903B_u8F91_u56DE_u5F52" class="headerlink" title="1.7.Gluon版多类逻辑回归"></a>1.7.Gluon版多类逻辑回归</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> image</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"><span class="comment">#这里我们用MNIST分类数字</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_fashion_mnist</span><span class="params">(batch_size, resize=None)</span>:</span></span><br><span class="line">    <span class="string">"""download the fashion mnist dataest and then load into memory"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform_mnist</span><span class="params">(data, label)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> resize:</span><br><span class="line">            <span class="comment"># resize to resize x resize</span></span><br><span class="line">            data = image.imresize(data, resize, resize)</span><br><span class="line">        <span class="comment"># change data from height x weight x channel to channel x height x weight</span></span><br><span class="line">        <span class="keyword">return</span> nd.transpose(data.astype(<span class="string">'float32'</span>), (<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>))/<span class="number">255</span>, label.astype(<span class="string">'float32'</span>)</span><br><span class="line">    mnist_train = gluon.data.vision.MNIST(train=<span class="keyword">True</span>, transform=transform_mnist)</span><br><span class="line">    mnist_test = gluon.data.vision.MNIST(train=<span class="keyword">False</span>, transform=transform_mnist)</span><br><span class="line">    train_data = gluon.data.DataLoader(mnist_train, batch_size, shuffle=<span class="keyword">True</span>)</span><br><span class="line">    test_data = gluon.data.DataLoader(mnist_test, batch_size, shuffle=<span class="keyword">False</span>)</span><br><span class="line">    <span class="keyword">return</span> (train_data, test_data)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(data, label)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> data.astype(<span class="string">'float32'</span>)/<span class="number">255</span>, label.astype(<span class="string">'float32'</span>)</span><br><span class="line">mnist_train,mnist_test  = load_data_fashion_mnist(batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们先使用Flatten层将输入数据转成batch_size的矩阵?</span></span><br><span class="line"><span class="comment"># 然后输入到10个输出节点的全连接层。</span></span><br><span class="line"><span class="comment"># 照例我们不需要制定每层输入的大小，gluon会做自动推导。</span></span><br><span class="line">net = gluon.nn.Sequential()</span><br><span class="line"><span class="keyword">with</span> net.name_scope():</span><br><span class="line">    net.add(gluon.nn.Flatten())</span><br><span class="line">    net.add(gluon.nn.Dense(<span class="number">10</span>))</span><br><span class="line">net.initialize()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后通过比较真实标号我们可以计算精度：</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(output, label)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> nd.mean(output.argmax(axis=<span class="number">1</span>)==label).asscalar()</span><br><span class="line"></span><br><span class="line"><span class="comment">#我们可以评估一个模型在这个数据上的精度。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span><span class="params">(data_iterator, net)</span>:</span></span><br><span class="line">    acc = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> data_iterator:</span><br><span class="line">        output = net(data)</span><br><span class="line">        acc += accuracy(output, label)</span><br><span class="line">    <span class="keyword">return</span> acc / len(data_iterator)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Softmax和交叉熵损失函数</span></span><br><span class="line"><span class="comment">#如果你做了上一章的练习，那么你可能意识到了分开定义Softmax和交叉熵会有数值不稳定性</span></span><br><span class="line">softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment">## 优化</span></span><br><span class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.1</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    train_loss = <span class="number">0.</span></span><br><span class="line">    train_acc = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> mnist_train:</span><br><span class="line">        <span class="keyword">with</span> autograd.record():</span><br><span class="line">            output = net(data)</span><br><span class="line">            loss = softmax_cross_entropy(output, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        trainer.step(batch_size)</span><br><span class="line"></span><br><span class="line">        train_loss += nd.mean(loss).asscalar()</span><br><span class="line">        train_acc += accuracy(output, label)</span><br><span class="line">    test_acc = evaluate_accuracy(mnist_test, net)</span><br><span class="line">    print(<span class="string">"Epoch %d. Loss: %f, Train acc %f, Test acc %f"</span> % (</span><br><span class="line">        epoch, train_loss/len(mnist_train), train_acc/len(mnist_train), test_acc))</span><br></pre></td></tr></table></figure>
<h2 id="2-1-_u4ECE0_u5F00_u59CB_u591A_u5C42_u611F_u77E5_u673A"><a href="#2-1-_u4ECE0_u5F00_u59CB_u591A_u5C42_u611F_u77E5_u673A" class="headerlink" title="2.1.从0开始多层感知机"></a>2.1.从0开始多层感知机</h2><p>前面我们介绍了包括线性回归和多类逻辑回归的数个模型，它们的一个共同点是全是只含有一个输入层，一个输出层。这一节我们将介绍多层神经网络，就是包含至少一个隐含层的网络。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> image</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#这里我们用MNIST分类数字</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_fashion_mnist</span><span class="params">(batch_size, resize=None)</span>:</span></span><br><span class="line">    <span class="string">"""download the fashion mnist dataest and then load into memory"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform_mnist</span><span class="params">(data, label)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> resize:</span><br><span class="line">            <span class="comment"># resize to resize x resize</span></span><br><span class="line">            data = image.imresize(data, resize, resize)</span><br><span class="line">        <span class="comment"># change data from height x weight x channel to channel x height x weight</span></span><br><span class="line">        <span class="keyword">return</span> nd.transpose(data.astype(<span class="string">'float32'</span>), (<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>))/<span class="number">255</span>, label.astype(<span class="string">'float32'</span>)</span><br><span class="line">    mnist_train = gluon.data.vision.MNIST(train=<span class="keyword">True</span>, transform=transform_mnist)</span><br><span class="line">    mnist_test = gluon.data.vision.MNIST(train=<span class="keyword">False</span>, transform=transform_mnist)</span><br><span class="line">    train_data = gluon.data.DataLoader(mnist_train, batch_size, shuffle=<span class="keyword">True</span>)</span><br><span class="line">    test_data = gluon.data.DataLoader(mnist_test, batch_size, shuffle=<span class="keyword">False</span>)</span><br><span class="line">    <span class="keyword">return</span> (train_data, test_data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_data, test_data = load_data_fashion_mnist(batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里我们定义一个只有一个隐含层的模型，这个隐含层输出256个节点。</span></span><br><span class="line">num_inputs = <span class="number">28</span>*<span class="number">28</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line">num_hidden = <span class="number">256</span></span><br><span class="line">weight_scale = <span class="number">.01</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##定义两层</span></span><br><span class="line">W1 = nd.random_normal(shape=(num_inputs, num_hidden), scale=weight_scale)</span><br><span class="line">b1 = nd.zeros(num_hidden)</span><br><span class="line">W2 = nd.random_normal(shape=(num_hidden, num_outputs), scale=weight_scale)</span><br><span class="line">b2 = nd.zeros(num_outputs)</span><br><span class="line"></span><br><span class="line">params = [W1, b1, W2, b2]</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">    param.attach_grad()</span><br><span class="line"></span><br><span class="line"><span class="comment">## 激活函数</span></span><br><span class="line"><span class="comment">#如果我们就用线性操作符来构造多层神经网络，那么整个模型仍然只是一个线性函数。</span></span><br><span class="line"><span class="comment"># 为了让我们的模型可以拟合非线性函数，我们需要在层之间插入非线性的激活函数。这里我们使用ReLU</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> nd.maximum(X, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 定义模型</span></span><br><span class="line"><span class="comment">#我们的模型就是将层（全连接）和激活函数（Relu）串起来：</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X)</span>:</span></span><br><span class="line">    X = X.reshape((-<span class="number">1</span>, num_inputs))</span><br><span class="line">    h = relu(nd.dot(X, W1) + b1)</span><br><span class="line">    output = nd.dot(h, W2) + b2</span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment">## Softmax和交叉熵损失函数</span></span><br><span class="line"><span class="comment">#在多类Logistic回归里我们提到分开实现Softmax和交叉熵损失函数可能导致数值不稳定。</span></span><br><span class="line"><span class="comment"># 这里我们直接使用Gluon提供的函数</span></span><br><span class="line">softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后通过比较真实标号我们可以计算精度：</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(output, label)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> nd.mean(output.argmax(axis=<span class="number">1</span>)==label).asscalar()</span><br><span class="line"></span><br><span class="line"><span class="comment">#我们可以评估一个模型在这个数据上的精度。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span><span class="params">(data_iterator, net)</span>:</span></span><br><span class="line">    acc = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> data_iterator:</span><br><span class="line">        output = net(data)</span><br><span class="line">        acc += accuracy(output, label)</span><br><span class="line">    <span class="keyword">return</span> acc / len(data_iterator)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(params, lr)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param[:] = param - lr * param.grad</span><br><span class="line"></span><br><span class="line"><span class="comment">## 训练</span></span><br><span class="line">learning_rate = <span class="number">.5</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    train_loss = <span class="number">0.</span></span><br><span class="line">    train_acc = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_data:</span><br><span class="line">        <span class="keyword">with</span> autograd.record():</span><br><span class="line">            output = net(data)</span><br><span class="line">            loss = softmax_cross_entropy(output, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        SGD(params, learning_rate/batch_size)</span><br><span class="line"></span><br><span class="line">        train_loss += nd.mean(loss).asscalar()</span><br><span class="line">        train_acc += accuracy(output, label)</span><br><span class="line"></span><br><span class="line">    test_acc = evaluate_accuracy(test_data, net)</span><br><span class="line">    print(<span class="string">"Epoch %d. Loss: %f, Train acc %f, Test acc %f"</span> % (</span><br><span class="line">        epoch, train_loss/len(train_data),</span><br><span class="line">        train_acc/len(train_data), test_acc))</span><br></pre></td></tr></table></figure></p>
<p>可以看到，加入一个隐含层后我们将精度提升了不少。</p>
<p> 练习</p>
<ul>
<li>我们使用了 <code>weight_scale</code> 来控制权重的初始化值大小，增大或者变小这个值会怎么样？</li>
<li>尝试改变 <code>num_hiddens</code> 来控制模型的复杂度</li>
<li>尝试加入一个新的隐含层<br>注意：针对不同的数据，模型可能并非越复杂越好<br>测试结果：加大num_hiddens得到了更好的结果，添加隐藏层后更加容易过拟合</li>
</ul>
<h2 id="2-2-_u4F7F_u7528Gluon_u591A_u5C42_u611F_u77E5_u673A"><a href="#2-2-_u4F7F_u7528Gluon_u591A_u5C42_u611F_u77E5_u673A" class="headerlink" title="2.2.使用Gluon多层感知机"></a>2.2.使用Gluon多层感知机</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> image</span><br><span class="line"></span><br><span class="line"><span class="comment">#这里我们用MNIST分类数字</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_fashion_mnist</span><span class="params">(batch_size, resize=None)</span>:</span></span><br><span class="line">    <span class="string">"""download the fashion mnist dataest and then load into memory"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform_mnist</span><span class="params">(data, label)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> resize:</span><br><span class="line">            <span class="comment"># resize to resize x resize</span></span><br><span class="line">            data = image.imresize(data, resize, resize)</span><br><span class="line">        <span class="comment"># change data from height x weight x channel to channel x height x weight</span></span><br><span class="line">        <span class="keyword">return</span> nd.transpose(data.astype(<span class="string">'float32'</span>), (<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>))/<span class="number">255</span>, label.astype(<span class="string">'float32'</span>)</span><br><span class="line">    mnist_train = gluon.data.vision.MNIST(train=<span class="keyword">True</span>, transform=transform_mnist)</span><br><span class="line">    mnist_test = gluon.data.vision.MNIST(train=<span class="keyword">False</span>, transform=transform_mnist)</span><br><span class="line">    train_data = gluon.data.DataLoader(mnist_train, batch_size, shuffle=<span class="keyword">True</span>)</span><br><span class="line">    test_data = gluon.data.DataLoader(mnist_test, batch_size, shuffle=<span class="keyword">False</span>)</span><br><span class="line">    <span class="keyword">return</span> (train_data, test_data)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(output, label)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> nd.mean(output.argmax(axis=<span class="number">1</span>)==label).asscalar()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span><span class="params">(data_iterator, net)</span>:</span></span><br><span class="line">    acc = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> data_iterator:</span><br><span class="line">        output = net(data)</span><br><span class="line">        acc += accuracy(output, label)</span><br><span class="line">    <span class="keyword">return</span> acc / len(data_iterator)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 定义模型</span></span><br><span class="line"><span class="comment">#唯一的区别在这里，我们加了一行进来。</span></span><br><span class="line">net = gluon.nn.Sequential()</span><br><span class="line"><span class="keyword">with</span> net.name_scope():</span><br><span class="line">    net.add(gluon.nn.Flatten())</span><br><span class="line">    net.add(gluon.nn.Dense(<span class="number">256</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">    net.add(gluon.nn.Dense(<span class="number">10</span>))</span><br><span class="line">net.initialize()</span><br><span class="line"></span><br><span class="line"><span class="comment">## 读取数据并训练</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_data, test_data = load_data_fashion_mnist(batch_size)</span><br><span class="line"></span><br><span class="line">softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()</span><br><span class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.5</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    train_loss = <span class="number">0.</span></span><br><span class="line">    train_acc = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_data:</span><br><span class="line">        <span class="keyword">with</span> autograd.record():</span><br><span class="line">            output = net(data)</span><br><span class="line">            loss = softmax_cross_entropy(output, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        trainer.step(batch_size)</span><br><span class="line"></span><br><span class="line">        train_loss += nd.mean(loss).asscalar()</span><br><span class="line">        train_acc += accuracy(output, label)</span><br><span class="line"></span><br><span class="line">    test_acc = evaluate_accuracy(test_data, net)</span><br><span class="line">    print(<span class="string">"Epoch %d. Loss: %f, Train acc %f, Test acc %f"</span> % (</span><br><span class="line">        epoch, train_loss/len(train_data), train_acc/len(train_data), test_acc))</span><br></pre></td></tr></table></figure>
<p>通过Gluon我们可以更方便地构造多层神经网络。<br> 练习</p>
<ul>
<li>尝试多加入几个隐含层，对比从0开始的实现。</li>
<li>尝试使用一个另外的激活函数，可以使用<code>help(nd.Activation)</code>或者<a href="https://mxnet.apache.org/api/python/ndarray.html#mxnet.ndarray.Activation" target="_blank" rel="external">线上文档</a>查看提供的选项。</li>
</ul>
<h2 id="2-3-_u4ECE0_u5F00_u59CB_u6B63_u5219_u5316"><a href="#2-3-_u4ECE0_u5F00_u59CB_u6B63_u5219_u5316" class="headerlink" title="2.3.从0开始正则化"></a>2.3.从0开始正则化</h2><p>本章从0开始介绍如何的正则化来应对过拟合问题。<br>L2范数正则化<br>这里我们引入$L_2$范数正则化。不同于在训练时仅仅最小化损失函数(Loss)，我们在训练时其实在最小化<br><img src="/img/deeplearn/img1.png" alt="img1"><br>直观上，L2范数正则化试图惩罚较大绝对值的参数值。训练模型时，如果λ=0则没有正则化，需要注意的是，测试模型时，λ必须为0。</p>
<p>高维线性回归<br>我们使用高维线性回归为例来引入一个过拟合问题。<br><img src="/img/deeplearn/img2.png" alt="img2"><br>需要注意的是，我们用以上相同的数据生成函数来生成训练数据集和测试数据集。为了观察过拟合，我们特意把训练数据样本数设低，例如n=20，同时把维度升高，例如p=200.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line"></span><br><span class="line">num_train = <span class="number">20</span></span><br><span class="line">num_test = <span class="number">100</span></span><br><span class="line">num_inputs = <span class="number">200</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 生成数据集</span></span><br><span class="line"><span class="comment"># 这里定义模型真实参数。</span></span><br><span class="line">true_w = nd.ones((num_inputs, <span class="number">1</span>)) * <span class="number">0.01</span></span><br><span class="line">true_b = <span class="number">0.05</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#我们接着生成训练和测试数据集。</span></span><br><span class="line">X = nd.random.normal(shape=(num_train + num_test, num_inputs))</span><br><span class="line">y = nd.dot(X, true_w)</span><br><span class="line">y += <span class="number">.01</span> * nd.random.normal(shape=y.shape)</span><br><span class="line"></span><br><span class="line">X_train, X_test = X[:num_train, :], X[num_train:, :]</span><br><span class="line">y_train, y_test = y[:num_train], y[num_train:]</span><br><span class="line"></span><br><span class="line"><span class="comment">#当我们开始训练神经网络的时候，我们需要不断读取数据块。</span></span><br><span class="line"><span class="comment"># 这里我们定义一个函数它每次返回batch_size个随机的样本和对应的目标。</span></span><br><span class="line"><span class="comment"># 我们通过python的yield来构造一个迭代器。</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter</span><span class="params">(num_examples)</span>:</span></span><br><span class="line">    idx = list(range(num_examples))</span><br><span class="line">    random.shuffle(idx)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        j = nd.array(idx[i:min(i+batch_size,num_examples)])</span><br><span class="line">        <span class="keyword">yield</span> X.take(j), y.take(j)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 初始化模型参数</span></span><br><span class="line"><span class="comment"># 下面我们随机初始化模型参数。</span></span><br><span class="line"><span class="comment"># 之后训练时我们需要对这些参数求导来更新它们的值，所以我们需要创建它们的梯度。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span><span class="params">()</span>:</span></span><br><span class="line">    w = nd.random.normal(shape=(num_inputs, <span class="number">1</span>))*<span class="number">0.1</span></span><br><span class="line">    b = nd.zeros((<span class="number">1</span>,))</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> (w, b):</span><br><span class="line">        param.attach_grad()</span><br><span class="line">    <span class="keyword">return</span> (w, b)</span><br><span class="line"></span><br><span class="line"><span class="comment">#下面我们定义L2正则化。注意有些时候大家对偏移加罚，有时候不加罚。</span></span><br><span class="line"><span class="comment"># 通常结果上两者区别不大。这里我们演示对偏移也加罚的情况：</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L2_penalty</span><span class="params">(w, b)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (w**<span class="number">2</span>).sum() + b**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 定义训练和测试</span></span><br><span class="line"><span class="comment">#下面我们定义剩下的所需要的函数。这个跟之前的教程大致一样，</span></span><br><span class="line"><span class="comment"># 主要是区别在于计算`loss`的时候我们加上了L2正则化，以及我们将训练和测试损失都画了出来。</span></span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line">mpl.rcParams[<span class="string">'figure.dpi'</span>]= <span class="number">120</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X, lambd, w, b)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> nd.dot(X, w) + b</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">square_loss</span><span class="params">(yhat, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (yhat - y.reshape(yhat.shape)) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(params, lr)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param[:] = param - lr * param.grad</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(params, X, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> square_loss(net(X, <span class="number">0</span>, *params), y).mean().asscalar()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(lambd)</span>:</span></span><br><span class="line">    epochs = <span class="number">10</span></span><br><span class="line">    learning_rate = <span class="number">0.002</span></span><br><span class="line">    params = get_params()</span><br><span class="line">    train_loss = []</span><br><span class="line">    test_loss = []</span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> range(epochs):</span><br><span class="line">        <span class="keyword">for</span> data, label <span class="keyword">in</span> data_iter(num_train):</span><br><span class="line">            <span class="keyword">with</span> autograd.record():</span><br><span class="line">                output = net(data, lambd, *params)</span><br><span class="line">                loss = square_loss(<span class="comment">##loss加上了正则化</span></span><br><span class="line">                    output, label) + lambd * L2_penalty(*params)</span><br><span class="line">            loss.backward()</span><br><span class="line">            SGD(params, learning_rate)</span><br><span class="line">        train_loss.append(test(params, X_train, y_train))</span><br><span class="line">        test_loss.append(test(params, X_test, y_test))</span><br><span class="line">    plt.plot(train_loss)</span><br><span class="line">    plt.plot(test_loss)</span><br><span class="line">    plt.legend([<span class="string">'train'</span>, <span class="string">'test'</span>])</span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="keyword">return</span> <span class="string">'learned w[:10]:'</span>, params[<span class="number">0</span>][:<span class="number">10</span>], <span class="string">'learend b:'</span>, params[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">##过拟合</span></span><br><span class="line"><span class="comment"># train(0)</span></span><br><span class="line"><span class="comment">##使用正则</span></span><br><span class="line">train(<span class="number">2</span>)</span><br></pre></td></tr></table></figure></p>
<p>过拟合<br><img src="/img/deeplearn/img3.png" alt="img3"><br>使用正则<br><img src="/img/deeplearn/img4.png" alt="img4"></p>
<h2 id="2-3-_u4F7F_u7528Gluon_u6B63_u5219_u5316"><a href="#2-3-_u4F7F_u7528Gluon_u6B63_u5219_u5316" class="headerlink" title="2.3.使用Gluon正则化"></a>2.3.使用Gluon正则化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line"></span><br><span class="line">num_train = <span class="number">20</span></span><br><span class="line">num_test = <span class="number">100</span></span><br><span class="line">num_inputs = <span class="number">200</span></span><br><span class="line"></span><br><span class="line">true_w = nd.ones((num_inputs, <span class="number">1</span>)) * <span class="number">0.01</span></span><br><span class="line">true_b = <span class="number">0.05</span></span><br><span class="line">X = nd.random.normal(shape=(num_train + num_test, num_inputs))</span><br><span class="line">y = nd.dot(X, true_w)</span><br><span class="line">y += <span class="number">.01</span> * nd.random.normal(shape=y.shape)</span><br><span class="line">X_train, X_test = X[:num_train, :], X[num_train:, :]</span><br><span class="line">y_train, y_test = y[:num_train], y[num_train:]</span><br><span class="line"></span><br><span class="line"><span class="comment">## 定义训练和测试</span></span><br><span class="line"><span class="comment">#跟前一样定义训练模块。你也许发现了主要区别，`Trainer`</span></span><br><span class="line"><span class="comment">#有一个新参数</span></span><br><span class="line"><span class="comment">#wd。我们通过优化算法的wd参数(weightdecay)实现对模型的正则化。这相当于L2范数正则化。</span></span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line"></span><br><span class="line">mpl.rcParams[<span class="string">'figure.dpi'</span>] = <span class="number">120</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line">dataset_train = gluon.data.ArrayDataset(X_train, y_train)</span><br><span class="line">data_iter_train = gluon.data.DataLoader(dataset_train, batch_size, shuffle=<span class="keyword">True</span>)</span><br><span class="line">square_loss = gluon.loss.L2Loss()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(net, X, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> square_loss(net(X), y).mean().asscalar()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(weight_decay)</span>:</span></span><br><span class="line">    learning_rate = <span class="number">0.005</span></span><br><span class="line">    epochs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">    net = gluon.nn.Sequential()</span><br><span class="line">    <span class="keyword">with</span> net.name_scope():</span><br><span class="line">        net.add(gluon.nn.Dense(<span class="number">1</span>))</span><br><span class="line">    net.initialize()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 注意到这里 'wd'</span></span><br><span class="line">    trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;</span><br><span class="line">        <span class="string">'learning_rate'</span>: learning_rate, <span class="string">'wd'</span>: weight_decay&#125;)<span class="comment">###参数wd</span></span><br><span class="line"></span><br><span class="line">    train_loss = []</span><br><span class="line">    test_loss = []</span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> range(epochs):</span><br><span class="line">        <span class="keyword">for</span> data, label <span class="keyword">in</span> data_iter_train:</span><br><span class="line">            <span class="keyword">with</span> autograd.record():</span><br><span class="line">                output = net(data)</span><br><span class="line">                loss = square_loss(output, label)</span><br><span class="line">            loss.backward()</span><br><span class="line">            trainer.step(batch_size)</span><br><span class="line">        train_loss.append(test(net, X_train, y_train))</span><br><span class="line">        test_loss.append(test(net, X_test, y_test))</span><br><span class="line">    plt.plot(train_loss)</span><br><span class="line">    plt.plot(test_loss)</span><br><span class="line">    plt.legend([<span class="string">'train'</span>, <span class="string">'test'</span>])</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (<span class="string">'learned w[:10]:'</span>, net[<span class="number">0</span>].weight.data()[:, :<span class="number">10</span>],</span><br><span class="line">            <span class="string">'learned b:'</span>, net[<span class="number">0</span>].bias.data())</span><br><span class="line"></span><br><span class="line">train(<span class="number">0</span>)</span><br><span class="line">train(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h2 id="2-4-_u4F7F_u7528GPU_u6765_u8BA1_u7B97"><a href="#2-4-_u4F7F_u7528GPU_u6765_u8BA1_u7B97" class="headerlink" title="2.4.使用GPU来计算"></a>2.4.使用GPU来计算</h2><p>MXNet使用Context来指定使用哪个设备来存储和计算。默认会将数据开在主内存，然后利用CPU来计算，这个由<code>mx.cpu()</code>来表示。GPU则由<code>mx.gpu()</code>来表示。注意<code>mx.cpu()</code>表示所有的物理CPU和内存，意味着计算上会尽量使用多有的CPU核。但<code>mx.gpu()</code>只代表一块显卡和其对应的显卡内存。如果有多块GPU，我们用<code>mx.gpu(i)</code>来表示第<em>i</em>块GPU（<em>i</em>从0开始）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> [mx.cpu(), mx.gpu(), mx.gpu(<span class="number">1</span>)]  <span class="comment">#[cpu(0), gpu(0), gpu(1)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#每个NDArray都有一个`context`属性来表示它存在哪个设备上，默认会是`cpu`。</span></span><br><span class="line"><span class="comment"># 这是为什么前面每次我们打印NDArray的时候都会看到`@cpu(0)`这个标识。</span></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line">x = nd.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"><span class="keyword">print</span> x.context  <span class="comment">#cpu(0)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### GPU上创建内存</span></span><br><span class="line"><span class="comment">#我们可以在创建的时候指定创建在哪个设备上</span></span><br><span class="line"><span class="comment"># （如果GPU不能用或者没有装MXNet GPU版本，这里会有error）：</span></span><br><span class="line">a = nd.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], ctx=mx.gpu())</span><br><span class="line">b = nd.zeros((<span class="number">3</span>,<span class="number">2</span>), ctx=mx.gpu())</span><br><span class="line">c = nd.random.uniform(shape=(<span class="number">2</span>,<span class="number">3</span>), ctx=mx.gpu())</span><br><span class="line"><span class="keyword">print</span> (a,b,c) <span class="comment">##a,b,c都在gpu上</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#我们可以通过`copyto`和`as_in_context`来在设备直接传输数据。</span></span><br><span class="line"></span><br><span class="line">y = x.copyto(mx.gpu())</span><br><span class="line">z = x.as_in_context(mx.gpu())</span><br><span class="line"><span class="keyword">print</span> (y, z)</span><br><span class="line"></span><br><span class="line"><span class="comment">#这两个函数的主要区别是，如果源和目标的context一致（都在gpu上）</span></span><br><span class="line"><span class="comment"># `as_in_context`不复制，而`copyto`总是会新建内存：</span></span><br><span class="line">yy = y.as_in_context(mx.gpu())</span><br><span class="line">zz = z.copyto(mx.gpu())</span><br><span class="line"><span class="keyword">print</span> (yy <span class="keyword">is</span> y, zz <span class="keyword">is</span> z) <span class="comment">#(True, False)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#计算会在数据的`context`上执行。所以为了使用GPU，我们只需要事先将数据放在上面就行了。</span></span><br><span class="line"><span class="comment"># 结果会自动保存在对应的设备上：</span></span><br><span class="line"><span class="keyword">print</span> nd.exp(z + <span class="number">2</span>) * y <span class="comment">##gpu上</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#注意所有计算要求输入数据在同一个设备上。不一致的时候系统不进行自动复制。</span></span><br><span class="line"><span class="comment"># 这个设计的目的是因为设备之间的数据交互通常比较昂贵，</span></span><br><span class="line"><span class="comment"># 我们希望用户确切的知道数据放在哪里，而不是隐藏这个细节。</span></span><br><span class="line"><span class="comment"># 下面代码尝试将CPU上`x`和GPU上的`y`做运算。</span></span><br><span class="line"><span class="comment"># #异常require all inputs live on the same context.</span></span><br><span class="line"><span class="comment">#     # But the first argument is on cpu(0) while the 2-th argument is on gpu(0)</span></span><br><span class="line"><span class="comment"># try:</span></span><br><span class="line"><span class="comment">#     x + y</span></span><br><span class="line"><span class="comment"># except mx.MXNetError as err:</span></span><br><span class="line"><span class="comment">#     sys.stderr.write(str(err))</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### 默认会复制回CPU的操作</span></span><br><span class="line"><span class="comment">#如果某个操作需要将NDArray里面的内容转出来，例如打印或变成numpy格式，</span></span><br><span class="line"><span class="comment"># 如果需要的话系统都会自动将数据copy到主内存。</span></span><br><span class="line">print(y)</span><br><span class="line">print(y.asnumpy())</span><br><span class="line">print(y.sum().asscalar())</span><br><span class="line"></span><br><span class="line"><span class="comment">## Gluon的GPU计算</span></span><br><span class="line"><span class="comment">#同NDArray类似，Gluon的大部分函数可以通过`ctx`指定设备。</span></span><br><span class="line"><span class="comment"># 下面代码将模型参数初始化在GPU上：</span></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line">net = gluon.nn.Sequential()</span><br><span class="line">net.add(gluon.nn.Dense(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">net.initialize(ctx=mx.gpu())  <span class="comment">####gpu,不给默认cpu</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入GPU上的数据，会在GPU上计算结果</span></span><br><span class="line">data = nd.random.uniform(shape=[<span class="number">3</span>,<span class="number">2</span>], ctx=mx.gpu())</span><br><span class="line"><span class="keyword">print</span> net(data)  <span class="comment">##gpu(0)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#确认下权重：</span></span><br><span class="line"><span class="keyword">print</span> net[<span class="number">0</span>].weight.data()</span><br></pre></td></tr></table></figure>
<p>通过<code>context</code>我们可以很容易在不同的设备上计算。<br>练习</p>
<ul>
<li>试试大一点的计算任务，例如大矩阵的乘法，看看CPU和GPU的速度区别。如果是计算量很小的任务呢？</li>
<li>试试CPU和GPU之间传递数据的速度</li>
<li>GPU上如何读写模型呢？</li>
</ul>
<h2 id="2-5-_u4ECE0_u5F00_u59CB_u5377_u79EF_u795E_u7ECF_u7F51_u7EDC"><a href="#2-5-_u4ECE0_u5F00_u59CB_u5377_u79EF_u795E_u7ECF_u7F51_u7EDC" class="headerlink" title="2.5.从0开始卷积神经网络"></a>2.5.从0开始卷积神经网络</h2><p>前面讲的把图片拉成了一个向量，行相关像素间信息保留了，但是列信息丢失啦，本节用卷积。<br>之前的教程里，在输入神经网络前我们将输入图片直接转成了向量。这样做有两个不好的地方：</p>
<ul>
<li>在图片里相近的像素在向量表示里可能很远，从而模型很难捕获他们的空间关系。</li>
<li>对于大图片输入，模型可能会很大。例如输入是256x256x3的照片（仍然远比手机拍的小），输入层是1000，那么这一层的模型大小是将近1GB.</li>
</ul>
<p>这一节我们介绍卷积神经网络，其有效了解决了上述两个问题。</p>
<p>卷积神经网络是指主要由卷积层构成的神经网络。<br>卷积层跟前面的全连接层类似，但输入和权重不是做简单的矩阵乘法，而是使用每次作用在一个窗口上的卷积。下图演示了输入是一个4x4矩阵，使用一个3x3的权重，计算得到2x2结果的过程。每次我们采样一个跟权重一样大小的窗口，让它跟权重做按元素的乘法然后相加。通常我们也是用卷积的术语把这个权重叫kernel或者filter。<br><img src="/img/deeplearn/img4.gif" alt="img4"><br>我们可以控制如何移动窗口，和在边缘的时候如何填充窗口。下图演示了<code>stride=2</code>和<code>pad=1</code>。<br><img src="/img/deeplearn/img5.gif" alt="img5"><br>utils.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(params, lr)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param[:] = param - lr * param.grad</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(output, label)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> nd.mean(output.argmax(axis=<span class="number">1</span>)==label).asscalar()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span><span class="params">(data_iterator, net, ctx=mx.cpu<span class="params">()</span>)</span>:</span></span><br><span class="line">    acc = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> data_iterator:</span><br><span class="line">        output = net(data.as_in_context(ctx))</span><br><span class="line">        acc += accuracy(output, label.as_in_context(ctx))</span><br><span class="line">    <span class="keyword">return</span> acc / len(data_iterator)</span><br></pre></td></tr></table></figure></p>
<p>cnnscratch.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> image</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入输出数据格式是 batch x channel x height x width，这里batch和channel都是1</span></span><br><span class="line"><span class="comment"># 权重格式是 output_channels x in_channels x height x width，这里input_filter和output_filter都是1。</span></span><br><span class="line">w = nd.arange(<span class="number">4</span>).reshape((<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">b = nd.array([<span class="number">1</span>])</span><br><span class="line">data = nd.arange(<span class="number">9</span>).reshape((<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line"><span class="comment">## kernel为后面两个维度大小 这里是2x2</span></span><br><span class="line">out = nd.Convolution(data, w, b, kernel=w.shape[<span class="number">2</span>:], num_filter=w.shape[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">print(<span class="string">'input:'</span>, data, <span class="string">'\n\nweight:'</span>, w, <span class="string">'\n\nbias:'</span>, b, <span class="string">'\n\noutput:'</span>, out)</span><br><span class="line"><span class="comment">## input: [[[[ 0.  1.  2.][ 3.  4.  5.][ 6.  7.  8.]]]]</span></span><br><span class="line"><span class="comment">## weight:[[[[ 0.  1.][ 2.  3.]]]] bias:', [ 1.]</span></span><br><span class="line"><span class="comment">## output:', [[[[ 20.  26.][ 38.  44.]]]]</span></span><br><span class="line"><span class="comment">##计算： 20=0x0+1x1+3x2+4x3+b</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们可以控制如何移动窗口，和在边缘的时候如何填充窗口。如stride=2和pad=1。</span></span><br><span class="line"><span class="comment"># stride=(2,2)每次移动两格  pad=(1,1)边缘补充0</span></span><br><span class="line">out = nd.Convolution(data, w, b, kernel=w.shape[<span class="number">2</span>:], num_filter=w.shape[<span class="number">1</span>],</span><br><span class="line">                     stride=(<span class="number">2</span>,<span class="number">2</span>), pad=(<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">print(<span class="string">'input:'</span>, data, <span class="string">'\n\nweight:'</span>, w, <span class="string">'\n\nbias:'</span>, b, <span class="string">'\n\noutput:'</span>, out)</span><br><span class="line"><span class="comment">#输出:[[[[  1.   9.] [ 22.  44.]]]] 因为周围填充了0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#当输入数据有多个通道的时候，每个通道会有对应的权重，然后会对每个通道做卷积之后在通道之间求和</span></span><br><span class="line"><span class="comment">#两个通道情况</span></span><br><span class="line">w = nd.arange(<span class="number">8</span>).reshape((<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">data = nd.arange(<span class="number">18</span>).reshape((<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">out = nd.Convolution(data, w, b, kernel=w.shape[<span class="number">2</span>:], num_filter=w.shape[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">'input:'</span>, data, <span class="string">'\n\nweight:'</span>, w, <span class="string">'\n\nbias:'</span>, b, <span class="string">'\n\noutput:'</span>, out)</span><br><span class="line"><span class="comment">## ('input:',</span></span><br><span class="line"><span class="comment"># [[[[  0.   1.   2.][  3.   4.   5.][  6.   7.   8.]]</span></span><br><span class="line"><span class="comment">#   [[  9.  10.  11.][ 12.  13.  14.][ 15.  16.  17.]]]]</span></span><br><span class="line"><span class="comment"># weight:[[[[ 0.  1.][ 2.  3.]]  [[ 4.  5.] [ 6.  7.]]]]</span></span><br><span class="line"><span class="comment"># bias:[ 1.]</span></span><br><span class="line"><span class="comment">#output [[[[ 269.  297.] [ 353.  381.]]]]</span></span><br><span class="line"><span class="comment"># 输出为两个通道卷积之后取和</span></span><br><span class="line"></span><br><span class="line"><span class="comment">######### 下一层输入需要?</span></span><br><span class="line"><span class="comment"># 当输入需要多通道时，每个输出通道有对应权重，然后每个通道上做卷积。</span></span><br><span class="line">w = nd.arange(<span class="number">16</span>).reshape((<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">data = nd.arange(<span class="number">18</span>).reshape((<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">b = nd.array([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">out = nd.Convolution(data, w, b, kernel=w.shape[<span class="number">2</span>:], num_filter=w.shape[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">'input:'</span>, data, <span class="string">'\n\nweight:'</span>, w, <span class="string">'\n\nbias:'</span>, b, <span class="string">'\n\noutput:'</span>, out)</span><br><span class="line"><span class="comment">## input: [[[[  0.   1.   2.][  3.   4.   5.][  6.   7.   8.]]</span></span><br><span class="line"><span class="comment">#           [[  9.  10.  11.][ 12.  13.  14.][ 15.  16.  17.]]]]</span></span><br><span class="line"><span class="comment"># weight: [[[[  0.   1.][  2.   3.]][[  4.   5.][  6.   7.]]]</span></span><br><span class="line"><span class="comment">#          [[[  8.   9.][ 10.  11.]][[ 12.  13.][ 14.  15.]]]]</span></span><br><span class="line"><span class="comment"># bias: [ 1.  2.]</span></span><br><span class="line"><span class="comment"># output: [[[[  269.   297.][  353.   381.]]</span></span><br><span class="line"><span class="comment">#          [[  686.   778.] [  962.  1054.]]]]</span></span><br><span class="line"><span class="comment"># 输出为 多个输出（这里为2个是不同weight（这里2个）分别与input卷积得到</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#### 池化层（pooling）</span></span><br><span class="line"><span class="comment"># 因为卷积层每次作用在一个窗口，它对位置很敏感。池化层能够很好的缓解这个问题。</span></span><br><span class="line"><span class="comment"># 它跟卷积类似每次看一个小窗口，然后选出窗口里面最大的元素，或者平均元素作为输出。</span></span><br><span class="line">data = nd.arange(<span class="number">18</span>).reshape((<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">max_pool = nd.Pooling(data=data, pool_type=<span class="string">"max"</span>, kernel=(<span class="number">2</span>,<span class="number">2</span>)) <span class="comment">#kernel=(2,2)每次作2x2矩阵</span></span><br><span class="line">avg_pool = nd.Pooling(data=data, pool_type=<span class="string">"avg"</span>, kernel=(<span class="number">2</span>,<span class="number">2</span>)) <span class="comment">#max/avg去最大或者平均</span></span><br><span class="line">print(<span class="string">'data:'</span>, data, <span class="string">'\n\nmax pooling:'</span>, max_pool, <span class="string">'\n\navg pooling:'</span>, avg_pool)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">####################################</span></span><br><span class="line"><span class="comment">#下面我们可以开始使用这些层构建模型了。</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_fashion_mnist</span><span class="params">(batch_size, resize=None)</span>:</span></span><br><span class="line">    <span class="string">"""download the fashion mnist dataest and then load into memory"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform_mnist</span><span class="params">(data, label)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> resize:</span><br><span class="line">            <span class="comment"># resize to resize x resize</span></span><br><span class="line">            data = image.imresize(data, resize, resize)</span><br><span class="line">        <span class="comment"># change data from height x weight x channel to channel x height x weight</span></span><br><span class="line">        <span class="keyword">return</span> nd.transpose(data.astype(<span class="string">'float32'</span>), (<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>))/<span class="number">255</span>, label.astype(<span class="string">'float32'</span>)</span><br><span class="line">    mnist_train = gluon.data.vision.MNIST(train=<span class="keyword">True</span>, transform=transform_mnist)</span><br><span class="line">    mnist_test = gluon.data.vision.MNIST(train=<span class="keyword">False</span>, transform=transform_mnist)</span><br><span class="line">    train_data = gluon.data.DataLoader(mnist_train, batch_size, shuffle=<span class="keyword">True</span>)</span><br><span class="line">    test_data = gluon.data.DataLoader(mnist_test, batch_size, shuffle=<span class="keyword">False</span>)</span><br><span class="line">    <span class="keyword">return</span> (train_data, test_data)</span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_data, test_data = load_data_fashion_mnist(batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 定义模型</span></span><br><span class="line"><span class="comment"># 因为卷积网络计算比全连接要复杂，这里我们默认使用GPU来计算。</span></span><br><span class="line"><span class="comment"># 如果GPU不能用，默认使用CPU。</span></span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    ctx = mx.gpu()</span><br><span class="line">    _ = nd.zeros((<span class="number">1</span>,), ctx=ctx)</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    ctx = mx.cpu()</span><br><span class="line"><span class="keyword">print</span> ctx</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们使用MNIST常用的LeNet，它有两个卷积层，之后是两个全连接层。</span></span><br><span class="line"><span class="comment"># 注意到我们将权重全部创建在ctx上：</span></span><br><span class="line">weight_scale = <span class="number">.01</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># output channels = 20, kernel = (5,5) ##卷积1</span></span><br><span class="line">W1 = nd.random_normal(shape=(<span class="number">20</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">5</span>), scale=weight_scale, ctx=ctx)</span><br><span class="line">b1 = nd.zeros(W1.shape[<span class="number">0</span>], ctx=ctx)</span><br><span class="line"><span class="comment"># output channels = 50, kernel = (3,3)  ##卷积2</span></span><br><span class="line">W2 = nd.random_normal(shape=(<span class="number">50</span>,<span class="number">20</span>,<span class="number">3</span>,<span class="number">3</span>), scale=weight_scale, ctx=ctx)</span><br><span class="line">b2 = nd.zeros(W2.shape[<span class="number">0</span>], ctx=ctx)</span><br><span class="line"><span class="comment"># output dim = 128</span></span><br><span class="line">W3 = nd.random_normal(shape=(<span class="number">1250</span>, <span class="number">128</span>), scale=weight_scale, ctx=ctx)</span><br><span class="line">b3 = nd.zeros(W3.shape[<span class="number">1</span>], ctx=ctx)</span><br><span class="line"><span class="comment"># output dim = 10</span></span><br><span class="line">W4 = nd.random_normal(shape=(W3.shape[<span class="number">1</span>], <span class="number">10</span>), scale=weight_scale, ctx=ctx)</span><br><span class="line">b4 = nd.zeros(W4.shape[<span class="number">1</span>], ctx=ctx)</span><br><span class="line"></span><br><span class="line">params = [W1, b1, W2, b2, W3, b3, W4, b4]</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">    param.attach_grad()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卷积模块通常是“卷积层-激活层-池化层”。然后转成2D矩阵输出给后面的全连接层。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X, verbose=False)</span>:</span></span><br><span class="line">    X = X.as_in_context(W1.context) <span class="comment">########数据都放在与W1相同的设备(cpu/gpu)</span></span><br><span class="line">    <span class="comment"># 第一层卷积</span></span><br><span class="line">    h1_conv = nd.Convolution(data=X, weight=W1, bias=b1, kernel=W1.shape[<span class="number">2</span>:], num_filter=W1.shape[<span class="number">0</span>])</span><br><span class="line">    h1_activation = nd.relu(h1_conv)</span><br><span class="line">    h1 = nd.Pooling(data=h1_activation, pool_type=<span class="string">"max"</span>, kernel=(<span class="number">2</span>,<span class="number">2</span>), stride=(<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">    <span class="comment"># 第二层卷积</span></span><br><span class="line">    h2_conv = nd.Convolution(data=h1, weight=W2, bias=b2, kernel=W2.shape[<span class="number">2</span>:], num_filter=W2.shape[<span class="number">0</span>])</span><br><span class="line">    h2_activation = nd.relu(h2_conv)</span><br><span class="line">    h2 = nd.Pooling(data=h2_activation, pool_type=<span class="string">"max"</span>, kernel=(<span class="number">2</span>,<span class="number">2</span>), stride=(<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">    h = nd.flatten(h2)</span><br><span class="line">    <span class="comment"># 第一层全连接</span></span><br><span class="line">    h3_linear = nd.dot(h, W3) + b3</span><br><span class="line">    h3 = nd.relu(h3_linear)</span><br><span class="line">    <span class="comment"># 第二层全连接</span></span><br><span class="line">    h4_linear = nd.dot(h3, W4) + b4</span><br><span class="line">    <span class="keyword">if</span> verbose:</span><br><span class="line">        print(<span class="string">'1st conv block:'</span>, h1.shape)</span><br><span class="line">        print(<span class="string">'2nd conv block:'</span>, h2.shape)</span><br><span class="line">        print(<span class="string">'flatten:'</span>, h.shape)</span><br><span class="line">        print(<span class="string">'1st dense:'</span>, h3.shape)</span><br><span class="line">        print(<span class="string">'2nd dense:'</span>, h4_linear.shape)</span><br><span class="line">        print(<span class="string">'output:'</span>, h4_linear)</span><br><span class="line">        <span class="comment"># ('1st conv block:', (256L, 20L, 12L, 12L))</span></span><br><span class="line">        <span class="comment"># ('2nd conv block:', (256L, 50L, 5L, 5L))</span></span><br><span class="line">        <span class="comment"># ('flatten:', (256L, 1250L))</span></span><br><span class="line">        <span class="comment"># ('1st dense:', (256L, 128L))</span></span><br><span class="line">        <span class="comment"># ('2nd dense:', (256L, 10L))</span></span><br><span class="line">        <span class="comment"># ('output:', ##256x10</span></span><br><span class="line">        <span class="comment">#  [[-1.40773540e-04 - 3.10097334e-06   2.64148621e-05..., 1.76987160e-04</span></span><br><span class="line">        <span class="comment">#    7.64008873e-05   9.32873518e-05]...]</span></span><br><span class="line">    <span class="keyword">return</span> h4_linear</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试一下，输出中间结果形状（当然可以直接打印结果)和最终结果。</span></span><br><span class="line"><span class="keyword">for</span> data, _ <span class="keyword">in</span> train_data:</span><br><span class="line">    net(data, verbose=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 训练</span></span><br><span class="line"><span class="comment"># 跟前面没有什么不同的，除了这里我们使用`as_in_context`将`data`和`label`都放置在需要的设备上。</span></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd <span class="keyword">as</span> autograd</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> SGD, accuracy, evaluate_accuracy</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line"></span><br><span class="line">softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()</span><br><span class="line">learning_rate = <span class="number">.2</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    train_loss = <span class="number">0.</span></span><br><span class="line">    train_acc = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_data:</span><br><span class="line">        label = label.as_in_context(ctx)</span><br><span class="line">        <span class="keyword">with</span> autograd.record():</span><br><span class="line">            output = net(data)</span><br><span class="line">            loss = softmax_cross_entropy(output, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        SGD(params, learning_rate/batch_size)</span><br><span class="line"></span><br><span class="line">        train_loss += nd.mean(loss).asscalar()</span><br><span class="line">        train_acc += accuracy(output, label)</span><br><span class="line"></span><br><span class="line">    test_acc = evaluate_accuracy(test_data, net, ctx)</span><br><span class="line">    print(<span class="string">"Epoch %d. Loss: %f, Train acc %f, Test acc %f"</span> % (</span><br><span class="line">        epoch, train_loss/len(train_data),</span><br><span class="line">        train_acc/len(train_data), test_acc))</span><br></pre></td></tr></table></figure></p>
<p>结论<br>可以看到卷积神经网络比前面的多层感知的分类精度更好。事实上，如果你看懂了这一章，那你基本知道了计算视觉里最重要的几个想法。LeNet早在90年代就提出来了。不管你相信不相信，如果你5年前懂了这个而且开了家公司，那么你很可能现在已经把公司作价几千万卖个某大公司了。幸运的是，或者不幸的是，现在的算法已经更加高级些了，接下来我们会看到一些更加新的想法。</p>
<p>练习</p>
<ul>
<li>试试改改卷积层设定，例如filter数量，kernel大小</li>
<li>试试把池化层从<code>max</code>改到<code>avg</code></li>
<li>如果你有GPU，那么尝试用CPU来跑一下看看</li>
<li>你可能注意到比前面的多层感知机慢了很多，那么尝试计算下这两个模型分别需要多少浮点计算。例如nxm和mxk的矩阵乘法需要浮点运算2nmk。</li>
</ul>
<h2 id="2-6-_u4F7F_u7528gluon_u5377_u79EF_u795E_u7ECF_u7F51_u7EDC"><a href="#2-6-_u4F7F_u7528gluon_u5377_u79EF_u795E_u7ECF_u7F51_u7EDC" class="headerlink" title="2.6.使用gluon卷积神经网络"></a>2.6.使用gluon卷积神经网络</h2><p>utils.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> image</span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(params, lr)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param[:] = param - lr * param.grad</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(output, label)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> nd.mean(output.argmax(axis=<span class="number">1</span>)==label).asscalar()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span><span class="params">(data_iterator, net, ctx=mx.cpu<span class="params">()</span>)</span>:</span></span><br><span class="line">    acc = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> data_iterator:</span><br><span class="line">        output = net(data.as_in_context(ctx))</span><br><span class="line">        acc += accuracy(output, label.as_in_context(ctx))</span><br><span class="line">    <span class="keyword">return</span> acc / len(data_iterator)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_fashion_mnist</span><span class="params">(batch_size, resize=None)</span>:</span></span><br><span class="line">    <span class="string">"""download the fashion mnist dataest and then load into memory"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform_mnist</span><span class="params">(data, label)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> resize:</span><br><span class="line">            <span class="comment"># resize to resize x resize</span></span><br><span class="line">            data = image.imresize(data, resize, resize)</span><br><span class="line">        <span class="comment"># change data from height x weight x channel to channel x height x weight</span></span><br><span class="line">        <span class="keyword">return</span> nd.transpose(data.astype(<span class="string">'float32'</span>), (<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>))/<span class="number">255</span>, label.astype(<span class="string">'float32'</span>)</span><br><span class="line">    mnist_train = gluon.data.vision.MNIST(</span><br><span class="line">        train=<span class="keyword">True</span>, transform=transform_mnist)</span><br><span class="line">    mnist_test = gluon.data.vision.MNIST(</span><br><span class="line">        train=<span class="keyword">False</span>, transform=transform_mnist)</span><br><span class="line">    train_data = gluon.data.DataLoader(</span><br><span class="line">        mnist_train, batch_size, shuffle=<span class="keyword">True</span>)</span><br><span class="line">    test_data = gluon.data.DataLoader(</span><br><span class="line">        mnist_test, batch_size, shuffle=<span class="keyword">False</span>)</span><br><span class="line">    <span class="keyword">return</span> (train_data, test_data)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(train_data, test_data, net, loss, trainer, ctx, num_epochs, print_batches=None)</span>:</span></span><br><span class="line">    <span class="string">"""Train a network"""</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        train_loss = <span class="number">0.</span></span><br><span class="line">        train_acc = <span class="number">0.</span></span><br><span class="line">        batch = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> data, label <span class="keyword">in</span> train_data:</span><br><span class="line">            label = label.as_in_context(ctx)</span><br><span class="line">            <span class="keyword">with</span> autograd.record():</span><br><span class="line">                output = net(data.as_in_context(ctx))</span><br><span class="line">                L = loss(output, label)</span><br><span class="line">            L.backward()</span><br><span class="line"></span><br><span class="line">            trainer.step(data.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">            train_loss += nd.mean(L).asscalar()</span><br><span class="line">            train_acc += accuracy(output, label)</span><br><span class="line"></span><br><span class="line">            batch += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> print_batches <span class="keyword">and</span> batch % print_batches == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">"Batch %d. Loss: %f, Train acc %f"</span> % (</span><br><span class="line">                    batch, train_loss/batch, train_acc/batch</span><br><span class="line">                ))</span><br><span class="line"></span><br><span class="line">        test_acc = evaluate_accuracy(test_data, net, ctx)</span><br><span class="line">        print(<span class="string">"Epoch %d. Loss: %f, Train acc %f, Test acc %f"</span> % (</span><br><span class="line">            epoch, train_loss/batch, train_acc/batch, test_acc</span><br><span class="line">        ))</span><br></pre></td></tr></table></figure></p>
<p>cnngluon.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> image</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</span><br><span class="line"><span class="keyword">import</span> utils</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="comment"># 下面是LeNet在Gluon里的实现，注意到我们不再需要实现去计算每层的输入大小，</span></span><br><span class="line"><span class="comment"># 尤其是接在卷积后面的那个全连接层。</span></span><br><span class="line">net = nn.Sequential()</span><br><span class="line"><span class="keyword">with</span> net.name_scope():</span><br><span class="line">    net.add(</span><br><span class="line">        nn.Conv2D(channels=<span class="number">20</span>, kernel_size=<span class="number">5</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">        nn.MaxPool2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>),</span><br><span class="line">        nn.Conv2D(channels=<span class="number">50</span>, kernel_size=<span class="number">3</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">        nn.MaxPool2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>),</span><br><span class="line">        nn.Flatten(),</span><br><span class="line">        nn.Dense(<span class="number">128</span>, activation=<span class="string">"relu"</span>),</span><br><span class="line">        nn.Dense(<span class="number">10</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment">## 获取数据和训练</span></span><br><span class="line"><span class="comment"># 剩下的跟上一章没什么不同，我们重用`utils.py`里定义的函数。</span></span><br><span class="line"><span class="comment"># 初始化</span></span><br><span class="line">ctx = mx.gpu()</span><br><span class="line">net.initialize(ctx=ctx)</span><br><span class="line">print(<span class="string">'initialize weight on'</span>, ctx)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取数据</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_data, test_data = utils.load_data_fashion_mnist(batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">loss = gluon.loss.SoftmaxCrossEntropyLoss()</span><br><span class="line">trainer = gluon.Trainer(net.collect_params(),</span><br><span class="line">                        <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.1</span>&#125;)</span><br><span class="line">utils.train(train_data, test_data, net, loss,</span><br><span class="line">            trainer, ctx, num_epochs=<span class="number">10</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="3-1-_u521B_u5EFA_u795E_u7ECF_u7F51_u7EDC"><a href="#3-1-_u521B_u5EFA_u795E_u7ECF_u7F51_u7EDC" class="headerlink" title="3.1.创建神经网络"></a>3.1.创建神经网络</h2><p>前面的教程我们教了大家如何实现线性回归，多类Logistic回归和多层感知机。我们既展示了如何从0开始实现，也提供使用<code>gluon</code>的更紧凑的实现。因为前面我们主要关注在模型本身，所以只解释了如何使用<code>gluon</code>，但没说明他们是如何工作的。我们使用了<code>nn.Sequential</code>，它是<code>nn.Block</code>的一个简单形式，但没有深入了解它们。<br>本教程和接下来几个教程，我们将详细解释如何使用这两个类来定义神经网络、初始化参数、以及保存和读取模型。<br>我们重新把使用Gluon多层感知机里的网络定义搬到这里作为开始的例子（为了简单起见，这里我们丢掉了Flatten层）。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">net = nn.Sequential()</span><br><span class="line"><span class="keyword">with</span> net.name_scope():</span><br><span class="line">    net.add(nn.Dense(<span class="number">256</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">    net.add(nn.Dense(<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">print(net)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 使用 `nn.Block` 来定义</span></span><br><span class="line"><span class="comment">#事实上，`nn.Sequential`是`nn.Block`的简单形式。</span></span><br><span class="line"><span class="comment"># 我们先来看下如何使用`nn.Block`来实现同样的网络。</span></span><br><span class="line"><span class="comment">## 使用nn.block定义更加灵活</span></span><br><span class="line"><span class="comment"># 可以看到`nn.Block`的使用是通过创建一个它子类的类，其中至少包含了两个函数。</span></span><br><span class="line"><span class="comment"># `__init__`：创建参数。上面例子我们使用了包含了参数的`dense`层</span></span><br><span class="line"><span class="comment"># `forward()`：定义网络的计算</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span><span class="params">(nn.Block)</span>:</span><span class="comment">## 定义MLP为nn.block的一个子class</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span><span class="comment">##初始化函数 self相当与自己的一个class</span></span><br><span class="line">        super(MLP, self).__init__(**kwargs)<span class="comment">##调用父类nn.block的初始化函数</span></span><br><span class="line">        <span class="keyword">with</span> self.name_scope():</span><br><span class="line">            self.dense0 = nn.Dense(<span class="number">256</span>) <span class="comment">##创建dense layer</span></span><br><span class="line">            self.dense1 = nn.Dense(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 注意：这里必须为foreard函数，不能改 调用forward会自动调用backward求导</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span><span class="comment">##创建一个forward函数  输入x时如下操作</span></span><br><span class="line">        <span class="keyword">return</span> self.dense1(nd.relu(self.dense0(x)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们所创建的类的使用跟前面`net`没有太多不一样。</span></span><br><span class="line">net2 = MLP()</span><br><span class="line">print(net2)</span><br><span class="line">net2.initialize()</span><br><span class="line">x = nd.random.uniform(shape=(<span class="number">4</span>,<span class="number">20</span>))</span><br><span class="line">y = net2(x)</span><br><span class="line"><span class="keyword">print</span> y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如何定义创建和使用`nn.Dense`比较好理解。接下来我们仔细看下`MLP`里面用的其他命令：</span></span><br><span class="line"><span class="comment"># `super(MLP, self).__init__(**kwargs)`：这句话调用`nn.Block`的 `__init__`函数，</span></span><br><span class="line"><span class="comment"># 它提供了`prefix`（指定名字）和`params`（指定模型参数）两个参数。我们会之后详细解释如何使用。</span></span><br><span class="line"><span class="comment"># `self.name_scope()`：调用`nn.Block`提供的`name_scope()`函数。</span></span><br><span class="line"><span class="comment"># `nn.Dense`的定义放在这个`scope`里面。</span></span><br><span class="line"><span class="comment"># 它的作用是给里面的所有层和参数的名字加上前缀（prefix）使得他们在系统里面</span></span><br><span class="line"><span class="comment"># 独一无二。默认自动会自动生成前缀，我们也可以在创建的时候手动指定。</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'default prefix:'</span>, net2.dense0.name) <span class="comment">#('default prefix:', 'mlp0_dense0')</span></span><br><span class="line">net3 = MLP(prefix=<span class="string">'another_mlp_'</span>)</span><br><span class="line">print(<span class="string">'customized prefix:'</span>, net3.dense0.name) <span class="comment">#('customized prefix:', 'another_mlp_dense0')</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 大家会发现这里并没有定义如何求导，或者是`backward()`函数。</span></span><br><span class="line"><span class="comment"># 事实上，系统会使用`autograd`对`forward()`自动生成对应的`backward()`函数。</span></span><br><span class="line"><span class="comment"># 在`gluon`里，`nn.Block`是一个一般化的部件。整个神经网络可以是一个`nn.Block`，单个层也是一个`nn.Block`。我们可以（近似）无限地嵌套`nn.Block`来构建新的`nn.Block`。</span></span><br><span class="line"><span class="comment"># `nn.Block`主要提供这个东西</span></span><br><span class="line"><span class="comment"># 1. 存储参数</span></span><br><span class="line"><span class="comment"># 2. 描述`forward`如何执行</span></span><br><span class="line"><span class="comment"># 3. 自动求导</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 那么现在可以解释`nn.Sequential`了吧</span></span><br><span class="line"><span class="comment"># `nn.Sequential`是一个`nn.Block`容器，它通过`add`来添加`nn.Block`。</span></span><br><span class="line"><span class="comment"># 它自动生成`forward()`函数，其就是把加进来的`nn.Block`逐一运行。</span></span><br><span class="line"><span class="comment"># 一个简单的实现是这样的：</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sequential</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super(Sequential, self).__init__(**kwargs)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(self, block)</span>:</span></span><br><span class="line">        self._children.append(block)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self._children:</span><br><span class="line">            x = block(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以跟`nn.Sequential`一样的使用这个自定义的类：</span></span><br><span class="line">net4 = Sequential()</span><br><span class="line"><span class="keyword">with</span> net4.name_scope():</span><br><span class="line">    net4.add(nn.Dense(<span class="number">256</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">    net4.add(nn.Dense(<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">net4.initialize()</span><br><span class="line">y = net4(x)</span><br><span class="line"><span class="keyword">print</span> y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以看到，`nn.Sequential`的主要好处是定义网络起来更加简单。</span></span><br><span class="line"><span class="comment"># 但`nn.Block`可以提供更加灵活的网络定义。考虑下面这个例子</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FancyMLP</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super(FancyMLP, self).__init__(**kwargs)</span><br><span class="line">        <span class="keyword">with</span> self.name_scope():</span><br><span class="line">            self.dense = nn.Dense(<span class="number">256</span>)</span><br><span class="line">            self.weight = nd.random_uniform(shape=(<span class="number">256</span>,<span class="number">20</span>))<span class="comment">##创建weight</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = nd.relu(self.dense(x))</span><br><span class="line">        x = nd.relu(nd.dot(x, self.weight)+<span class="number">1</span>) <span class="comment">##手动添加一个乘法</span></span><br><span class="line">        x = nd.relu(self.dense(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 看到这里我们直接手动创建和初始了权重`weight`，并重复用了`dense`的层。测试一下：</span></span><br><span class="line">fancy_mlp = FancyMLP()</span><br><span class="line">fancy_mlp.initialize()</span><br><span class="line">y = fancy_mlp(x)</span><br><span class="line">print(y.shape)<span class="comment">##(4L, 256L)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## `nn.Block`和`nn.Sequential`的嵌套使用</span></span><br><span class="line"><span class="comment"># 现在我们知道了`nn`下面的类基本都是`nn.Block`的子类，他们可以很方便地嵌套使用。</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RecMLP</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super(RecMLP, self).__init__(**kwargs)</span><br><span class="line">        self.net = nn.Sequential()</span><br><span class="line">        <span class="keyword">with</span> self.name_scope():</span><br><span class="line">            self.net.add(nn.Dense(<span class="number">256</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">            self.net.add(nn.Dense(<span class="number">128</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">            self.dense = nn.Dense(<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> nd.relu(self.dense(self.net(x)))</span><br><span class="line"></span><br><span class="line">rec_mlp = nn.Sequential()</span><br><span class="line">rec_mlp.add(RecMLP())</span><br><span class="line">rec_mlp.add(nn.Dense(<span class="number">10</span>))</span><br><span class="line">print(rec_mlp)</span><br></pre></td></tr></table></figure></p>
<h2 id="3-2-_u521D_u59CB_u5316_u6A21_u578B_u53C2_u6570"><a href="#3-2-_u521D_u59CB_u5316_u6A21_u578B_u53C2_u6570" class="headerlink" title="3.2.初始化模型参数"></a>3.2.初始化模型参数</h2><p>我们仍然用MLP这个例子来详细解释如何初始化模型参数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_net</span><span class="params">()</span>:</span></span><br><span class="line">    net = nn.Sequential()</span><br><span class="line">    <span class="keyword">with</span> net.name_scope():</span><br><span class="line">        net.add(nn.Dense(<span class="number">4</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">        net.add(nn.Dense(<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line">x = nd.random.uniform(shape=(<span class="number">3</span>,<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># # 我们知道如果不`initialize()`直接跑forward，那么系统会抱怨说参数没有初始化。</span></span><br><span class="line"><span class="comment"># import sys</span></span><br><span class="line"><span class="comment"># try:</span></span><br><span class="line"><span class="comment">#     net = get_net()</span></span><br><span class="line"><span class="comment">#     net(x)</span></span><br><span class="line"><span class="comment"># except RuntimeError as err:</span></span><br><span class="line"><span class="comment">#     sys.stderr.write(str(err))</span></span><br><span class="line"><span class="comment"># ## Parameter sequential0_dense0_bias has not been initialized.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 正确的打开方式是这样</span></span><br><span class="line">net = get_net()</span><br><span class="line">net.initialize()</span><br><span class="line"><span class="keyword">print</span> net(x)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 访问模型参数</span></span><br><span class="line"><span class="comment"># 之前我们提到过可以通过`weight`和`bias`访问`Dense`的参数，他们是`Parameter`这个类：</span></span><br><span class="line">w = net[<span class="number">0</span>].weight</span><br><span class="line">b = net[<span class="number">0</span>].bias</span><br><span class="line">print(<span class="string">'name: '</span>, net[<span class="number">0</span>].name, <span class="string">'\nweight: '</span>, w, <span class="string">'\nbias: '</span>, b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后我们可以通过`data`来访问参数，`grad`来访问对应的梯度</span></span><br><span class="line">print(<span class="string">'weight:'</span>, w.data()) <span class="comment">##w的数值</span></span><br><span class="line">print(<span class="string">'weight gradient'</span>, w.grad()) <span class="comment">#w的梯度</span></span><br><span class="line">print(<span class="string">'bias:'</span>, b.data())</span><br><span class="line">print(<span class="string">'bias gradient'</span>, b.grad())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们也可以通过`collect_params`来访问Block里面所有的参数（这个会包括所有的子Block）。</span></span><br><span class="line"><span class="comment"># 它会返回一个名字到对应Parameter的dict。既可以用正常`[]`来访问参数，也可以用`get()`，它不需要填写名字的前缀。</span></span><br><span class="line">params = net.collect_params()</span><br><span class="line">print(params)</span><br><span class="line">print(params[<span class="string">'sequential0_dense0_bias'</span>].data())</span><br><span class="line">print(params.get(<span class="string">'dense0_weight'</span>).data())</span><br><span class="line"></span><br><span class="line"><span class="comment">## 使用不同的初始函数来初始化</span></span><br><span class="line"><span class="comment"># 我们一直在使用默认的`initialize`来初始化权重（除了指定GPU `ctx`外）。它会把所有权重初始化成在`[-0.07, 0.07]`之间均匀分布的随机数。</span></span><br><span class="line"><span class="comment"># 我们可以使用别的初始化方法。例如使用均值为0，方差为0.02的正态分布</span></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> init</span><br><span class="line">params.initialize(init=init.Normal(sigma=<span class="number">0.02</span>), force_reinit=<span class="keyword">True</span>) <span class="comment">##为了防止误操作初始化覆盖参数 需要force_reinit=True强制初始化</span></span><br><span class="line">print(net[<span class="number">0</span>].weight.data(), net[<span class="number">0</span>].bias.data())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 看得更加清楚点：  全部初始化为1</span></span><br><span class="line">params.initialize(init=init.One(), force_reinit=<span class="keyword">True</span>)</span><br><span class="line">print(net[<span class="number">0</span>].weight.data(), net[<span class="number">0</span>].bias.data())</span><br><span class="line"><span class="comment"># [[ 1.  1.  1.  1.  1.]</span></span><br><span class="line"><span class="comment">#  [ 1.  1.  1.  1.  1.]</span></span><br><span class="line"><span class="comment">#  [ 1.  1.  1.  1.  1.]</span></span><br><span class="line"><span class="comment">#  [ 1.  1.  1.  1.  1.]]</span></span><br><span class="line"><span class="comment"># [ 0.  0.  0.  0.]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 更多的方法参见[init的API](https://mxnet.incubator.apache.org/api/python/optimization.html#the-mxnet-initializer-package).</span></span><br><span class="line"><span class="comment"># 下面我们自定义一个初始化方法。</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyInit</span><span class="params">(init.Initializer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(MyInit, self).__init__()</span><br><span class="line">        self._verbose = <span class="keyword">True</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_weight</span><span class="params">(self, _, arr)</span>:</span></span><br><span class="line">        <span class="comment"># 初始化权重，使用out=arr后我们不需指定形状</span></span><br><span class="line">        print(<span class="string">'init weight'</span>, arr.shape)</span><br><span class="line">        nd.random.uniform(low=<span class="number">5</span>, high=<span class="number">10</span>, out=arr)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_bias</span><span class="params">(self, _, arr)</span>:</span></span><br><span class="line">        print(<span class="string">'init bias'</span>, arr.shape)</span><br><span class="line">        <span class="comment"># 初始化偏移</span></span><br><span class="line">        arr[:] = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">FIXME:</span> init_bias doesn't work</span></span><br><span class="line">params.initialize(init=MyInit(), force_reinit=<span class="keyword">True</span>)</span><br><span class="line">print(net[<span class="number">0</span>].weight.data(), net[<span class="number">0</span>].bias.data())</span><br><span class="line"></span><br><span class="line"><span class="comment">## 延后的初始化</span></span><br><span class="line"><span class="comment"># 我们之前提到过Gluon的一个便利的地方是模型定义的时候不需要指定输入的大小，在之后做forward的时候会自动推测参数的大小。我们具体来看这是怎么工作的。</span></span><br><span class="line"><span class="comment"># 新创建一个网络，然后打印参数。你会发现两个全连接层的权重的形状里都有0。 这是因为在不知道输入数据的情况下，我们无法判断它们的形状。</span></span><br><span class="line">net = get_net()</span><br><span class="line">print(net.collect_params())</span><br><span class="line"><span class="comment"># Parameter sequential1_dense0_weight(shape=(4, 0), dtype= &lt; type 'numpy.float32' &gt;)</span></span><br><span class="line"><span class="comment"># Parameter sequential1_dense0_bias(shape=(4,), dtype= &lt; type 'numpy.float32' &gt;)</span></span><br><span class="line"><span class="comment"># Parameter sequential1_dense1_weight(shape=(2, 0), dtype= &lt; type 'numpy.float32' &gt;)</span></span><br><span class="line"><span class="comment"># Parameter sequential1_dense1_bias(shape=(2,), dtype= &lt; type 'numpy.float32' &gt;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后我们初始化</span></span><br><span class="line">net.initialize(init=MyInit())</span><br><span class="line"><span class="comment"># 你会看到我们并没有看到MyInit打印的东西，这是因为我们仍然不知道形状。真正的初始化发生在我们看到数据时。</span></span><br><span class="line"><span class="keyword">print</span> net(x)</span><br><span class="line"><span class="comment"># 这时候我们看到shape里面的0被填上正确的值了。</span></span><br><span class="line">print(net.collect_params())</span><br><span class="line"><span class="comment"># Parameter sequential1_dense0_weight(shape=(4L, 5L), dtype= &lt; type 'numpy.float32' &gt;)</span></span><br><span class="line"><span class="comment"># Parameter sequential1_dense0_bias(shape=(4L,), dtype= &lt; type 'numpy.float32' &gt;)</span></span><br><span class="line"><span class="comment"># Parameter sequential1_dense1_weight(shape=(2L, 4L), dtype= &lt; type 'numpy.float32' &gt;)</span></span><br><span class="line"><span class="comment"># Parameter sequential1_dense1_bias(shape=(2L,), dtype= &lt; type 'numpy.float32' &gt;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 避免延后初始化</span></span><br><span class="line"><span class="comment"># 有时候我们不想要延后初始化，这时候可以在创建网络的时候指定输入大小。</span></span><br><span class="line">net = nn.Sequential()</span><br><span class="line"><span class="keyword">with</span> net.name_scope():</span><br><span class="line">    net.add(nn.Dense(<span class="number">4</span>, in_units=<span class="number">5</span>, activation=<span class="string">"relu"</span>)) <span class="comment">##输入input 5</span></span><br><span class="line">    net.add(nn.Dense(<span class="number">2</span>, in_units=<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">net.initialize(MyInit())</span><br><span class="line"><span class="comment"># ('init weight', (4L, 5L))</span></span><br><span class="line"><span class="comment"># ('init weight', (2L, 4L))</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 共享模型参数</span></span><br><span class="line"><span class="comment"># 有时候我们想在层之间共享同一份参数，我们可以通过Block的`params`输出参数来手动指定参数，而不是让系统自动生成。</span></span><br><span class="line">net = nn.Sequential()</span><br><span class="line"><span class="keyword">with</span> net.name_scope():</span><br><span class="line">    net.add(nn.Dense(<span class="number">4</span>, in_units=<span class="number">4</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">    <span class="comment">## 参数与最后一层共用 即与上一层参数相同</span></span><br><span class="line">    net.add(nn.Dense(<span class="number">4</span>, in_units=<span class="number">4</span>, activation=<span class="string">"relu"</span>, params=net[-<span class="number">1</span>].params)) <span class="comment">#net[-1].params net最后一层的参数</span></span><br><span class="line">    net.add(nn.Dense(<span class="number">2</span>, in_units=<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化然后打印</span></span><br><span class="line">net.initialize(MyInit())</span><br><span class="line"><span class="comment">## 如下两个参数相同</span></span><br><span class="line">print(net[<span class="number">0</span>].weight.data())</span><br><span class="line">print(net[<span class="number">1</span>].weight.data())</span><br></pre></td></tr></table></figure></p>
<p>我们可以很灵活地访问和修改模型参数。</p>
<p>练习</p>
<ol>
<li>研究下<code>net.collect_params()</code>返回的是什么？<code>net.params</code>呢？</li>
<li>如何对每个层使用不同的初始化函数</li>
<li>如果两个层共用一个参数，那么求梯度的时候会发生什么？</li>
</ol>
<h2 id="3-3-_u5E8F_u5217_u5316_u8BFB_u5199_u6A21_u578B"><a href="#3-3-_u5E8F_u5217_u5316_u8BFB_u5199_u6A21_u578B" class="headerlink" title="3.3.序列化读写模型"></a>3.3.序列化读写模型</h2><p>但即使知道了所有这些，我们还没有完全准备好来构建一个真正的机器学习系统。这是因为我们还没有讲如何读和写模型。因为现实中，我们通常在一个地方训练好模型，然后部署到很多不同的地方。我们需要把内存中的训练好的模型存在硬盘上好下次使用。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 读写NDArrays</span></span><br><span class="line"><span class="comment"># 作为开始，我们先看看如何读写NDArray。虽然我们可以使用Python的序列化包例如`Pickle`，不过我们更倾向直接`save`和`load`，</span></span><br><span class="line"><span class="comment"># 通常这样更快，而且别的语言，例如R和Scala也能用到。</span></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"></span><br><span class="line">x = nd.ones(<span class="number">3</span>)</span><br><span class="line">y = nd.zeros(<span class="number">4</span>)</span><br><span class="line">filename = <span class="string">"../data/test1.params"</span></span><br><span class="line">nd.save(filename, [x, y])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读回来</span></span><br><span class="line">a, b = nd.load(filename)</span><br><span class="line">print(a, b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不仅可以读写单个NDArray，NDArray list，dict也是可以的：</span></span><br><span class="line">mydict = &#123;<span class="string">"x"</span>: x, <span class="string">"y"</span>: y&#125;</span><br><span class="line">filename = <span class="string">"../data/test2.params"</span></span><br><span class="line">nd.save(filename, mydict)</span><br><span class="line">c = nd.load(filename)</span><br><span class="line">print(c)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 读写Gluon模型的参数</span></span><br><span class="line"><span class="comment"># 跟NDArray类似，Gluon的模型（就是`nn.Block`）提供便利的`save_params`和`load_params`函数来读写数据。</span></span><br><span class="line"><span class="comment"># 我们同前一样创建一个简单的多层感知机</span></span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_net</span><span class="params">()</span>:</span></span><br><span class="line">    net = nn.Sequential()</span><br><span class="line">    <span class="keyword">with</span> net.name_scope():</span><br><span class="line">        net.add(nn.Dense(<span class="number">10</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">        net.add(nn.Dense(<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line">net = get_net()</span><br><span class="line">net.initialize()</span><br><span class="line">x = nd.random.uniform(shape=(<span class="number">2</span>,<span class="number">10</span>))</span><br><span class="line">print(net(x))</span><br><span class="line"><span class="comment">#[[ 0.00205935 -0.00979935]</span></span><br><span class="line"> <span class="comment"># [ 0.00107034 -0.00423382]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面我们把模型参数存起来</span></span><br><span class="line">filename = <span class="string">"../data/mlp.params"</span></span><br><span class="line">net.save_params(filename)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 之后我们构建一个一样的多层感知机，但不像前面那样随机初始化，我们直接读取前面的模型参数。</span></span><br><span class="line"><span class="comment"># 这样给定同样的输入，新的模型应该会输出同样的结果。</span></span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line">net2 = get_net()</span><br><span class="line">net2.load_params(filename, mx.cpu())  <span class="comment"># FIXME, gluon will support default ctx later</span></span><br><span class="line">print(net2(x))</span><br><span class="line"><span class="comment">#[[ 0.00205935 -0.00979935]</span></span><br><span class="line"> <span class="comment"># [ 0.00107034 -0.00423382]]</span></span><br></pre></td></tr></table></figure></p>
<p>通过<code>load_params</code>和<code>save_params</code>可以很方便的读写模型参数。</p>
<h2 id="3-4-_u8BBE_u8BA1_u81EA_u5B9A_u4E49_u5C42"><a href="#3-4-_u8BBE_u8BA1_u81EA_u5B9A_u4E49_u5C42" class="headerlink" title="3.4.设计自定义层"></a>3.4.设计自定义层</h2><p>神经网络的一个魅力是它有大量的层，例如全连接、卷积、循环、激活，和各式花样的连接方式。我们之前学到了如何使用Gluon提供的层来构建新的层(<code>nn.Block</code>)继而得到神经网络。虽然Gluon提供了大量的<a href="https://mxnet.incubator.apache.org/versions/master/api/python/gluon/gluon.html#neural-network-layers" target="_blank" rel="external">层的定义</a>，但我们仍然会遇到现有层不够用的情况。</p>
<p>这时候的一个自然的想法是，我们不是学习了如何只使用基础数值运算包<code>NDArray</code>来实现各种的模型吗？它提供了大量的<a href="https://mxnet.incubator.apache.org/versions/master/api/python/ndarray/ndarray.html" target="_blank" rel="external">底层计算函数</a>足以实现即使不是100%那也是95%的神经网络吧。</p>
<p>但每次都从头写容易写到怀疑人生。实际上，即使在纯研究的领域里，我们也很少发现纯新的东西，大部分时候是在现有模型的基础上做一些改进。所以很可能大部分是可以沿用前面的而只有一部分是需要自己来实现。</p>
<p>这个教程我们将介绍如何使用底层的<code>NDArray</code>接口来实现一个<code>Gluon</code>的层，从而可以以后被重复调用。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们先来看如何定义一个简单层，它不需要维护模型参数。事实上这个跟前面介绍的如何使用nn.Block没什么区别。</span></span><br><span class="line"><span class="comment"># 下面代码定义一个层将输入减掉均值。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CenteredLayer</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super(CenteredLayer, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> x - x.mean()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们可以马上实例化这个层用起来。</span></span><br><span class="line">layer = CenteredLayer()</span><br><span class="line"><span class="keyword">print</span> layer(nd.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])) <span class="comment">##[-2. -1.  0.  1.  2.]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们也可以用它来构造更复杂的神经网络：</span></span><br><span class="line">net = nn.Sequential()</span><br><span class="line"><span class="keyword">with</span> net.name_scope():</span><br><span class="line">    net.add(nn.Dense(<span class="number">128</span>))</span><br><span class="line">    net.add(nn.Dense(<span class="number">10</span>))</span><br><span class="line">    net.add(CenteredLayer())</span><br><span class="line"><span class="comment"># 确认下输出的均值确实是0：</span></span><br><span class="line">net.initialize()</span><br><span class="line">y = net(nd.random.uniform(shape=(<span class="number">4</span>, <span class="number">8</span>)))</span><br><span class="line"><span class="keyword">print</span> y.mean() <span class="comment">##[  2.32830647e-11] (因为用的浮点32为约等于0)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 带模型参数的自定义层</span></span><br><span class="line"><span class="comment"># 虽然`CenteredLayer`可能会告诉实现自定义层大概是什么样子，但它缺少了重要的一块，就是它没有可以学习的模型参数。</span></span><br><span class="line"><span class="comment"># 记得我们之前访问`Dense`的权重的时候是通过`dense.weight.data()`，这里`weight`是一个`Parameter`的类型。</span></span><br><span class="line"><span class="comment"># 我们可以显示的构建这样的一个参数。</span></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line">my_param = gluon.Parameter(<span class="string">"exciting_parameter_yay"</span>, shape=(<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line"><span class="comment"># 这里我们创建一个3x3大小的参数并取名为"exciting_parameter_yay"。然后用默认方法初始化打印结果。</span></span><br><span class="line">my_param.initialize()</span><br><span class="line"><span class="keyword">print</span> (my_param.data(), my_param.grad())</span><br><span class="line"><span class="comment"># [[ 0.02332029  0.04696382  0.03078182]</span></span><br><span class="line"><span class="comment">#  [ 0.00755873  0.03193929 -0.0059346 ]</span></span><br><span class="line"><span class="comment">#  [-0.00809445  0.01710822 -0.03057443]]</span></span><br><span class="line"><span class="comment"># [[ 0.  0.  0.]</span></span><br><span class="line"><span class="comment">#  [ 0.  0.  0.]</span></span><br><span class="line"><span class="comment">#  [ 0.  0.  0.]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通常自定义层的时候我们不会直接创建Parameter，而是用过Block自带的一个ParamterDict类型的成员变量`params`，顾名思义，</span></span><br><span class="line"><span class="comment"># 这是一个由字符串名字映射到Parameter的字典。</span></span><br><span class="line">pd = gluon.ParameterDict(prefix=<span class="string">"block1_"</span>)</span><br><span class="line">pd.get(<span class="string">"exciting_parameter_yay"</span>, shape=(<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line"><span class="keyword">print</span> pd</span><br><span class="line"><span class="comment">#block1_ (Parameter block1_exciting_parameter_yay (shape=(3, 3), dtype=&lt;type 'numpy.float32'&gt;))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 现在我们看下如果如果实现一个跟`Dense`一样功能的层，它概念跟前面的`CenteredLayer`的主要区别是我们</span></span><br><span class="line"><span class="comment"># 在初始函数里通过`params`创建了参数：</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDense</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, units, in_units, **kwargs)</span>:</span><span class="comment">#in_units输入大小 units输出大小</span></span><br><span class="line">        super(MyDense, self).__init__(**kwargs)</span><br><span class="line">        <span class="keyword">with</span> self.name_scope():</span><br><span class="line">            self.weight = self.params.get(</span><br><span class="line">                <span class="string">'weight'</span>, shape=(in_units, units))</span><br><span class="line">            self.bias = self.params.get(<span class="string">'bias'</span>, shape=(units,))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        linear = nd.dot(x, self.weight.data()) + self.bias.data() <span class="comment">##WX+b</span></span><br><span class="line">        <span class="keyword">return</span> nd.relu(linear)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们创建实例化一个对象来看下它的参数，这里我们特意加了前缀`prefix`，这是`nn.Block`初始化函数自带的参数。</span></span><br><span class="line">dense = MyDense(<span class="number">5</span>, in_units=<span class="number">10</span>, prefix=<span class="string">'o_my_dense_'</span>)</span><br><span class="line"><span class="keyword">print</span> dense.params</span><br><span class="line"></span><br><span class="line"><span class="comment"># 它的使用跟前面没有什么不一致：</span></span><br><span class="line">dense.initialize()</span><br><span class="line"><span class="keyword">print</span> dense(nd.random.uniform(shape=(<span class="number">2</span>,<span class="number">10</span>))) <span class="comment">#batch=2 长度=10输入 输出为batch不变 长度=5</span></span><br><span class="line"><span class="comment"># [[ 0.          0.          0.05519049  0.01345633  0.07244172]</span></span><br><span class="line"><span class="comment">#  [ 0.          0.          0.06741175  0.01634707  0.0257601 ]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们构造的层跟Gluon提供的层用起来没太多区别：</span></span><br><span class="line">net = nn.Sequential()</span><br><span class="line"><span class="keyword">with</span> net.name_scope():</span><br><span class="line">    net.add(MyDense(<span class="number">32</span>, in_units=<span class="number">64</span>))</span><br><span class="line">    net.add(MyDense(<span class="number">2</span>, in_units=<span class="number">32</span>))</span><br><span class="line">net.initialize()</span><br><span class="line"><span class="keyword">print</span> net(nd.random.uniform(shape=(<span class="number">2</span>,<span class="number">64</span>)))</span><br><span class="line"><span class="comment"># [[ 0.          0.        ]</span></span><br><span class="line"><span class="comment">#  [ 0.02434103  0.        ]]</span></span><br></pre></td></tr></table></figure></p>
<p>仔细的你可能还是注意到了，我们这里指定了输入的大小，而Gluon自带的<code>Dense</code>则无需如此。我们已经在前面节介绍过了这个延迟初始化如何使用。但如果实现一个这样的层我们将留到后面介绍了hybridize后。<br>现在我们知道了如何把前面手写过的层全部包装了Gluon能用的Block，之后再用到的时候就可以飞起来了！</p>
<p>练习</p>
<ol>
<li>怎么修改自定义层里参数的默认初始化函数。</li>
<li>(这个比较难），在一个代码Cell里面输入<code>nn.Dense??</code>，看看它是怎么实现的。为什么它就可以支持延迟初始化了。</li>
</ol>
<h2 id="3-5-dropout"><a href="#3-5-dropout" class="headerlink" title="3.5.dropout"></a>3.5.dropout</h2><p>丢弃法（Dropout）— 从0开始<br>前面我们介绍了多层神经网络，就是包含至少一个隐含层的网络。我们也介绍了正则法来应对过拟合问题。在深度学习中，一个常用的应对过拟合问题的方法叫做丢弃法（Dropout）。本节以多层神经网络为例，从0开始介绍丢弃法。<br>由于丢弃法的概念和实现非常容易，在本节中，我们先介绍丢弃法的概念以及它在现代神经网络中是如何实现的。然后我们一起探讨丢弃法的本质。</p>
<p>丢弃法的概念<br>在现代神经网络中，我们所指的丢弃法，通常是对输入层或者隐含层做以下操作：</p>
<ul>
<li>随机选择一部分该层的输出作为丢弃元素；</li>
<li>把丢弃元素乘以0；</li>
<li>把非丢弃元素拉伸。</li>
</ul>
<p>丢弃法的本质<br>了解了丢弃法的概念与实现，那你可能对它的本质产生了好奇。<br>如果你了解集成学习，你可能知道它在提升弱分类器准确率上的威力。一般来说，在集成学习里，我们可以对训练数据集有放回地采样若干次并分别训练若干个不同的分类器；测试时，把这些分类器的结果集成一下作为最终分类结果。<br>事实上，丢弃法在模拟集成学习。试想，一个使用了丢弃法的多层神经网络本质上是原始网络的子集（节点和边）。举个例子，它可能长这个样子。<br><img src="/../img/deeplearn/dropout.png" alt="img"></p>
<p>丢弃法的实现<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 丢弃法的实现很容易，例如像下面这样。这里的标量`drop_probability`</span></span><br><span class="line"><span class="comment"># 定义了一个`X`（`NDArray`类）中任何一个元素被丢弃的概率。-----元素置0</span></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout</span><span class="params">(X, drop_probability)</span>:</span></span><br><span class="line">    keep_probability = <span class="number">1</span> - drop_probability</span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= keep_probability &lt;= <span class="number">1</span></span><br><span class="line">    <span class="comment"># 这种情况下把全部元素都丢弃,全部元素置0。</span></span><br><span class="line">    <span class="keyword">if</span> keep_probability == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> X.zeros_like()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 随机选择一部分该层的输出作为丢弃元素。</span></span><br><span class="line">    <span class="comment">##随机产生产生0~1之间的数，小于keep_probability变1 大于keep_probability变0 之后乘以X就会随机将某些值变0</span></span><br><span class="line">    mask=nd.random.uniform(<span class="number">0</span>, <span class="number">1.0</span>, X.shape, ctx=X.context)&gt;<span class="number">1</span>-keep_probability  <span class="comment">#&gt;1-keep_probability等效于小于keep_probability</span></span><br><span class="line">	<span class="comment"># 保证 E[dropout(X)] == X 期望值不变</span></span><br><span class="line">    scale = <span class="number">1</span> / keep_probability</span><br><span class="line">    <span class="keyword">return</span> mask * X * scale</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们运行几个实例来验证一下。</span></span><br><span class="line">A = nd.arange(<span class="number">8</span>).reshape((<span class="number">2</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">print</span> dropout(A, <span class="number">0.0</span>) <span class="comment">#[[ 0.  1.  2.  3.][ 4.  5.  6.  7.]]</span></span><br><span class="line"><span class="keyword">print</span> dropout(A, <span class="number">0.5</span>) <span class="comment">#[[  0.   2.   4.   6.][  8.  10.   0.   0.]]</span></span><br><span class="line"><span class="keyword">print</span> dropout(A, <span class="number">1.0</span>) <span class="comment"># [[ 0.  0.  0.  0.] [ 0.  0.  0.  0.]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们在训练神经网络模型时一般随机采样一个批量的训练数据。丢弃法实质上是对每一个这样的数据集分别训练一个原神经网络子集的分类器。</span></span><br><span class="line"><span class="comment"># 与一般的集成学习不同，这里每个原神经网络子集的分类器用的是同一套参数。因此丢弃法只是在模拟集成学习。</span></span><br><span class="line"><span class="comment"># 我们刚刚强调了，原神经网络子集的分类器在不同的训练数据批量上训练并使用同一套参数。因此，使用丢弃法的神经网络实质上是对输入层和</span></span><br><span class="line"><span class="comment"># 隐含层的参数做了正则化：学到的参数使得原神经网络不同子集在训练数据上都尽可能表现良好。</span></span><br><span class="line"><span class="comment"># 下面我们动手实现一下在多层神经网络里加丢弃层。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 数据获取</span></span><br><span class="line"><span class="keyword">import</span> utils</span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_data, test_data = utils.load_data_fashion_mnist(batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 含两个隐藏层的多层感知机</span></span><br><span class="line"><span class="comment"># 这里我们定义一个包含两个隐含层的模型，两个隐含层都输出256个节点。我们定义激活函数Relu并直接使用Gluon提供的交叉熵损失函数。</span></span><br><span class="line">num_inputs = <span class="number">28</span>*<span class="number">28</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line">num_hidden1 = <span class="number">256</span></span><br><span class="line">num_hidden2 = <span class="number">256</span></span><br><span class="line">weight_scale = <span class="number">.01</span></span><br><span class="line"></span><br><span class="line">W1 = nd.random_normal(shape=(num_inputs, num_hidden1), scale=weight_scale)</span><br><span class="line">b1 = nd.zeros(num_hidden1)</span><br><span class="line">W2 = nd.random_normal(shape=(num_hidden1, num_hidden2), scale=weight_scale)</span><br><span class="line">b2 = nd.zeros(num_hidden2)</span><br><span class="line">W3 = nd.random_normal(shape=(num_hidden2, num_outputs), scale=weight_scale)</span><br><span class="line">b3 = nd.zeros(num_outputs)</span><br><span class="line">params = [W1, b1, W2, b2, W3, b3]</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">    param.attach_grad()</span><br><span class="line"></span><br><span class="line"><span class="comment">## 定义包含丢弃层的模型</span></span><br><span class="line"><span class="comment"># 我们的模型就是将层（全连接）和激活函数（Relu）串起来，并在应用激活函数后添加丢弃层。每个丢弃层的元素丢弃概率可以分别设置。</span></span><br><span class="line"><span class="comment"># 一般情况下，我们推荐把更靠近输入层的元素丢弃概率设的更小一点。这个试验中，我们把第一层全连接后的元素丢弃概率设为0.2，</span></span><br><span class="line"><span class="comment"># 把第二层全连接后的元素丢弃概率设为0.5。</span></span><br><span class="line">drop_prob1 = <span class="number">0.1</span></span><br><span class="line">drop_prob2 = <span class="number">0.4</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X)</span>:</span></span><br><span class="line">    X = X.reshape((-<span class="number">1</span>, num_inputs))</span><br><span class="line">    <span class="comment"># 第一层全连接。</span></span><br><span class="line">    h1 = nd.relu(nd.dot(X, W1) + b1)</span><br><span class="line">    <span class="comment"># 在第一层全连接后添加丢弃层。</span></span><br><span class="line">    h1 = dropout(h1, drop_prob1)</span><br><span class="line">    <span class="comment"># 第二层全连接。</span></span><br><span class="line">    h2 = nd.relu(nd.dot(h1, W2) + b2)</span><br><span class="line">    <span class="comment"># 在第二层全连接后添加丢弃层。</span></span><br><span class="line">    h2 = dropout(h2, drop_prob2)</span><br><span class="line">    <span class="keyword">return</span> nd.dot(h2, W3) + b3</span><br><span class="line"></span><br><span class="line"><span class="comment">## 训练</span></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line"></span><br><span class="line">softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()</span><br><span class="line">learning_rate = <span class="number">.5</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    train_loss = <span class="number">0.</span></span><br><span class="line">    train_acc = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_data:</span><br><span class="line">        <span class="keyword">with</span> autograd.record():</span><br><span class="line">            output = net(data)</span><br><span class="line">            loss = softmax_cross_entropy(output, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        utils.SGD(params, learning_rate/batch_size)</span><br><span class="line"></span><br><span class="line">        train_loss += nd.mean(loss).asscalar()</span><br><span class="line">        train_acc += utils.accuracy(output, label)</span><br><span class="line"></span><br><span class="line">    test_acc = utils.evaluate_accuracy(test_data, net)</span><br><span class="line">    print(<span class="string">"Epoch %d. Loss: %f, Train acc %f, Test acc %f"</span> % (</span><br><span class="line">        epoch, train_loss/len(train_data),</span><br><span class="line">        train_acc/len(train_data), test_acc)</span><br></pre></td></tr></table></figure></p>
<p>我们可以通过使用丢弃法对神经网络正则化。<br>练习</p>
<ul>
<li>尝试不使用丢弃法，看看这个包含两个隐含层的多层感知机可以得到什么结果。</li>
<li>我们推荐把更靠近输入层的元素丢弃概率设的更小一点。想想这是为什么？如果把本节教程中的两个元素丢弃参数对调会有什么结果？</li>
</ul>
<h2 id="3-6-_u4F7F_u7528Gluon_u4E22_u5F03_u6CD5_uFF08Dropout_uFF09"><a href="#3-6-_u4F7F_u7528Gluon_u4E22_u5F03_u6CD5_uFF08Dropout_uFF09" class="headerlink" title="3.6.使用Gluon丢弃法（Dropout）"></a>3.6.使用Gluon丢弃法（Dropout）</h2><p>本章介绍如何使用<code>Gluon</code>在训练和测试深度学习模型中使用丢弃法(Dropout)。<br>定义模型并添加丢弃层<br>有了<code>Gluon</code>，我们模型的定义工作变得简单了许多。我们只需要在全连接层后添加<code>gluon.nn.Dropout</code>层并指定元素丢弃概率。一般情况下，我们推荐把<br>更靠近输入层的元素丢弃概率设的更小一点。这个试验中，我们把第一层全连接后的元素丢弃概率设为0.2，把第二层全连接后的元素丢弃概率设为0.5。<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">from</span> mxnet.gluon import nn</span><br><span class="line"></span><br><span class="line">net = nn.Sequential()</span><br><span class="line">drop_prob1 = <span class="number">0.2</span></span><br><span class="line">drop_prob2 = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="operator">with</span> net.name_scope():</span><br><span class="line">    net.<span class="built_in">add</span>(nn.Flatten())</span><br><span class="line">    <span class="comment"># 第一层全连接。</span></span><br><span class="line">    net.<span class="built_in">add</span>(nn.Dense(<span class="number">256</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">    <span class="comment"># 在第一层全连接后添加丢弃层。</span></span><br><span class="line">    net.<span class="built_in">add</span>(nn.Dropout(drop_prob1))<span class="comment">########通过`Gluon`我们可以更方便地构造多层神经网络并使用丢弃法</span></span><br><span class="line">    <span class="comment"># 第二层全连接。</span></span><br><span class="line">    net.<span class="built_in">add</span>(nn.Dense(<span class="number">256</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">    <span class="comment"># 在第二层全连接后添加丢弃层。</span></span><br><span class="line">    net.<span class="built_in">add</span>(nn.Dropout(drop_prob2))</span><br><span class="line">    net.<span class="built_in">add</span>(nn.Dense(<span class="number">10</span>))</span><br><span class="line">net.initialize()</span><br><span class="line"></span><br><span class="line"><span class="comment">## 读取数据并训练</span></span><br><span class="line">import utils</span><br><span class="line"><span class="built_in">from</span> mxnet import nd</span><br><span class="line"><span class="built_in">from</span> mxnet import autograd</span><br><span class="line"><span class="built_in">from</span> mxnet import gluon</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_data, test_data = utils.load_data_fashion_mnist(batch_size)</span><br><span class="line">softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()</span><br><span class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.5</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="operator">in</span> range(<span class="number">5</span>):</span><br><span class="line">    train_loss = <span class="number">0.</span></span><br><span class="line">    train_acc = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">for</span> data, label <span class="operator">in</span> train_data:</span><br><span class="line">        <span class="operator">with</span> autograd.record():</span><br><span class="line">            output = net(data)</span><br><span class="line">            loss = softmax_cross_entropy(output, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        trainer.step(batch_size)</span><br><span class="line"></span><br><span class="line">        train_loss += nd.mean(loss).asscalar()</span><br><span class="line">        train_acc += utils.accuracy(output, label)</span><br><span class="line"></span><br><span class="line">    test_acc = utils.evaluate_accuracy(test_data, net)</span><br><span class="line">    print(<span class="string">"Epoch %d. Loss: %f, Train acc %f, Test acc %f"</span> % (</span><br><span class="line">        epoch, train_loss/<span class="built_in">len</span>(train_data),</span><br><span class="line">        train_acc/<span class="built_in">len</span>(train_data), test_acc))</span><br></pre></td></tr></table></figure></p>
<p>效果比3.5好！</p>
<h2 id="3-7-_u6DF1_u5EA6_u5377_u79EF_u795E_u7ECF_u7F51_u7EDC_u548CAlexNet"><a href="#3-7-_u6DF1_u5EA6_u5377_u79EF_u795E_u7ECF_u7F51_u7EDC_u548CAlexNet" class="headerlink" title="3.7.深度卷积神经网络和AlexNet"></a>3.7.深度卷积神经网络和AlexNet</h2><p>原版的AlexNet有每层大小为4096个节点的全连接层们。这两个巨大的全连接层带来将近1GB的模型大小。由于早期GPU显存的限制，最早的AlexNet包括了双数据流的设计，以让网络中一半的节点能存入一个GPU。这两个数据流，也就是说两个GPU只在一部分层进行通信，这样达到限制GPU同步时的额外开销的效果。有幸的是，GPU在过去几年得到了长足的发展，除了一些特殊的结构外，我们也就不再需要这样的特别设计了。<br>下面的Gluon代码定义了（稍微简化过的）Alexnet<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">net = nn.Sequential()</span><br><span class="line"><span class="keyword">with</span> net.name_scope():</span><br><span class="line">    net.add(</span><br><span class="line">        <span class="comment"># 第一阶段</span></span><br><span class="line">        nn.Conv2D(channels=<span class="number">96</span>, kernel_size=<span class="number">11</span>,strides=<span class="number">4</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">        nn.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>),</span><br><span class="line">        <span class="comment"># 第二阶段</span></span><br><span class="line">        nn.Conv2D(channels=<span class="number">256</span>, kernel_size=<span class="number">5</span>,padding=<span class="number">2</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">        nn.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>),</span><br><span class="line">        <span class="comment"># 第三阶段</span></span><br><span class="line">        nn.Conv2D(channels=<span class="number">384</span>, kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">        nn.Conv2D(channels=<span class="number">384</span>, kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">        nn.Conv2D(channels=<span class="number">256</span>, kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">        nn.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>),</span><br><span class="line">        <span class="comment"># 第四阶段</span></span><br><span class="line">        nn.Flatten(),</span><br><span class="line">        nn.Dense(<span class="number">4096</span>, activation=<span class="string">"relu"</span>),</span><br><span class="line">        nn.Dropout(<span class="number">.5</span>),</span><br><span class="line">        <span class="comment"># 第五阶段</span></span><br><span class="line">        nn.Dense(<span class="number">4096</span>, activation=<span class="string">"relu"</span>),</span><br><span class="line">        nn.Dropout(<span class="number">.5</span>),</span><br><span class="line">        <span class="comment"># 第六阶段</span></span><br><span class="line">        nn.Dense(<span class="number">10</span>) <span class="comment">##当时是1000</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment">## 读取数据</span></span><br><span class="line"><span class="comment"># Alexnet使用Imagenet数据，其中输入图片大小一般是224x224。因为Imagenet数据训练时间过长，</span></span><br><span class="line"><span class="comment"># 我们还是用前面的MNIST来演示。读取数据的时候我们额外做了一步将数据扩大到原版Alexnet使用的224x224。</span></span><br><span class="line"><span class="keyword">import</span> utils</span><br><span class="line">train_data, test_data = utils.load_data_fashion_mnist(batch_size=<span class="number">64</span>, resize=<span class="number">224</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 训练</span></span><br><span class="line"><span class="comment"># 这时候我们可以开始训练。相对于前面的LeNet，我们做了如下三个改动：</span></span><br><span class="line"><span class="comment"># 1. 我们使用`Xavier`来初始化参数</span></span><br><span class="line"><span class="comment"># 2. 使用了更小的学习率</span></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"></span><br><span class="line">ctx = mx.gpu()</span><br><span class="line">net.initialize(ctx=ctx, init=init.Xavier()) <span class="comment">##初始化用了Xavier()而不是默认的随机初始化</span></span><br><span class="line"></span><br><span class="line">loss = gluon.loss.SoftmaxCrossEntropyLoss()</span><br><span class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.01</span>&#125;)</span><br><span class="line">utils.train(train_data, test_data, net, loss,trainer, ctx, num_epochs=<span class="number">5</span>)</span><br></pre></td></tr></table></figure></p>
<p>效果比以前的好些!</p>
<h2 id="3-8-VGGNet"><a href="#3-8-VGGNet" class="headerlink" title="3.8.VGGNet"></a>3.8.VGGNet</h2><p>我们从Alexnet看到网络的层数的激增。这个意味着即使是用Gluon手动写代码一层一层的堆每一层也很麻烦，更不用说从0开始了。幸运的是编程语言提供了很好的方法来解决这个问题：函数和循环。如果网络结构里面有大量重复结构，那么我们可以很紧凑来构造这些网络。第一个使用这种结构的深度网络是VGG。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># VGG的一个关键是使用很多有着相对小的kernel（3x3）的卷积层然后接上一个池化层，</span></span><br><span class="line"><span class="comment"># 之后再将这个模块重复多次。下面我们先定义一个这样的块</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg_block</span><span class="params">(num_convs, channels)</span>:</span></span><br><span class="line">    out = nn.Sequential()</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_convs):</span><br><span class="line">        out.add(nn.Conv2D(channels=channels, kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">    out.add(nn.MaxPool2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们实例化一个这样的块，里面有两个卷积层，每个卷积层输出通道是128：</span></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"></span><br><span class="line">blk = vgg_block(<span class="number">2</span>, <span class="number">128</span>)</span><br><span class="line">blk.initialize()</span><br><span class="line">x = nd.random.uniform(shape=(<span class="number">2</span>,<span class="number">3</span>,<span class="number">16</span>,<span class="number">16</span>))</span><br><span class="line">y = blk(x)</span><br><span class="line"><span class="keyword">print</span> y.shape  <span class="comment">##(2L, 128L, 8L, 8L)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以看到经过一个这样的块后，长宽会减半，通道也会改变。然后我们定义如何将这些块堆起来：</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg_stack</span><span class="params">(architecture)</span>:</span></span><br><span class="line">    out = nn.Sequential()</span><br><span class="line">    <span class="keyword">for</span> (num_convs, channels) <span class="keyword">in</span> architecture:</span><br><span class="line">        out.add(vgg_block(num_convs, channels))</span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里我们定义一个最简单的一个VGG结构，它有8个卷积层，和跟Alexnet一样的3个全连接层。这个网络又称VGG 11.</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line">architecture = ((<span class="number">1</span>,<span class="number">64</span>), (<span class="number">1</span>,<span class="number">128</span>), (<span class="number">2</span>,<span class="number">256</span>), (<span class="number">2</span>,<span class="number">512</span>), (<span class="number">2</span>,<span class="number">512</span>))</span><br><span class="line">net = nn.Sequential()</span><br><span class="line"><span class="keyword">with</span> net.name_scope():</span><br><span class="line">    net.add(</span><br><span class="line">        vgg_stack(architecture),</span><br><span class="line">        nn.Flatten(),</span><br><span class="line">        nn.Dense(<span class="number">4096</span>, activation=<span class="string">"relu"</span>),</span><br><span class="line">        nn.Dropout(<span class="number">.5</span>),</span><br><span class="line">        nn.Dense(<span class="number">4096</span>, activation=<span class="string">"relu"</span>),</span><br><span class="line">        nn.Dropout(<span class="number">.5</span>),</span><br><span class="line">        nn.Dense(num_outputs))</span><br><span class="line"><span class="keyword">print</span> net</span><br><span class="line"><span class="comment"># Sequential(</span></span><br><span class="line"><span class="comment">#   (0): Sequential(</span></span><br><span class="line"><span class="comment">#     (0): Sequential(</span></span><br><span class="line"><span class="comment">#       (0): Conv2D(64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span></span><br><span class="line"><span class="comment">#       (1): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)</span></span><br><span class="line"><span class="comment">#     )</span></span><br><span class="line"><span class="comment">#     (1): Sequential(</span></span><br><span class="line"><span class="comment">#       (0): Conv2D(128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span></span><br><span class="line"><span class="comment">#       (1): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)</span></span><br><span class="line"><span class="comment">#     )</span></span><br><span class="line"><span class="comment">#     (2): Sequential(</span></span><br><span class="line"><span class="comment">#       (0): Conv2D(256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span></span><br><span class="line"><span class="comment">#       (1): Conv2D(256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span></span><br><span class="line"><span class="comment">#       (2): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)</span></span><br><span class="line"><span class="comment">#     )</span></span><br><span class="line"><span class="comment">#     (3): Sequential(</span></span><br><span class="line"><span class="comment">#       (0): Conv2D(512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span></span><br><span class="line"><span class="comment">#       (1): Conv2D(512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span></span><br><span class="line"><span class="comment">#       (2): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)</span></span><br><span class="line"><span class="comment">#     )</span></span><br><span class="line"><span class="comment">#     (4): Sequential(</span></span><br><span class="line"><span class="comment">#       (0): Conv2D(512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span></span><br><span class="line"><span class="comment">#       (1): Conv2D(512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span></span><br><span class="line"><span class="comment">#       (2): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)</span></span><br><span class="line"><span class="comment">#     )</span></span><br><span class="line"><span class="comment">#   )</span></span><br><span class="line"><span class="comment">#   (1): Flatten</span></span><br><span class="line"><span class="comment">#   (2): Dense(4096, Activation(relu))</span></span><br><span class="line"><span class="comment">#   (3): Dropout(p = 0.5)</span></span><br><span class="line"><span class="comment">#   (4): Dense(4096, Activation(relu))</span></span><br><span class="line"><span class="comment">#   (5): Dropout(p = 0.5)</span></span><br><span class="line"><span class="comment">#   (6): Dense(10, linear)</span></span><br><span class="line"><span class="comment"># )</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 模型训练</span></span><br><span class="line"><span class="comment"># 这里跟Alexnet的训练代码一样除了我们只将图片扩大到96x96来节省些计算，和默认使用稍微大点的学习率。</span></span><br><span class="line"><span class="keyword">import</span> utils</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"></span><br><span class="line">train_data, test_data = utils.load_data_fashion_mnist(batch_size=<span class="number">64</span>, resize=<span class="number">96</span>)</span><br><span class="line">ctx = mx.gpu()</span><br><span class="line">net.initialize(ctx=ctx, init=init.Xavier())</span><br><span class="line">loss = gluon.loss.SoftmaxCrossEntropyLoss()</span><br><span class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.05</span>&#125;)</span><br><span class="line">utils.train(train_data, test_data, net, loss,trainer, ctx, num_epochs=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p>
<p>#通过使用重复的元素，我们可以通过循环和函数来定义模型。使用不同的配置(<code>architecture</code>)可以得到一系列不同的模型。<br>练习</p>
<ul>
<li>尝试多跑几轮，看看跟LeNet/Alexnet比怎么样？</li>
<li>尝试下构造VGG其他常用模型，例如VGG16， VGG19. （提示：可以参考<a href="https://arxiv.org/abs/1409.1556" target="_blank" rel="external">VGG论文</a>里的表1。）</li>
<li>把图片从默认的224x224降到96x96有什么影响？</li>
</ul>
<h2 id="4-1-__u4ECE0_u5F00_u59CB_u6279_u91CF_u5F52_u4E00_u5316BatchNorm"><a href="#4-1-__u4ECE0_u5F00_u59CB_u6279_u91CF_u5F52_u4E00_u5316BatchNorm" class="headerlink" title="4.1. 从0开始批量归一化BatchNorm"></a>4.1. 从0开始批量归一化BatchNorm</h2><p>在实际应用中，我们通常将输入数据的每个样本或者每个特征进行归一化，就是将均值变为0方差变为1，来使得数值更稳定。<br>它对很深的神经网络能够训练，对learningrate不那么敏感。<br>批量归一化对每层都归一化。<br>批量归一化试图对深度学习模型的某一层所使用的激活函数的输入进行归一化：使批量呈标准正态分布（均值为0，标准差为1）。<br>批量归一化通常应用于输入层或任意中间层。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们现在来动手实现一个简化的批量归一化层。实现时对全连接层和二维卷积层两种情况做了区分。</span></span><br><span class="line"><span class="comment"># 对于全连接层，很明显我们要对每个批量进行归一化。然而这里需要注意的是，对于二维卷积，我们要对每个通道进行归一化，</span></span><br><span class="line"><span class="comment"># 并需要保持四维形状使得可以正确地广播。</span></span><br><span class="line"><span class="comment">#均值变0 方差变1</span></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pure_batch_norm</span><span class="params">(X, gamma, beta, eps=<span class="number">1e-5</span>)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> len(X.shape) <span class="keyword">in</span> (<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">    <span class="comment"># 全连接: batch_size x feature</span></span><br><span class="line">    <span class="keyword">if</span> len(X.shape) == <span class="number">2</span>:<span class="comment">#全连接情况</span></span><br><span class="line">        <span class="comment"># 每个输入维度在样本上的平均和方差</span></span><br><span class="line">        mean = X.mean(axis=<span class="number">0</span>)</span><br><span class="line">        variance = ((X - mean)**<span class="number">2</span>).mean(axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 2D卷积: batch_size x channel x height x width</span></span><br><span class="line">    <span class="keyword">else</span>: <span class="comment">#卷积情况</span></span><br><span class="line">        <span class="comment"># 对每个通道算均值和方差，需要保持4D形状使得可以正确地广播</span></span><br><span class="line">        mean = X.mean(axis=(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>), keepdims=<span class="keyword">True</span>)</span><br><span class="line">        variance = ((X - mean)**<span class="number">2</span>).mean(axis=(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>), keepdims=<span class="keyword">True</span>) <span class="comment">##rgb的话通道为3</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 均一化</span></span><br><span class="line">    X_hat = (X - mean) / nd.sqrt(variance + eps)</span><br><span class="line">    <span class="comment"># reshape 拉升和偏移</span></span><br><span class="line">    <span class="comment">##Y=gamma*X（均一化后的数据）+beta 不想归一化时可以利用训练的gamma，beta还原数据</span></span><br><span class="line">    <span class="keyword">return</span> gamma.reshape(mean.shape) * X_hat + beta.reshape(mean.shape)</span><br><span class="line"></span><br><span class="line">X=mx.ndarray.random_normal(shape=(<span class="number">2</span>,<span class="number">3</span>,<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line"><span class="keyword">print</span> X</span><br><span class="line"><span class="keyword">print</span> X.mean(axis=(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>),keepdims=<span class="keyword">True</span>) <span class="comment">#[[[[ 0.04687225]] [[-0.05943033]] [[-0.2027849 ]]]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面我们检查一下。我们先定义全连接层的输入是这样的。每一行是批量中的一个实例。</span></span><br><span class="line">A = nd.arange(<span class="number">6</span>).reshape((<span class="number">3</span>,<span class="number">2</span>))</span><br><span class="line"><span class="keyword">print</span> A <span class="comment">#[[ 0.  1.][ 2.  3.][ 4.  5.]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们希望批量中的  每一列  都被归一化。结果符合预期。</span></span><br><span class="line"><span class="comment">## ？？？？全部一起均一化呢？？？</span></span><br><span class="line"><span class="keyword">print</span> pure_batch_norm(A, gamma=nd.array([<span class="number">1</span>,<span class="number">1</span>]), beta=nd.array([<span class="number">0</span>,<span class="number">0</span>]))</span><br><span class="line"><span class="comment">#[[-1.22474265 -1.22474265][ 0.          0.        ] [ 1.22474265  1.22474265]]##均值0 方差1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面我们定义二维卷积网络层的输入是这样的。</span></span><br><span class="line">B = nd.arange(<span class="number">18</span>).reshape((<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line"><span class="keyword">print</span> B</span><br><span class="line"><span class="comment"># 结果也如预期那样，我们对每个通道做了归一化。</span></span><br><span class="line"><span class="keyword">print</span> pure_batch_norm(B, gamma=nd.array([<span class="number">1</span>,<span class="number">1</span>]), beta=nd.array([<span class="number">0</span>,<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 批量归一化层</span></span><br><span class="line"><span class="comment"># 你可能会想，既然训练时用了批量归一化，那么测试时也该用批量归一化吗？其实这个问题乍一想不是很好回答，因为：</span></span><br><span class="line"><span class="comment"># * 不用的话，训练出的模型参数很可能在测试时就不准确了；</span></span><br><span class="line"><span class="comment"># * 用的话，万一测试的数据就只有一个数据实例就不好办了。</span></span><br><span class="line"><span class="comment"># 事实上，在测试时我们还是需要继续使用批量归一化的，只是需要做些改动。</span></span><br><span class="line"><span class="comment"># 在测试时，我们需要把原先训练时用到的批量均值和方差替换成**整个**训练数据的均值和方差。</span></span><br><span class="line"><span class="comment"># 但是当训练数据极大时，这个计算开销很大。因此，我们用移动平均的方法来近似计算</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了方便讨论批量归一化层的实现，我们先看下面这段代码来理解``Python``变量可以如何修改。</span></span><br><span class="line"><span class="comment">## 拿到全部训练数据的均值方差供测试数据用</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_norm</span><span class="params">(X, gamma, beta, is_training, moving_mean, moving_variance,</span><br><span class="line">               eps = <span class="number">1e-5</span>, moving_momentum = <span class="number">0.9</span>)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> len(X.shape) <span class="keyword">in</span> (<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">    <span class="comment"># 全连接: batch_size x feature</span></span><br><span class="line">    <span class="keyword">if</span> len(X.shape) == <span class="number">2</span>:</span><br><span class="line">        <span class="comment"># 每个输入维度在样本上的平均和方差</span></span><br><span class="line">        mean = X.mean(axis=<span class="number">0</span>)</span><br><span class="line">        variance = ((X - mean)**<span class="number">2</span>).mean(axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 2D卷积: batch_size x channel x height x width</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 对每个通道算均值和方差，需要保持4D形状使得可以正确的广播</span></span><br><span class="line">        mean = X.mean(axis=(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>), keepdims=<span class="keyword">True</span>)</span><br><span class="line">        variance = ((X - mean)**<span class="number">2</span>).mean(axis=(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>), keepdims=<span class="keyword">True</span>)</span><br><span class="line">        <span class="comment"># 变形使得可以正确的广播</span></span><br><span class="line">        moving_mean = moving_mean.reshape(mean.shape)</span><br><span class="line">        moving_variance = moving_variance.reshape(mean.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 均一化</span></span><br><span class="line">    <span class="keyword">if</span> is_training:</span><br><span class="line">        X_hat = (X - mean) / nd.sqrt(variance + eps)</span><br><span class="line">        <span class="comment">#!!! 更新全局的均值和方差</span></span><br><span class="line">        moving_mean[:] = moving_momentum * moving_mean + (</span><br><span class="line">            <span class="number">1.0</span> - moving_momentum) * mean</span><br><span class="line">        moving_variance[:] = moving_momentum * moving_variance + (</span><br><span class="line">            <span class="number">1.0</span> - moving_momentum) * variance</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment">#!!! 测试阶段使用全局的均值和方差</span></span><br><span class="line">        X_hat = (X - moving_mean) / nd.sqrt(moving_variance + eps)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 拉升和偏移</span></span><br><span class="line">    <span class="keyword">return</span> gamma.reshape(mean.shape) * X_hat + beta.reshape(mean.shape)</span><br><span class="line"></span><br><span class="line">ctx = mx.gpu()</span><br><span class="line">weight_scale = <span class="number">.01</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># output channels = 20, kernel = (5,5)</span></span><br><span class="line">c1 = <span class="number">20</span></span><br><span class="line">W1 = nd.random.normal(shape=(c1,<span class="number">1</span>,<span class="number">5</span>,<span class="number">5</span>), scale=weight_scale, ctx=ctx)</span><br><span class="line">b1 = nd.zeros(c1, ctx=ctx)</span><br><span class="line"><span class="comment"># batch norm 1  gamma beta也是需要学</span></span><br><span class="line">gamma1 = nd.random.normal(shape=c1, scale=weight_scale, ctx=ctx)</span><br><span class="line">beta1 = nd.random.normal(shape=c1, scale=weight_scale, ctx=ctx)</span><br><span class="line">moving_mean1 = nd.zeros(c1, ctx=ctx)</span><br><span class="line">moving_variance1 = nd.zeros(c1, ctx=ctx)</span><br><span class="line"><span class="comment"># output channels = 50, kernel = (3,3)</span></span><br><span class="line">c2 = <span class="number">50</span></span><br><span class="line">W2 = nd.random_normal(shape=(c2,c1,<span class="number">3</span>,<span class="number">3</span>), scale=weight_scale, ctx=ctx)</span><br><span class="line">b2 = nd.zeros(c2, ctx=ctx)</span><br><span class="line"><span class="comment"># batch norm 2</span></span><br><span class="line">gamma2 = nd.random.normal(shape=c2, scale=weight_scale, ctx=ctx)</span><br><span class="line">beta2 = nd.random.normal(shape=c2, scale=weight_scale, ctx=ctx)</span><br><span class="line">moving_mean2 = nd.zeros(c2, ctx=ctx)</span><br><span class="line">moving_variance2 = nd.zeros(c2, ctx=ctx)</span><br><span class="line"><span class="comment"># output dim = 128</span></span><br><span class="line">o3 = <span class="number">128</span></span><br><span class="line">W3 = nd.random.normal(shape=(<span class="number">1250</span>, o3), scale=weight_scale, ctx=ctx)</span><br><span class="line">b3 = nd.zeros(o3, ctx=ctx)</span><br><span class="line"><span class="comment"># output dim = 10</span></span><br><span class="line">W4 = nd.random_normal(shape=(W3.shape[<span class="number">1</span>], <span class="number">10</span>), scale=weight_scale, ctx=ctx)</span><br><span class="line">b4 = nd.zeros(W4.shape[<span class="number">1</span>], ctx=ctx)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意这里moving_*是不需要更新的</span></span><br><span class="line">params = [W1, b1, gamma1, beta1,</span><br><span class="line">          W2, b2, gamma2, beta2,</span><br><span class="line">          W3, b3, W4, b4]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">    param.attach_grad()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面定义模型。我们添加了批量归一化层。特别要注意我们添加的位置：在卷积层后，在激活函数前。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X, is_training=False, verbose=False)</span>:</span></span><br><span class="line">    X = X.as_in_context(W1.context) <span class="comment">##GPU or CPU</span></span><br><span class="line">    <span class="comment"># 第一层卷积</span></span><br><span class="line">    h1_conv = nd.Convolution(data=X, weight=W1, bias=b1, kernel=W1.shape[<span class="number">2</span>:], num_filter=c1)</span><br><span class="line">    <span class="comment">### 添加了批量归一化层</span></span><br><span class="line">    h1_bn = batch_norm(h1_conv, gamma1, beta1, is_training,moving_mean1, moving_variance1)</span><br><span class="line">    h1_activation = nd.relu(h1_bn)</span><br><span class="line">    h1 = nd.Pooling(data=h1_activation, pool_type=<span class="string">"max"</span>, kernel=(<span class="number">2</span>,<span class="number">2</span>), stride=(<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">    <span class="comment"># 第二层卷积</span></span><br><span class="line">    h2_conv = nd.Convolution(data=h1, weight=W2, bias=b2, kernel=W2.shape[<span class="number">2</span>:], num_filter=c2)</span><br><span class="line">    <span class="comment">### 添加了批量归一化层</span></span><br><span class="line">    h2_bn = batch_norm(h2_conv, gamma2, beta2, is_training,moving_mean2, moving_variance2)</span><br><span class="line">    h2_activation = nd.relu(h2_bn)</span><br><span class="line">    h2 = nd.Pooling(data=h2_activation, pool_type=<span class="string">"max"</span>, kernel=(<span class="number">2</span>,<span class="number">2</span>), stride=(<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">    h2 = nd.flatten(h2)</span><br><span class="line">    <span class="comment"># 第一层全连接</span></span><br><span class="line">    h3_linear = nd.dot(h2, W3) + b3</span><br><span class="line">    h3 = nd.relu(h3_linear)</span><br><span class="line">    <span class="comment"># 第二层全连接</span></span><br><span class="line">    h4_linear = nd.dot(h3, W4) + b4</span><br><span class="line">    <span class="keyword">if</span> verbose:</span><br><span class="line">        print(<span class="string">'1st conv block:'</span>, h1.shape)</span><br><span class="line">        print(<span class="string">'2nd conv block:'</span>, h2.shape)</span><br><span class="line">        print(<span class="string">'1st dense:'</span>, h3.shape)</span><br><span class="line">        print(<span class="string">'2nd dense:'</span>, h4_linear.shape)</span><br><span class="line">        print(<span class="string">'output:'</span>, h4_linear)</span><br><span class="line">    <span class="keyword">return</span> h4_linear</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面我们训练并测试模型。</span></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line"><span class="keyword">import</span> utils</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_data, test_data = utils.load_data_fashion_mnist(batch_size)</span><br><span class="line">softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()</span><br><span class="line">learning_rate = <span class="number">0.2</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    train_loss = <span class="number">0.</span></span><br><span class="line">    train_acc = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_data:</span><br><span class="line">        label = label.as_in_context(ctx)</span><br><span class="line">        <span class="keyword">with</span> autograd.record():</span><br><span class="line">            output = net(data, is_training=<span class="keyword">True</span>)</span><br><span class="line">            loss = softmax_cross_entropy(output, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        utils.SGD(params, learning_rate/batch_size)</span><br><span class="line"></span><br><span class="line">        train_loss += nd.mean(loss).asscalar()</span><br><span class="line">        train_acc += utils.accuracy(output, label)</span><br><span class="line"></span><br><span class="line">    test_acc = utils.evaluate_accuracy(test_data, net, ctx)</span><br><span class="line">    print(<span class="string">"Epoch %d. Loss: %f, Train acc %f, Test acc %f"</span> % (</span><br><span class="line">            epoch, train_loss/len(train_data), train_acc/len(train_data), test_acc))</span><br></pre></td></tr></table></figure></p>
<h2 id="4-2-gluon_u7248batchnorm"><a href="#4-2-gluon_u7248batchnorm" class="headerlink" title="4.2.gluon版batchnorm"></a>4.2.gluon版batchnorm</h2><p>使用Gluon我们可以很轻松地添加批量归一化层。<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 有了`Gluon`，我们模型的定义工作变得简单了许多。我们只需要添加`nn.BatchNorm`层并指定对二维卷积的通道(`axis=1`)进行批量归一化。</span></span><br><span class="line"><span class="built_in">from</span> mxnet.gluon import nn</span><br><span class="line"></span><br><span class="line">net = nn.Sequential()</span><br><span class="line"><span class="operator">with</span> net.name_scope():</span><br><span class="line">    <span class="comment"># 第一层卷积</span></span><br><span class="line">    net.<span class="built_in">add</span>(nn.Conv2D(channels=<span class="number">20</span>, kernel_size=<span class="number">5</span>))</span><br><span class="line">    <span class="comment">### 添加了批量归一化层</span></span><br><span class="line">    net.<span class="built_in">add</span>(nn.BatchNorm(axis=<span class="number">1</span>))<span class="comment">##########################</span></span><br><span class="line">    net.<span class="built_in">add</span>(nn.Activation(activation=<span class="string">'relu'</span>))</span><br><span class="line">    net.<span class="built_in">add</span>(nn.MaxPool2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>))</span><br><span class="line">    <span class="comment"># 第二层卷积</span></span><br><span class="line">    net.<span class="built_in">add</span>(nn.Conv2D(channels=<span class="number">50</span>, kernel_size=<span class="number">3</span>))</span><br><span class="line">    <span class="comment">### 添加了批量归一化层</span></span><br><span class="line">    net.<span class="built_in">add</span>(nn.BatchNorm(axis=<span class="number">1</span>))</span><br><span class="line">    net.<span class="built_in">add</span>(nn.Activation(activation=<span class="string">'relu'</span>))</span><br><span class="line">    net.<span class="built_in">add</span>(nn.MaxPool2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>))</span><br><span class="line">    net.<span class="built_in">add</span>(nn.Flatten())</span><br><span class="line">    <span class="comment"># 第一层全连接</span></span><br><span class="line">    net.<span class="built_in">add</span>(nn.Dense(<span class="number">128</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">    <span class="comment"># 第二层全连接</span></span><br><span class="line">    net.<span class="built_in">add</span>(nn.Dense(<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">## 模型训练</span></span><br><span class="line"><span class="comment"># 剩下的代码跟之前没什么不一样。</span></span><br><span class="line">import utils</span><br><span class="line"><span class="built_in">from</span> mxnet import autograd</span><br><span class="line"><span class="built_in">from</span> mxnet import gluon</span><br><span class="line"><span class="built_in">from</span> mxnet import nd</span><br><span class="line">import mxnet <span class="keyword">as</span> mx</span><br><span class="line"><span class="comment"># from mxnet import init</span></span><br><span class="line"></span><br><span class="line">ctx = mx.gpu()</span><br><span class="line">net.initialize(ctx=ctx)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_data, test_data = utils.load_data_fashion_mnist(batch_size)</span><br><span class="line"></span><br><span class="line">softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()</span><br><span class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.2</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="operator">in</span> range(<span class="number">5</span>):</span><br><span class="line">    train_loss = <span class="number">0.</span></span><br><span class="line">    train_acc = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">for</span> data, label <span class="operator">in</span> train_data:</span><br><span class="line">        label = label.as_in_context(ctx)</span><br><span class="line">        <span class="operator">with</span> autograd.record():</span><br><span class="line">            output = net(data.as_in_context(ctx))</span><br><span class="line">            loss = softmax_cross_entropy(output, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        trainer.step(batch_size)</span><br><span class="line"></span><br><span class="line">        train_loss += nd.mean(loss).asscalar()</span><br><span class="line">        train_acc += utils.accuracy(output, label)</span><br><span class="line">    test_acc = utils.evaluate_accuracy(test_data, net, ctx)</span><br><span class="line">    print(<span class="string">"Epoch %d. Loss: %f, Train acc %f, Test acc %f"</span> % (</span><br><span class="line">        epoch, train_loss/<span class="built_in">len</span>(train_data),</span><br><span class="line">        train_acc/<span class="built_in">len</span>(train_data), test_acc))</span><br></pre></td></tr></table></figure></p>
<h2 id="4-3-_u7F51_u7EDC_u4E2D_u7684_u7F51_u7EDCNIN"><a href="#4-3-_u7F51_u7EDC_u4E2D_u7684_u7F51_u7EDCNIN" class="headerlink" title="4.3.网络中的网络NIN"></a>4.3.网络中的网络NIN</h2><p>首先一点是注意到卷积神经网络一般分成两块，一块主要由卷积层构成，另一块主要是全连接层。在Alexnet里我们看到如何把卷积层块和全连接层分别加深加宽从而得到深度网络。另外一个自然的想法是，我们可以串联数个卷积层块和全连接层块来构建深度网络。<br><img src="/img/deeplearn/nin.svg" alt="nin"><br>不过这里的一个难题是，卷积的输入输出是4D矩阵，然而全连接是2D。同时在卷积神经网络里我们提到如果把4D矩阵转成2D做全连接，这个会导致全连接层有过多的参数。NiN提出只对通道层做全连接并且像素之间共享权重来解决上述两个问题。就是说，我们使用kernel大小是1x1的卷积。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mlpconv</span><span class="params">(channels, kernel_size, padding,strides=<span class="number">1</span>, max_pooling=True)</span>:</span></span><br><span class="line">    out = nn.Sequential()</span><br><span class="line">    <span class="keyword">with</span> out.name_scope():<span class="comment">### NIN</span></span><br><span class="line">        out.add(</span><br><span class="line">            nn.Conv2D(channels=channels, kernel_size=kernel_size,strides=strides, padding=padding,activation=<span class="string">'relu'</span>),</span><br><span class="line">            nn.Conv2D(channels=channels, kernel_size=<span class="number">1</span>,padding=<span class="number">0</span>, strides=<span class="number">1</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">            nn.Conv2D(channels=channels, kernel_size=<span class="number">1</span>,padding=<span class="number">0</span>, strides=<span class="number">1</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">        <span class="keyword">if</span> max_pooling:</span><br><span class="line">            out.add(nn.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试一下：</span></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"></span><br><span class="line">blk = mlpconv(<span class="number">64</span>, <span class="number">3</span>, <span class="number">0</span>)</span><br><span class="line">blk.initialize()</span><br><span class="line"></span><br><span class="line">x = nd.random.uniform(shape=(<span class="number">32</span>, <span class="number">3</span>, <span class="number">16</span>, <span class="number">16</span>))</span><br><span class="line">y = blk(x)</span><br><span class="line"><span class="keyword">print</span> y.shape <span class="comment">##(32L, 64L, 6L, 6L)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># NiN的卷积层的参数跟Alexnet类似，使用三组不同的设定</span></span><br><span class="line"><span class="comment"># kernel: 11x11, channels: 96</span></span><br><span class="line"><span class="comment"># kernel: 5x5, channels: 256</span></span><br><span class="line"><span class="comment"># kernel: 3x3, channels: 384</span></span><br><span class="line"><span class="comment"># 除了使用了1x1卷积外，NiN在最后不是使用全连接，而是使用通道数为输出类别个数的</span></span><br><span class="line"><span class="comment"># `mlpconv`，外接一个平均池化层来将每个通道里的数值平均成一个标量。</span></span><br><span class="line">net = nn.Sequential()</span><br><span class="line"><span class="keyword">with</span> net.name_scope():</span><br><span class="line">    net.add(</span><br><span class="line">        mlpconv(<span class="number">96</span>, <span class="number">11</span>, <span class="number">0</span>, strides=<span class="number">4</span>),</span><br><span class="line">        mlpconv(<span class="number">256</span>, <span class="number">5</span>, <span class="number">2</span>),</span><br><span class="line">        mlpconv(<span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>),</span><br><span class="line">        nn.Dropout(<span class="number">.5</span>),</span><br><span class="line">        <span class="comment"># 目标类为10类</span></span><br><span class="line">        mlpconv(<span class="number">10</span>, <span class="number">3</span>, <span class="number">1</span>, max_pooling=<span class="keyword">False</span>),</span><br><span class="line">        <span class="comment"># 输入为 batch_size x 10 x 5 x 5, 通过AvgPool2D转成</span></span><br><span class="line">        <span class="comment"># batch_size x 10 x 1 x 1。</span></span><br><span class="line">        nn.AvgPool2D(pool_size=<span class="number">5</span>),</span><br><span class="line">        <span class="comment"># 转成 batch_size x 10</span></span><br><span class="line">        nn.Flatten()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment">## 获取数据并训练</span></span><br><span class="line"><span class="comment"># 跟Alexnet类似，但使用了更大的学习率。</span></span><br><span class="line"><span class="keyword">import</span> utils</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"></span><br><span class="line">train_data, test_data = utils.load_data_fashion_mnist(batch_size=<span class="number">64</span>, resize=<span class="number">224</span>)</span><br><span class="line">ctx = mx.gpu()</span><br><span class="line">net.initialize(ctx=ctx, init=init.Xavier())</span><br><span class="line">loss = gluon.loss.SoftmaxCrossEntropyLoss()</span><br><span class="line">trainer = gluon.Trainer(net.collect_params(),<span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.1</span>&#125;)</span><br><span class="line">utils.train(train_data, test_data, net, loss,trainer, ctx, num_epochs=<span class="number">10</span>)</span><br></pre></td></tr></table></figure></p>
<p>这种“一卷卷到底”最后加一个平均池化层的做法也成为了深度卷积神经网络的常用设计。</p>
<h2 id="4-4-GoogleNet"><a href="#4-4-GoogleNet" class="headerlink" title="4.4.GoogleNet"></a>4.4.GoogleNet</h2><p>在2014年的Imagenet竞赛里，Google的研究人员利用一个新的网络结构取得很大的优先。这个叫做GoogleLeNet的网络虽然在名字上是向LeNet致敬，但网络结构里很难看到LeNet的影子。它颠覆的大家对卷积神经网络串联一系列层的固定做法。下图是其<a href="https://arxiv.org/abs/1409.4842" target="_blank" rel="external">论文</a>对GoogLeNet的可视化<br><img src="/img/deeplearn/googlenet.png" alt="googlenet"><br>可以看到其中有多个四个并行卷积层的块。这个块一般叫做Inception，其基于Network in network的思想做了很大的改进。我们先看下如何定义一个下图所示的Inception块。<br><img src="/img/deeplearn/inception.svg" alt="Inception"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Inception</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n1_1, n2_1, n2_3, n3_1, n3_5, n4_1, **kwargs)</span>:</span></span><br><span class="line">        super(Inception, self).__init__(**kwargs)</span><br><span class="line">        <span class="keyword">with</span> self.name_scope():</span><br><span class="line">            <span class="comment"># path 1</span></span><br><span class="line">            self.p1_conv_1 = nn.Conv2D(n1_1, kernel_size=<span class="number">1</span>,activation=<span class="string">'relu'</span>)</span><br><span class="line">            <span class="comment"># path 2</span></span><br><span class="line">            self.p2_conv_1 = nn.Conv2D(n2_1, kernel_size=<span class="number">1</span>,activation=<span class="string">'relu'</span>)</span><br><span class="line">            self.p2_conv_3 = nn.Conv2D(n2_3, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>,activation=<span class="string">'relu'</span>)</span><br><span class="line">            <span class="comment"># path 3</span></span><br><span class="line">            self.p3_conv_1 = nn.Conv2D(n3_1, kernel_size=<span class="number">1</span>,activation=<span class="string">'relu'</span>)</span><br><span class="line">            self.p3_conv_5 = nn.Conv2D(n3_5, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>,activation=<span class="string">'relu'</span>)</span><br><span class="line">            <span class="comment"># path 4</span></span><br><span class="line">            self.p4_pool_3 = nn.MaxPool2D(pool_size=<span class="number">3</span>, padding=<span class="number">1</span>,strides=<span class="number">1</span>)</span><br><span class="line">            self.p4_conv_1 = nn.Conv2D(n4_1, kernel_size=<span class="number">1</span>,activation=<span class="string">'relu'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        p1 = self.p1_conv_1(x)</span><br><span class="line">        p2 = self.p2_conv_3(self.p2_conv_1(x))</span><br><span class="line">        p3 = self.p3_conv_5(self.p3_conv_1(x))</span><br><span class="line">        p4 = self.p4_conv_1(self.p4_pool_3(x))</span><br><span class="line">        <span class="keyword">return</span> nd.concat(p1, p2, p3, p4, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以看到Inception里有四个并行的线路。</span></span><br><span class="line"><span class="comment"># 1. 单个1x1卷积。</span></span><br><span class="line"><span class="comment"># 2. 1x1卷积接上3x3卷积。通常前者的通道数少于输入通道，这样减少后者的计算量。后者加上了`padding=1`使得输出的长宽的输入一致</span></span><br><span class="line"><span class="comment"># 3. 同2，但换成了5x5卷积</span></span><br><span class="line"><span class="comment"># 4. 和1类似，但卷积前用了最大池化层</span></span><br><span class="line"><span class="comment"># 最后将这四个并行线路的结果在通道这个维度上合并在一起。</span></span><br><span class="line"><span class="comment"># 测试一下：</span></span><br><span class="line">incp = Inception(<span class="number">64</span>, <span class="number">96</span>, <span class="number">128</span>, <span class="number">16</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">incp.initialize()</span><br><span class="line"></span><br><span class="line">x = nd.random.uniform(shape=(<span class="number">32</span>,<span class="number">3</span>,<span class="number">64</span>,<span class="number">64</span>))</span><br><span class="line"><span class="keyword">print</span> incp(x).shape <span class="comment">##(32L, 256L, 64L, 64L)  256=64+128+32+32</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># GoogLeNet将数个Inception串联在一起。注意到原论文里使用了多个输出，为了简化我们这里就使用一个输出。</span></span><br><span class="line"><span class="comment"># 为了可以更方便的查看数据在内部的形状变化，我们对每个块使用一个`nn.Sequential`，然后再把所有这些块连起来。</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GoogLeNet</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes, verbose=False, **kwargs)</span>:</span></span><br><span class="line">        super(GoogLeNet, self).__init__(**kwargs)</span><br><span class="line">        self.verbose = verbose</span><br><span class="line">        <span class="keyword">with</span> self.name_scope():</span><br><span class="line">            <span class="comment"># block 1</span></span><br><span class="line">            b1 = nn.Sequential()</span><br><span class="line">            b1.add(</span><br><span class="line">                nn.Conv2D(<span class="number">64</span>, kernel_size=<span class="number">7</span>, strides=<span class="number">2</span>,padding=<span class="number">3</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">                nn.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>)</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># block 2</span></span><br><span class="line">            b2 = nn.Sequential()</span><br><span class="line">            b2.add(</span><br><span class="line">                nn.Conv2D(<span class="number">64</span>, kernel_size=<span class="number">1</span>),</span><br><span class="line">                nn.Conv2D(<span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">                nn.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            <span class="comment"># block 3</span></span><br><span class="line">            b3 = nn.Sequential()</span><br><span class="line">            b3.add(</span><br><span class="line">                Inception(<span class="number">64</span>, <span class="number">96</span>, <span class="number">128</span>, <span class="number">16</span>, <span class="number">32</span>, <span class="number">32</span>),</span><br><span class="line">                Inception(<span class="number">128</span>, <span class="number">128</span>, <span class="number">192</span>, <span class="number">32</span>, <span class="number">96</span>, <span class="number">64</span>),</span><br><span class="line">                nn.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            <span class="comment"># block 4</span></span><br><span class="line">            b4 = nn.Sequential()</span><br><span class="line">            b4.add(</span><br><span class="line">                Inception(<span class="number">192</span>, <span class="number">96</span>, <span class="number">208</span>, <span class="number">16</span>, <span class="number">48</span>, <span class="number">64</span>),</span><br><span class="line">                Inception(<span class="number">160</span>, <span class="number">112</span>, <span class="number">224</span>, <span class="number">24</span>, <span class="number">64</span>, <span class="number">64</span>),</span><br><span class="line">                Inception(<span class="number">128</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">24</span>, <span class="number">64</span>, <span class="number">64</span>),</span><br><span class="line">                Inception(<span class="number">112</span>, <span class="number">144</span>, <span class="number">288</span>, <span class="number">32</span>, <span class="number">64</span>, <span class="number">64</span>),</span><br><span class="line">                Inception(<span class="number">256</span>, <span class="number">160</span>, <span class="number">320</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>),</span><br><span class="line">                nn.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            <span class="comment"># block 5</span></span><br><span class="line">            b5 = nn.Sequential()</span><br><span class="line">            b5.add(</span><br><span class="line">                Inception(<span class="number">256</span>, <span class="number">160</span>, <span class="number">320</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>),</span><br><span class="line">                Inception(<span class="number">384</span>, <span class="number">192</span>, <span class="number">384</span>, <span class="number">48</span>, <span class="number">128</span>, <span class="number">128</span>),</span><br><span class="line">                nn.AvgPool2D(pool_size=<span class="number">2</span>)</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># block 6</span></span><br><span class="line">            b6 = nn.Sequential()</span><br><span class="line">            b6.add(</span><br><span class="line">                nn.Flatten(),</span><br><span class="line">                nn.Dense(num_classes)</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># chain blocks together</span></span><br><span class="line">            self.net = nn.Sequential()</span><br><span class="line">            self.net.add(b1, b2, b3, b4, b5, b6)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = x</span><br><span class="line">        <span class="keyword">for</span> i, b <span class="keyword">in</span> enumerate(self.net):</span><br><span class="line">            out = b(out)</span><br><span class="line">            <span class="keyword">if</span> self.verbose:</span><br><span class="line">                print(<span class="string">'Block %d output: %s'</span> % (i + <span class="number">1</span>, out.shape))</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们看一下每个块对输出的改变。</span></span><br><span class="line">net = GoogLeNet(<span class="number">10</span>, verbose=<span class="keyword">True</span>)</span><br><span class="line">net.initialize()</span><br><span class="line"></span><br><span class="line">x = nd.random.uniform(shape=(<span class="number">4</span>, <span class="number">3</span>, <span class="number">96</span>, <span class="number">96</span>))</span><br><span class="line">y = net(x)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 获取数据并训练</span></span><br><span class="line"><span class="comment"># 跟VGG一样我们使用了较小的输入$96\times 96$来加速计算。</span></span><br><span class="line"><span class="keyword">import</span> utils</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"></span><br><span class="line">train_data, test_data = utils.load_data_fashion_mnist(batch_size=<span class="number">64</span>, resize=<span class="number">96</span>)</span><br><span class="line"></span><br><span class="line">ctx = mx.gpu()</span><br><span class="line">net = GoogLeNet(<span class="number">10</span>)</span><br><span class="line">net.initialize(ctx=ctx, init=init.Xavier())</span><br><span class="line"></span><br><span class="line">loss = gluon.loss.SoftmaxCrossEntropyLoss()</span><br><span class="line">trainer = gluon.Trainer(net.collect_params(),<span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.01</span>&#125;)</span><br><span class="line">utils.train(train_data, test_data, net, loss,trainer, ctx, num_epochs=<span class="number">10</span>)</span><br></pre></td></tr></table></figure></p>
<p>GoogLeNet加入了更加结构化的Inception块来使得我们可以使用更大的通道，更多的层，同时控制计算量和模型大小在合理范围内。<br>练习<br>GoogLeNet有数个后续版本，尝试实现他们并运行看看有什么不一样</p>
<ul>
<li>v1: 本节介绍的是最早版本：<a href="http://arxiv.org/abs/1409.4842" target="_blank" rel="external">Going Deeper with Convolutions</a></li>
<li>v2: 加入和Batch Normalization：<a href="http://arxiv.org/abs/1502.03167" target="_blank" rel="external">Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li>
<li>v3: 对Inception做了调整：<a href="http://arxiv.org/abs/1512.00567" target="_blank" rel="external">Rethinking the Inception Architecture for Computer Vision</a></li>
<li>v4: 基于ResNet加入了Residual Connections：<a href="http://arxiv.org/abs/1602.07261" target="_blank" rel="external">Inception-ResNet and the Impact of Residual Connections on Learning</a></li>
</ul>
<h2 id="4-5-Resnet"><a href="#4-5-Resnet" class="headerlink" title="4.5.Resnet"></a>4.5.Resnet</h2><p>ResNet有效的解决了深度卷积神经网络难训练的问题。这是因为在误差反传的过程中，梯度通常变得越来越小，从而权重的更新量也变小。这个导致远离损失函数的层训练缓慢，随着层数的增加这个现象更加明显。之前有两种常用方案来尝试解决这个问题：</p>
<ol>
<li>按层训练。先训练靠近数据的层，然后慢慢的增加后面的层。但效果不是特别好，而且比较麻烦。</li>
<li>使用更宽的层（增加输出通道）而不是更深来增加模型复杂度。但更宽的模型经常不如更深的效果好。<br>ResNet通过增加跨层的连接来解决梯度逐层回传时变小的问题。虽然这个想法之前就提出过了，但ResNet真正的把效果做好了。<br>下图演示了一个跨层的连接。<br><img src="/img/deeplearn/residual.svg" alt="residual"><br>最底下那层的输入不仅仅是输出给了中间层，而且其与中间层结果相加进入最上层。这样在梯度反传时，最上层梯度可以直接跳过中间层传到最下层，从而避免最下层梯度过小情况。<br>为什么叫做残差网络呢？我们可以将上面示意图里的结构拆成两个网络的和，一个一层，一个两层，最下面层是共享的。<br><img src="/img/deeplearn/residual2.svg" alt="residual2"><br>在训练过程中，左边的网络因为更简单所以更容易训练。这个小网络没有拟合到的部分，或者说残差，则被右边的网络抓取住。所以直观上来说，即使加深网络，跨层连接仍然可以使得底层网络可以充分的训练，从而不会让训练更难。<br>Residual块:<br>ResNet沿用了VGG的那种全用3x3卷积，但在卷积和池化层之间加入了批量归一层来加速训练。每次跨层连接跨过两层卷积。这里我们定义一个这样的残差块。注意到如果输入的通道数和输出不一样时（<code>same_shape=False</code>），我们使用一个额外的1x1卷积来做通道变化，同时使用<code>strides=2</code>来把长宽减半。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Residual</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, channels, same_shape=True, **kwargs)</span>:</span></span><br><span class="line">        super(Residual, self).__init__(**kwargs)</span><br><span class="line">        self.same_shape = same_shape</span><br><span class="line">        <span class="keyword">with</span> self.name_scope():</span><br><span class="line">            strides = <span class="number">1</span> <span class="keyword">if</span> same_shape <span class="keyword">else</span> <span class="number">2</span></span><br><span class="line">            self.conv1 = nn.Conv2D(channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>,strides=strides)</span><br><span class="line">            self.bn1 = nn.BatchNorm()</span><br><span class="line">            self.conv2 = nn.Conv2D(channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">            self.bn2 = nn.BatchNorm()</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> same_shape:</span><br><span class="line">                self.conv3 = nn.Conv2D(channels, kernel_size=<span class="number">1</span>,strides=strides)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = nd.relu(self.bn1(self.conv1(x)))</span><br><span class="line">        out = self.bn2(self.conv2(out))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.same_shape:</span><br><span class="line">            x = self.conv3(x)</span><br><span class="line">        <span class="keyword">return</span> nd.relu(out + x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入输出通道相同：</span></span><br><span class="line">blk = Residual(<span class="number">3</span>)</span><br><span class="line">blk.initialize()</span><br><span class="line">x = nd.random.uniform(shape=(<span class="number">4</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">6</span>))</span><br><span class="line"><span class="keyword">print</span> blk(x).shape <span class="comment">##(4L, 3L, 6L, 6L)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入输出通道不同：</span></span><br><span class="line">blk2 = Residual(<span class="number">8</span>, same_shape=<span class="keyword">False</span>)</span><br><span class="line">blk2.initialize()</span><br><span class="line"><span class="keyword">print</span> blk2(x).shape <span class="comment">##(4L, 8L, 3L, 3L)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#构建ResNet</span></span><br><span class="line"><span class="comment"># 类似GoogLeNet主体是由Inception块串联而成，ResNet的主体部分串联多个Residual块。下面我们定义18层的ResNet。</span></span><br><span class="line"><span class="comment"># 同样为了阅读更加容易，我们这里使用了多个`nn.Sequential`。另外注意到一点是，这里我们没用池化层来减小数据长宽，</span></span><br><span class="line"><span class="comment"># 而是通过有通道变化的Residual块里面的使用`strides=2`的卷积层。</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes, verbose=False, **kwargs)</span>:</span></span><br><span class="line">        super(ResNet, self).__init__(**kwargs)</span><br><span class="line">        self.verbose = verbose</span><br><span class="line">        <span class="keyword">with</span> self.name_scope():</span><br><span class="line">            <span class="comment"># block 1</span></span><br><span class="line">            b1 = nn.Conv2D(<span class="number">64</span>, kernel_size=<span class="number">7</span>, strides=<span class="number">2</span>)</span><br><span class="line">            <span class="comment"># block 2</span></span><br><span class="line">            b2 = nn.Sequential()</span><br><span class="line">            b2.add(</span><br><span class="line">                nn.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>),</span><br><span class="line">                Residual(<span class="number">64</span>),</span><br><span class="line">                Residual(<span class="number">64</span>)</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># block 3</span></span><br><span class="line">            b3 = nn.Sequential()</span><br><span class="line">            b3.add(</span><br><span class="line">                Residual(<span class="number">128</span>, same_shape=<span class="keyword">False</span>),</span><br><span class="line">                Residual(<span class="number">128</span>)</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># block 4</span></span><br><span class="line">            b4 = nn.Sequential()</span><br><span class="line">            b4.add(</span><br><span class="line">                Residual(<span class="number">256</span>, same_shape=<span class="keyword">False</span>),</span><br><span class="line">                Residual(<span class="number">256</span>)</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># block 5</span></span><br><span class="line">            b5 = nn.Sequential()</span><br><span class="line">            b5.add(</span><br><span class="line">                Residual(<span class="number">512</span>, same_shape=<span class="keyword">False</span>),</span><br><span class="line">                Residual(<span class="number">512</span>)</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># block 6</span></span><br><span class="line">            b6 = nn.Sequential()</span><br><span class="line">            b6.add(</span><br><span class="line">                nn.AvgPool2D(pool_size=<span class="number">3</span>),</span><br><span class="line">                nn.Dense(num_classes)</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># chain all blocks together</span></span><br><span class="line">            self.net = nn.Sequential()</span><br><span class="line">            self.net.add(b1, b2, b3, b4, b5, b6)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = x</span><br><span class="line">        <span class="keyword">for</span> i, b <span class="keyword">in</span> enumerate(self.net):</span><br><span class="line">            out = b(out)</span><br><span class="line">            <span class="keyword">if</span> self.verbose:</span><br><span class="line">                print(<span class="string">'Block %d output: %s'</span>%(i+<span class="number">1</span>, out.shape))</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里演示数据在块之间的形状变化：</span></span><br><span class="line">net = ResNet(<span class="number">10</span>, verbose=<span class="keyword">True</span>)</span><br><span class="line">net.initialize()</span><br><span class="line"></span><br><span class="line">x = nd.random.uniform(shape=(<span class="number">4</span>, <span class="number">3</span>, <span class="number">96</span>, <span class="number">96</span>))</span><br><span class="line">y = net(x)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 获取数据并训练</span></span><br><span class="line"><span class="comment"># 跟前面类似，但因为有批量归一化，所以使用了较大的学习率。</span></span><br><span class="line"><span class="keyword">import</span> utils</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"></span><br><span class="line">train_data, test_data = utils.load_data_fashion_mnist(batch_size=<span class="number">64</span>, resize=<span class="number">96</span>)</span><br><span class="line"></span><br><span class="line">ctx = mx.gpu()</span><br><span class="line">net = ResNet(<span class="number">10</span>)</span><br><span class="line">net.initialize(ctx=ctx, init=init.Xavier())</span><br><span class="line"></span><br><span class="line">loss = gluon.loss.SoftmaxCrossEntropyLoss()</span><br><span class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.05</span>&#125;)</span><br><span class="line">utils.train(train_data, test_data, net, loss,trainer, ctx, num_epochs=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>结论<br>ResNet使用跨层通道使得训练非常深的卷积神经网络成为可能。同样它使用很简单的卷积层配置，使得其拓展更加简单。</p>
<p>练习</p>
<ul>
<li>这里我们实现了ResNet 18，原论文中还讨论了更深的配置。尝试实现它们。（提示：参考论文中的表1）</li>
<li>原论文中还介绍了一个“bottleneck”架构，尝试实现它</li>
<li>ResNet作者在<a href="https://arxiv.org/abs/1603.05027" target="_blank" rel="external">接下来的一篇论文</a>讨论了将Residual块里面的<code>Conv-&gt;BN-&gt;Relu</code>结构改成了<code>BN-&gt;Relu-&gt;Conv</code>（参考论文图1），尝试实现它</li>
</ul>
<h2 id="4-6-DenseNet"><a href="#4-6-DenseNet" class="headerlink" title="4.6.DenseNet"></a>4.6.DenseNet</h2><p>ResNet的跨层连接思想影响了接下来的众多工作。这里我们介绍其中的一个：<a href="https://arxiv.org/pdf/1608.06993.pdf" target="_blank" rel="external">DenseNet</a>。下图展示了这两个的主要区别：<br><img src="/img/deeplearn/densenet.svg" alt="densenet"><br>可以看到DenseNet里来自跳层的输出不是通过加法（<code>+</code>）而是拼接（<code>concat</code>）来跟目前层的输出合并。因为是拼接，所以底层的输出会保留的进入上面所有层。这是为什么叫“稠密连接”的原因<br>稠密块（Dense Block）:<br>我们先来定义一个稠密连接块。DenseNet的卷积块使用ResNet改进版本的<code>BN-&gt;Relu-&gt;Conv</code>。每个卷积的输出通道数被称之为<code>growth_rate</code>，这是因为假设输出为<code>in_channels</code>，而且有<code>layers</code>层，那么输出的通道数就是<code>in_channels+growth_rate*layers</code>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_block</span><span class="params">(channels)</span>:</span></span><br><span class="line">    out = nn.Sequential()</span><br><span class="line">    out.add(</span><br><span class="line">        nn.BatchNorm(),</span><br><span class="line">        nn.Activation(<span class="string">'relu'</span>),</span><br><span class="line">        nn.Conv2D(channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DenseBlock</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layers, growth_rate, **kwargs)</span>:</span></span><br><span class="line">        super(DenseBlock, self).__init__(**kwargs)</span><br><span class="line">        self.net = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(layers):</span><br><span class="line">            self.net.add(conv_block(growth_rate))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.net:</span><br><span class="line">            out = layer(x)</span><br><span class="line">            x = nd.concat(x, out, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们验证下输出通道数是不是符合预期。</span></span><br><span class="line">dblk = DenseBlock(<span class="number">2</span>, <span class="number">10</span>)</span><br><span class="line">dblk.initialize()</span><br><span class="line"></span><br><span class="line">x = nd.random.uniform(shape=(<span class="number">4</span>,<span class="number">3</span>,<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line"><span class="keyword">print</span> dblk(x).shape  <span class="comment">##(4L, 23L, 8L, 8L)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 过渡块（Transition Block）</span></span><br><span class="line"><span class="comment"># 因为使用拼接的缘故，每经过一次过渡块输出通道数可能会激增。为了控制模型复杂度，</span></span><br><span class="line"><span class="comment"># 这里引入一个过渡块，它不仅把输入的长宽减半，同时也使用1x1卷积来改变通道数。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transition_block</span><span class="params">(channels)</span>:</span></span><br><span class="line">    out = nn.Sequential()</span><br><span class="line">    out.add(</span><br><span class="line">        nn.BatchNorm(),</span><br><span class="line">        nn.Activation(<span class="string">'relu'</span>),</span><br><span class="line">        nn.Conv2D(channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">        nn.AvgPool2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证一下结果：</span></span><br><span class="line">tblk = transition_block(<span class="number">10</span>)</span><br><span class="line">tblk.initialize()</span><br><span class="line"><span class="keyword">print</span> tblk(x).shape <span class="comment">##(4L, 10L, 4L, 4L)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DenseNet的主体就是交替串联稠密块和过渡块。它使用全局的`growth_rate`使得配置更加简单。过渡层每次都将通道数减半。</span></span><br><span class="line"><span class="comment"># 下面定义一个121层的DenseNet。</span></span><br><span class="line">init_channels = <span class="number">64</span></span><br><span class="line">growth_rate = <span class="number">32</span></span><br><span class="line">block_layers = [<span class="number">6</span>, <span class="number">12</span>, <span class="number">24</span>, <span class="number">16</span>]</span><br><span class="line">num_classes = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dense_net</span><span class="params">()</span>:</span></span><br><span class="line">    net = nn.Sequential()</span><br><span class="line">    <span class="keyword">with</span> net.name_scope():</span><br><span class="line">        <span class="comment"># first block</span></span><br><span class="line">        net.add(</span><br><span class="line">            nn.Conv2D(init_channels, kernel_size=<span class="number">7</span>, strides=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">            nn.BatchNorm(),</span><br><span class="line">            nn.Activation(<span class="string">'relu'</span>),</span><br><span class="line">            nn.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># dense blocks</span></span><br><span class="line">        channels = init_channels</span><br><span class="line">        <span class="keyword">for</span> i, layers <span class="keyword">in</span> enumerate(block_layers): <span class="comment">#i,layers---0 6   1 12   2 24   3 16</span></span><br><span class="line">            net.add(DenseBlock(layers, growth_rate))</span><br><span class="line">            channels += layers * growth_rate</span><br><span class="line">            <span class="keyword">if</span> i != len(block_layers)-<span class="number">1</span>:<span class="comment">##前三个每个denseblock后channels减半  最后一个不用</span></span><br><span class="line">                net.add(transition_block(channels//<span class="number">2</span>))</span><br><span class="line">        <span class="comment"># last block</span></span><br><span class="line">        net.add(</span><br><span class="line">            nn.BatchNorm(),</span><br><span class="line">            nn.Activation(<span class="string">'relu'</span>),</span><br><span class="line">            nn.AvgPool2D(pool_size=<span class="number">1</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Dense(num_classes)</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line"><span class="comment">## 获取数据并训练</span></span><br><span class="line"><span class="comment"># 因为这里我们使用了比较深的网络，所以我们进一步把输入减少到32x32来训练。</span></span><br><span class="line"><span class="keyword">import</span> utils</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"></span><br><span class="line">train_data, test_data = utils.load_data_fashion_mnist(batch_size=<span class="number">64</span>, resize=<span class="number">32</span>)</span><br><span class="line">ctx = mx.gpu()</span><br><span class="line">net = dense_net()</span><br><span class="line">net.initialize(ctx=ctx, init=init.Xavier())</span><br><span class="line"></span><br><span class="line">loss = gluon.loss.SoftmaxCrossEntropyLoss()</span><br><span class="line">trainer = gluon.Trainer(net.collect_params(),<span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.1</span>&#125;)</span><br><span class="line">utils.train(train_data, test_data, net, loss,trainer, ctx, num_epochs=<span class="number">10</span>)</span><br></pre></td></tr></table></figure></p>
<p>Desnet通过将ResNet里的<code>+</code>替换成<code>concat</code>从而获得更稠密的连接。<br>练习</p>
<ul>
<li>DesNet论文中提交的一个优点是其模型参数比ResNet更小，想想为什么？</li>
<li>DesNet被人诟病的一个问题是内存消耗过多。真的会这样吗？可以把输入换成$224\times 224$（需要改最后的<code>AvgPool2D</code>大小），来看看实际（GPU）内存消耗。</li>
<li>这里的FashionMNIST有必要用100+层的网络吗？尝试将其改简单看看效果。</li>
</ul>
<h2 id="4-7-_u56FE_u7247_u589E_u5F3A"><a href="#4-7-_u56FE_u7247_u589E_u5F3A" class="headerlink" title="4.7.图片增强"></a>4.7.图片增强</h2><p>图片增强通过一系列的随机变化生成大量“新”的样本，从而减低过拟合的可能。现在在深度卷积神经网络训练中，图片增强是必不可少的一部分。<br>常用增强方法:<br>我们首先读取一张400x500的图片作为样例<br><img src="/img/deeplearn/img5.png" alt="img5"><br>水平方向翻转图片是最早也是最广泛使用的一种增强。<br>以.5的概率做翻转<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aug = image.<span class="function"><span class="title">HorizontalFlipAug</span><span class="params">(.<span class="number">5</span>)</span></span> ##左右变换对人没有啥区别，对卷积比较难看，可以用</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/deeplearn/img6.png" alt="img6"><br>随机裁剪一个块 200 x 200 的区域<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aug = image.RandomCropAug([<span class="number">200</span>,<span class="number">200</span>]) <span class="preprocessor">##随机裁剪可以一定概率将物体放在各个位置出现，降低卷积对位置的敏感</span></span><br></pre></td></tr></table></figure></p>
<p><img src="/img/deeplearn/img7.png" alt="img7"><br>我们也可以随机裁剪一块随机大小的区域<br>随机裁剪，要求保留至少0.1的区域，随机长宽比在.5和2之间。 常用一些 有剪切和变形<br>最后将结果resize到200x200<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aug = image.RandomSizedCropAug((<span class="number">200</span>,<span class="number">200</span>), <span class="number">.1</span>, (<span class="number">.5</span>,<span class="number">2</span>))</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/deeplearn/img8.png" alt="img8"><br>颜色变化<br>形状变化外的一个另一大类是变化颜色。<br>随机将亮度增加或者减小在0-50%间的一个量<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aug = image.<span class="function"><span class="title">BrightnessJitterAug</span><span class="params">(.<span class="number">5</span>)</span></span></span><br></pre></td></tr></table></figure></p>
<p><img src="/img/deeplearn/img9.png" alt="img9"><br>随机色调变化 ##模拟不同光照情况下<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aug = image.<span class="function"><span class="title">HueJitterAug</span><span class="params">(.<span class="number">5</span>)</span></span></span><br></pre></td></tr></table></figure></p>
<p><img src="/img/deeplearn/img10.png" alt="img10"></p>
<p>cifar10使用增强<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># 对于训练图片我们随机水平翻转和剪裁。对于测试图片仅仅就是中心剪裁。</span></span><br><span class="line"><span class="preprocessor"># CIFAR10图片尺寸是<span class="number">32</span>x32x3，我们剪裁成<span class="number">28</span>x28x3.</span></span><br><span class="line">train_augs = [image.HorizontalFlipAug(<span class="number">.5</span>),image.RandomCropAug((<span class="number">28</span>,<span class="number">28</span>))]</span><br><span class="line">test_augs = [image.CenterCropAug((<span class="number">28</span>,<span class="number">28</span>))]</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/deeplearn/img11.png" alt="img11"><br>使用增强训练结果<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">0.</span> Loss: <span class="number">1.483978</span>, Train acc <span class="number">0.469281</span>, Test acc <span class="number">0.447093</span></span><br><span class="line">Epoch <span class="number">1.</span> Loss: <span class="number">1.069175</span>, Train acc <span class="number">0.619841</span>, Test acc <span class="number">0.582377</span></span><br><span class="line">Epoch <span class="number">2.</span> Loss: <span class="number">0.883304</span>, Train acc <span class="number">0.689666</span>, Test acc <span class="number">0.685621</span></span><br><span class="line">Epoch <span class="number">3.</span> Loss: <span class="number">0.770580</span>, Train acc <span class="number">0.729875</span>, Test acc <span class="number">0.721717</span></span><br><span class="line">Epoch <span class="number">4.</span> Loss: <span class="number">0.688039</span>, Train acc <span class="number">0.759827</span>, Test acc <span class="number">0.766515</span></span><br><span class="line">Epoch <span class="number">5.</span> Loss: <span class="number">0.629892</span>, Train acc <span class="number">0.779352</span>, Test acc <span class="number">0.720134</span></span><br><span class="line">Epoch <span class="number">6.</span> Loss: <span class="number">0.578624</span>, Train acc <span class="number">0.799772</span>, Test acc <span class="number">0.774426</span></span><br><span class="line">Epoch <span class="number">7.</span> Loss: <span class="number">0.535755</span>, Train acc <span class="number">0.812544</span>, Test acc <span class="number">0.766021</span></span><br><span class="line">Epoch <span class="number">8.</span> Loss: <span class="number">0.507056</span>, Train acc <span class="number">0.820888</span>, Test acc <span class="number">0.783525</span></span><br><span class="line">Epoch <span class="number">9.</span> Loss: <span class="number">0.476048</span>, Train acc <span class="number">0.835066</span>, Test acc <span class="number">0.792623</span></span><br></pre></td></tr></table></figure></p>
<p>不使用增强训练结果<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">0.</span> Loss: <span class="number">1.440451</span>, Train acc <span class="number">0.486553</span>, Test acc <span class="number">0.565269</span></span><br><span class="line">Epoch <span class="number">1.</span> Loss: <span class="number">0.978142</span>, Train acc <span class="number">0.654356</span>, Test acc <span class="number">0.623418</span></span><br><span class="line">Epoch <span class="number">2.</span> Loss: <span class="number">0.758181</span>, Train acc <span class="number">0.733584</span>, Test acc <span class="number">0.683643</span></span><br><span class="line">Epoch <span class="number">3.</span> Loss: <span class="number">0.590786</span>, Train acc <span class="number">0.790821</span>, Test acc <span class="number">0.703323</span></span><br><span class="line">Epoch <span class="number">4.</span> Loss: <span class="number">0.454785</span>, Train acc <span class="number">0.840873</span>, Test acc <span class="number">0.714498</span></span><br><span class="line">Epoch <span class="number">5.</span> Loss: <span class="number">0.352663</span>, Train acc <span class="number">0.877777</span>, Test acc <span class="number">0.653481</span></span><br><span class="line">Epoch <span class="number">6.</span> Loss: <span class="number">0.248027</span>, Train acc <span class="number">0.913731</span>, Test acc <span class="number">0.722903</span></span><br><span class="line">Epoch <span class="number">7.</span> Loss: <span class="number">0.176261</span>, Train acc <span class="number">0.938675</span>, Test acc <span class="number">0.732892</span></span><br><span class="line">Epoch <span class="number">8.</span> Loss: <span class="number">0.118719</span>, Train acc <span class="number">0.959439</span>, Test acc <span class="number">0.696697</span></span><br><span class="line">Epoch <span class="number">9.</span> Loss: <span class="number">0.091157</span>, Train acc <span class="number">0.969106</span>, Test acc <span class="number">0.705103</span></span><br></pre></td></tr></table></figure></p>
<p>完整代码：<br>resnet18<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">from mxnet<span class="class">.gluon</span> import nn</span><br><span class="line">class <span class="function"><span class="title">Residual</span><span class="params">(nn.HybridBlock)</span></span>:</span><br><span class="line">    def __init__(self, channels, same_shape=True, **kwargs):</span><br><span class="line">        <span class="function"><span class="title">super</span><span class="params">(Residual, self)</span></span>.__init__(**kwargs)</span><br><span class="line">        self<span class="class">.same_shape</span> = same_shape</span><br><span class="line">        with self.<span class="function"><span class="title">name_scope</span><span class="params">()</span></span>:</span><br><span class="line">            strides = <span class="number">1</span> <span class="keyword">if</span> same_shape <span class="keyword">else</span> <span class="number">2</span></span><br><span class="line">            self<span class="class">.conv1</span> = nn.Conv2D(channels, kernel_size=<span class="number">3</span>, <span class="attribute">padding</span>=<span class="number">1</span>,</span><br><span class="line">                                  strides=strides)</span><br><span class="line">            self<span class="class">.bn1</span> = nn.<span class="function"><span class="title">BatchNorm</span><span class="params">()</span></span></span><br><span class="line">            self<span class="class">.conv2</span> = nn.<span class="function"><span class="title">Conv2D</span><span class="params">(channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span></span></span><br><span class="line">            self<span class="class">.bn2</span> = nn.<span class="function"><span class="title">BatchNorm</span><span class="params">()</span></span></span><br><span class="line">            <span class="keyword">if</span> not same_shape:</span><br><span class="line">                self<span class="class">.conv3</span> = nn.Conv2D(channels, kernel_size=<span class="number">1</span>,</span><br><span class="line">                                      strides=strides)</span><br><span class="line"></span><br><span class="line">    def <span class="function"><span class="title">hybrid_forward</span><span class="params">(self, F, x)</span></span>:</span><br><span class="line">        out = F.<span class="function"><span class="title">relu</span><span class="params">(self.bn1(self.conv1(x)</span></span>))</span><br><span class="line">        out = self.<span class="function"><span class="title">bn2</span><span class="params">(self.conv2(out)</span></span>)</span><br><span class="line">        <span class="keyword">if</span> not self<span class="class">.same_shape</span>:</span><br><span class="line">            x = self.<span class="function"><span class="title">conv3</span><span class="params">(x)</span></span></span><br><span class="line">        return F.<span class="function"><span class="title">relu</span><span class="params">(out + x)</span></span></span><br><span class="line"></span><br><span class="line">def <span class="function"><span class="title">resnet18</span><span class="params">(num_classes)</span></span>:</span><br><span class="line">    net = nn.<span class="function"><span class="title">HybridSequential</span><span class="params">()</span></span></span><br><span class="line">    with net.<span class="function"><span class="title">name_scope</span><span class="params">()</span></span>:</span><br><span class="line">        net.add(</span><br><span class="line">            nn.<span class="function"><span class="title">BatchNorm</span><span class="params">()</span></span>,</span><br><span class="line">            nn.<span class="function"><span class="title">Conv2D</span><span class="params">(<span class="number">64</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>)</span></span>,</span><br><span class="line">            nn.<span class="function"><span class="title">MaxPool2D</span><span class="params">(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>)</span></span>,</span><br><span class="line">            <span class="function"><span class="title">Residual</span><span class="params">(<span class="number">64</span>)</span></span>,</span><br><span class="line">            <span class="function"><span class="title">Residual</span><span class="params">(<span class="number">64</span>)</span></span>,</span><br><span class="line">            <span class="function"><span class="title">Residual</span><span class="params">(<span class="number">128</span>, same_shape=False)</span></span>,</span><br><span class="line">            <span class="function"><span class="title">Residual</span><span class="params">(<span class="number">128</span>)</span></span>,</span><br><span class="line">            <span class="function"><span class="title">Residual</span><span class="params">(<span class="number">256</span>, same_shape=False)</span></span>,</span><br><span class="line">            <span class="function"><span class="title">Residual</span><span class="params">(<span class="number">256</span>)</span></span>,</span><br><span class="line">            nn.<span class="function"><span class="title">GlobalAvgPool2D</span><span class="params">()</span></span>,</span><br><span class="line">            nn.<span class="function"><span class="title">Dense</span><span class="params">(num_classes)</span></span></span><br><span class="line">        )</span><br><span class="line">    return net</span><br></pre></td></tr></table></figure></p>
<p>imageaugmentation<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> image</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> four <span class="keyword">import</span> utils</span><br><span class="line"></span><br><span class="line">img = image.imdecode(open(<span class="string">'../data/cat1.jpg'</span>, <span class="string">'rb'</span>).read())</span><br><span class="line">plt.imshow(img.asnumpy())</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 接下来我们定义一个辅助函数，给定输入图片`img`的增强方法`aug`，它会运行多次并画出结果。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply</span><span class="params">(img, aug, n=<span class="number">3</span>)</span>:</span></span><br><span class="line">    _, figs = plt.subplots(n, n, figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="comment"># 转成float，一是因为aug需要float类型数据来方便做变化。</span></span><br><span class="line">            <span class="comment"># 二是这里会有一次copy操作，因为有些aug直接通过改写输入</span></span><br><span class="line">            <span class="comment">#（而不是新建输出）获取性能的提升</span></span><br><span class="line">            x = img.astype(<span class="string">'float32'</span>) <span class="comment">##将图片默认的int8转float32方便计算</span></span><br><span class="line">            <span class="comment"># 有些aug不保证输入是合法值，所以做一次clip</span></span><br><span class="line">            y = aug(x).clip(<span class="number">0</span>,<span class="number">254</span>)</span><br><span class="line">            <span class="comment"># 显示浮点图片时imshow要求输入在[0,1]之间</span></span><br><span class="line">            figs[i][j].imshow(y.asnumpy()/<span class="number">255.0</span>)</span><br><span class="line">            figs[i][j].axes.get_xaxis().set_visible(<span class="keyword">False</span>)</span><br><span class="line">            figs[i][j].axes.get_yaxis().set_visible(<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">### 变形</span></span><br><span class="line"><span class="comment"># 水平方向翻转图片是最早也是最广泛使用的一种增强。</span></span><br><span class="line"><span class="comment"># 以.5的概率做翻转</span></span><br><span class="line">aug = image.HorizontalFlipAug(<span class="number">.5</span>) <span class="comment">##左右变换对人没有啥区别，对卷积比较难看，可以用</span></span><br><span class="line">apply(img, aug)</span><br><span class="line">plt.show() <span class="comment">##此行可以让上面的apply函数的imshow显示</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 样例图片里我们关心的猫在图片正中间，但一般情况下可能不是这样。前面我们提到池化层能弱化卷积层对目标位置的敏感度，</span></span><br><span class="line"><span class="comment"># 但也不能完全解决这个问题。一个常用增强方法是随机的截取其中的一块。</span></span><br><span class="line"><span class="comment"># 注意到随机截取一般会缩小输入的形状。如果原始输入图片过小，导致没有太多空间进行随机裁剪，</span></span><br><span class="line"><span class="comment"># 通常做法是先将其放大的足够大的尺寸。所以如果你的原始图片足够大，建议不要事先将它们裁到网络需要的大小。</span></span><br><span class="line"><span class="comment"># 随机裁剪一个块 200 x 200 的区域</span></span><br><span class="line">aug = image.RandomCropAug([<span class="number">200</span>,<span class="number">200</span>]) <span class="comment">##随机裁剪可以一定概率将物体放在各个位置出现，降低卷积对位置的敏感</span></span><br><span class="line">apply(img, aug)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们也可以随机裁剪一块随机大小的区域</span></span><br><span class="line"><span class="comment"># 随机裁剪，要求保留至少0.1的区域，随机长宽比在.5和2之间。 ################################常用一些 有剪切和变形</span></span><br><span class="line"><span class="comment"># 最后将结果resize到200x200</span></span><br><span class="line">aug = image.RandomSizedCropAug((<span class="number">200</span>,<span class="number">200</span>), <span class="number">.1</span>, (<span class="number">.5</span>,<span class="number">2</span>))</span><br><span class="line">apply(img, aug)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">### 颜色变化</span></span><br><span class="line"><span class="comment"># 形状变化外的一个另一大类是变化颜色。</span></span><br><span class="line"><span class="comment"># 随机将亮度增加或者减小在0-50%间的一个量</span></span><br><span class="line">aug = image.BrightnessJitterAug(<span class="number">.5</span>)</span><br><span class="line">apply(img, aug)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机色调变化 ##模拟不同光照情况下</span></span><br><span class="line">aug = image.HueJitterAug(<span class="number">.5</span>)</span><br><span class="line">apply(img, aug)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># ## 如何使用</span></span><br><span class="line"><span class="comment"># 通常使用时我们会将数个增强方法一起使用。注意到图片增强通常只是针对训练数据，对于测试数据则用得较小。后者常用的是做5次随机剪裁，</span></span><br><span class="line"><span class="comment"># 然后讲5张图片的预测结果做均值。</span></span><br><span class="line"><span class="comment"># 下面我们使用CIFAR10来演示图片增强对训练的影响。我们这里不使用前面一直用的FashionMNIST，这是因为这个数据的图片基本已经对齐好了，</span></span><br><span class="line"><span class="comment"># 而且是黑白图片，所以不管是变形还是变色增强效果都不会明显。</span></span><br><span class="line"><span class="comment"># ### 数据读取</span></span><br><span class="line"><span class="comment"># 我们首先定义一个辅助函数可以对图片按顺序应用数个增强：</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply_aug_list</span><span class="params">(img, augs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> augs:</span><br><span class="line">        img = f(img)</span><br><span class="line">    <span class="keyword">return</span> img</span><br><span class="line"><span class="comment"># 对于训练图片我们随机水平翻转和剪裁。对于测试图片仅仅就是中心剪裁。</span></span><br><span class="line"><span class="comment"># CIFAR10图片尺寸是32x32x3，我们剪裁成28x28x3.</span></span><br><span class="line">train_augs = [image.HorizontalFlipAug(<span class="number">.5</span>),image.RandomCropAug((<span class="number">28</span>,<span class="number">28</span>))]</span><br><span class="line">test_augs = [image.CenterCropAug((<span class="number">28</span>,<span class="number">28</span>))]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后定义数据读取，这里跟前面的FashionMNIST类似，但在`transform`中加入了图片增强：</span></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_transform</span><span class="params">(augs)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(data, label)</span>:</span></span><br><span class="line">        data = data.astype(<span class="string">'float32'</span>)</span><br><span class="line">        <span class="keyword">if</span> augs <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            data = apply_aug_list(data, augs)<span class="comment">############</span></span><br><span class="line">        data = nd.transpose(data, (<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)) / <span class="number">255.0</span></span><br><span class="line">        <span class="keyword">return</span> data, label.astype(<span class="string">'float32'</span>)</span><br><span class="line">    <span class="keyword">return</span> transform</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data</span><span class="params">(batch_size, train_augs, test_augs=None)</span>:</span></span><br><span class="line">    cifar10_train = gluon.data.vision.CIFAR10(train=<span class="keyword">True</span>, transform=get_transform(train_augs))</span><br><span class="line">    cifar10_test = gluon.data.vision.CIFAR10(train=<span class="keyword">False</span>, transform=get_transform(test_augs))</span><br><span class="line">    train_data = gluon.data.DataLoader(cifar10_train, batch_size, shuffle=<span class="keyword">True</span>)</span><br><span class="line">    test_data = gluon.data.DataLoader(cifar10_test, batch_size, shuffle=<span class="keyword">False</span>)</span><br><span class="line">    <span class="keyword">return</span> (train_data, test_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出前几张看看</span></span><br><span class="line">train_data, _ = get_data(<span class="number">36</span>, train_augs)</span><br><span class="line"><span class="keyword">for</span> imgs, _ <span class="keyword">in</span> train_data:</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">_, figs = plt.subplots(<span class="number">6</span>, <span class="number">6</span>, figsize=(<span class="number">6</span>, <span class="number">6</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">6</span>):</span><br><span class="line">        x = nd.transpose(imgs[i * <span class="number">3</span> + j], (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))</span><br><span class="line">        figs[i][j].imshow(x.asnumpy())</span><br><span class="line">        figs[i][j].axes.get_xaxis().set_visible(<span class="keyword">False</span>)</span><br><span class="line">        figs[i][j].axes.get_yaxis().set_visible(<span class="keyword">False</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练 我们使用[ResNet 18]训练。并且训练代码整理成一个函数使得可以重读调用：</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(train_augs, test_augs, learning_rate=<span class="number">.1</span>)</span>:</span></span><br><span class="line">    batch_size = <span class="number">128</span></span><br><span class="line">    num_epochs = <span class="number">10</span></span><br><span class="line">    ctx = mx.gpu(<span class="number">0</span>)</span><br><span class="line">    loss = gluon.loss.SoftmaxCrossEntropyLoss()</span><br><span class="line">    train_data, test_data = get_data(</span><br><span class="line">        batch_size, train_augs, test_augs)</span><br><span class="line">    net = utils.resnet18(<span class="number">10</span>)</span><br><span class="line">    net.initialize(ctx=ctx, init=init.Xavier())</span><br><span class="line">    net.hybridize()</span><br><span class="line">    trainer = gluon.Trainer(net.collect_params(),</span><br><span class="line">                            <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: learning_rate&#125;)</span><br><span class="line">    utils.train(</span><br><span class="line">        train_data, test_data, net, loss, trainer, ctx, num_epochs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用增强：</span></span><br><span class="line">train(train_augs, test_augs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不使用增强：</span></span><br><span class="line"><span class="comment"># train(test_augs, test_augs)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以看到使用增强后，训练精度提升更慢，但测试精度比不使用更好。</span></span><br><span class="line"><span class="comment">## 总结</span></span><br><span class="line"><span class="comment"># 图片增强可以有效避免过拟合。</span></span><br><span class="line"><span class="comment">## 练习</span></span><br><span class="line"><span class="comment"># 尝试换不同的增强方法试试。</span></span><br></pre></td></tr></table></figure></p>
<h2 id="8-1__u4F7F_u7528Gluon_u5B9E_u73B0SSD"><a href="#8-1__u4F7F_u7528Gluon_u5B9E_u73B0SSD" class="headerlink" title="8.1 使用Gluon实现SSD"></a>8.1 使用Gluon实现SSD</h2><p>本章利用介绍的SSD来检测野生皮卡丘<br><img src="/img/deeplearn/pikachu.png" alt="pikachu"><br>数据集下载：<br><a href="https://apache-mxnet.s3-accelerate.amazonaws.com/gluon/dataset/pikachu/train.rec" target="_blank" rel="external">训练数据rec下载</a><br><a href="https://apache-mxnet.s3-accelerate.amazonaws.com/gluon/dataset/pikachu/train.idx" target="_blank" rel="external">训练数据idx下载</a><br><a href="https://apache-mxnet.s3-accelerate.amazonaws.com/gluon/dataset/pikachu/val.rec" target="_blank" rel="external">测试数据rec下载</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### 读取数据集</span></span><br><span class="line"><span class="comment"># 我们使用`image.ImageDetIter`来读取数据。</span></span><br><span class="line"><span class="comment"># 这是针对物体检测的迭代器，(Det表示Detection)。它跟`image.ImageIter`使用很类似。</span></span><br><span class="line"><span class="comment"># 主要不同是它返回的标号不是单个图片标号，而是每个图片里所有物体的标号，以及其对用的边框。</span></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> image</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"></span><br><span class="line">data_dir=<span class="string">'../data/pikachu/'</span></span><br><span class="line">data_shape = <span class="number">256</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">rgb_mean = nd.array([<span class="number">123</span>, <span class="number">117</span>, <span class="number">104</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_iterators</span><span class="params">(data_shape, batch_size)</span>:</span></span><br><span class="line">    class_names = [<span class="string">'pikachu'</span>]</span><br><span class="line">    num_class = len(class_names)</span><br><span class="line">    train_iter = image.ImageDetIter(</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        data_shape=(<span class="number">3</span>, data_shape, data_shape),</span><br><span class="line">        path_imgrec=data_dir+<span class="string">'train.rec'</span>,</span><br><span class="line">        path_imgidx=data_dir+<span class="string">'train.idx'</span>,</span><br><span class="line">        shuffle=<span class="keyword">True</span>,</span><br><span class="line">        mean=<span class="keyword">True</span>,</span><br><span class="line">        rand_crop=<span class="number">1</span>,</span><br><span class="line">        min_object_covered=<span class="number">0.95</span>,</span><br><span class="line">        max_attempts=<span class="number">200</span>)</span><br><span class="line">    val_iter = image.ImageDetIter(</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        data_shape=(<span class="number">3</span>, data_shape, data_shape),</span><br><span class="line">        path_imgrec=data_dir+<span class="string">'val.rec'</span>,</span><br><span class="line">        shuffle=<span class="keyword">False</span>,</span><br><span class="line">        mean=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> train_iter, val_iter, class_names, num_class</span><br><span class="line"></span><br><span class="line">train_data, test_data, class_names, num_class = get_iterators(data_shape, batch_size)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们读取一个批量。可以看到标号的形状是`batch_size x num_object_per_image x 5`。这里数据里每个图片里面只有一个标号。</span></span><br><span class="line"><span class="comment"># 每个标号由长为5的数组表示，第一个元素是其对用物体的标号，其中`-1`表示非法物体，仅做填充使用。后面4个元素表示边框。</span></span><br><span class="line">batch = train_data.next()</span><br><span class="line">print(batch) <span class="comment">#DataBatch: data shapes: [(32L, 3L, 256L, 256L)] label shapes: [(32L, 1L, 5L)] # 1L 因为每张图只有一个pikachu 如果有多个的化将取最多的那个数</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 图示数据</span></span><br><span class="line"><span class="comment"># 我们画出几张图片和其对应的标号。可以看到比卡丘的角度大小位置在每张图图片都不一样。不过也注意到这个数据集是直接将二次元动漫皮卡丘跟三次元背景相结合。</span></span><br><span class="line"><span class="comment"># 可能通过简单判断区域的色彩直方图就可以有效的区别是不是有我们要的物体。我们用这个简单数据集来演示SSD是如何工作的。实际中遇到的数据集通常会复杂很多。</span></span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line">mpl.rcParams[<span class="string">'figure.dpi'</span>]= <span class="number">120</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">box_to_rect</span><span class="params">(box, color, linewidth=<span class="number">3</span>)</span>:</span></span><br><span class="line">    <span class="string">"""convert an anchor box to a matplotlib rectangle"""</span></span><br><span class="line">    box = box.asnumpy()</span><br><span class="line">    <span class="keyword">return</span> plt.Rectangle(</span><br><span class="line">        (box[<span class="number">0</span>], box[<span class="number">1</span>]), box[<span class="number">2</span>]-box[<span class="number">0</span>], box[<span class="number">3</span>]-box[<span class="number">1</span>],</span><br><span class="line">        fill=<span class="keyword">False</span>, edgecolor=color, linewidth=linewidth)</span><br><span class="line"></span><br><span class="line">_, figs = plt.subplots(<span class="number">3</span>, <span class="number">3</span>, figsize=(<span class="number">6</span>,<span class="number">6</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        img, labels = batch.data[<span class="number">0</span>][<span class="number">3</span>*i+j], batch.label[<span class="number">0</span>][<span class="number">3</span>*i+j] <span class="comment">###########</span></span><br><span class="line">        img = img.transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)) + rgb_mean</span><br><span class="line">        img = img.clip(<span class="number">0</span>,<span class="number">255</span>).asnumpy()/<span class="number">255</span></span><br><span class="line">        fig = figs[i][j]</span><br><span class="line">        fig.imshow(img)</span><br><span class="line">        <span class="keyword">for</span> label <span class="keyword">in</span> labels:</span><br><span class="line">            rect = box_to_rect(label[<span class="number">1</span>:<span class="number">5</span>]*data_shape,<span class="string">'red'</span>,<span class="number">2</span>)</span><br><span class="line">            fig.add_patch(rect)</span><br><span class="line">        fig.axes.get_xaxis().set_visible(<span class="keyword">False</span>)</span><br><span class="line">        fig.axes.get_yaxis().set_visible(<span class="keyword">False</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">## SSD模型</span></span><br><span class="line"><span class="comment">### 锚框：默认的边界框</span></span><br><span class="line"><span class="comment"># 因为边框可以出现在图片中的任何位置，并且可以有任意大小。为了简化计算，SSD跟Faster R-CNN一样使用一些默认的边界框，或者称之为锚框（anchor box），做为搜索起点。</span></span><br><span class="line"><span class="comment"># 具体来说，对输入的每个像素，以其为中心采样数个有不同形状和不同比例的边界框。假设输入大小是 w x h，</span></span><br><span class="line"><span class="comment"># - 给定大小 s in (0,1]，那么生成的边界框形状是 [ws,hs]</span></span><br><span class="line"><span class="comment"># - 给定比例 r &gt; 0，那么生成的边界框形状是 [w*sqrt&#123;r&#125;,h\sqrt&#123;r&#125;]</span></span><br><span class="line"><span class="comment"># 在采样的时候我们提供 n 个大小（`sizes`）和 m 个比例（`ratios`）。为了计算简单这里不生成nm个锚框，而是n+m-1个。其中第 i 个锚框使用</span></span><br><span class="line"><span class="comment"># - `sizes[i]`和`ratios[0]` 如果 i &lt;= n</span></span><br><span class="line"><span class="comment"># - `sizes[0]`和`ratios[i-n]` 如果 i&gt;n</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们可以使用`contribe.ndarray`里的`MultiBoxPrior`来采样锚框。这里锚框通过左下角和右上角两个点来确定，而且被标准化成了区间[0,1]的实数。</span></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"><span class="keyword">from</span> mxnet.ndarray.contrib <span class="keyword">import</span> MultiBoxPrior</span><br><span class="line"></span><br><span class="line"><span class="comment"># shape: batch x channel x height x weight</span></span><br><span class="line">n = <span class="number">40</span></span><br><span class="line">x = nd.random.uniform(shape=(<span class="number">1</span>, <span class="number">3</span>, n, n))</span><br><span class="line">y = MultiBoxPrior(x, sizes=[<span class="number">.5</span>,<span class="number">.25</span>,<span class="number">.1</span>], ratios=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">.5</span>])</span><br><span class="line">boxes = y.reshape((n, n, -<span class="number">1</span>, <span class="number">4</span>))</span><br><span class="line">print(boxes.shape) <span class="comment">##(40L, 40L, 5L, 4L)</span></span><br><span class="line"><span class="comment"># The first anchor box centered on (20, 20)</span></span><br><span class="line"><span class="comment"># its format is (x_min, y_min, x_max, y_max)</span></span><br><span class="line"><span class="comment"># 我们可以画出以`(20,20)`为中心的所有锚框：</span></span><br><span class="line">colors = [<span class="string">'blue'</span>, <span class="string">'green'</span>, <span class="string">'red'</span>, <span class="string">'black'</span>, <span class="string">'magenta'</span>]</span><br><span class="line">plt.imshow(nd.ones((n, n, <span class="number">3</span>)).asnumpy())</span><br><span class="line">anchors = boxes[<span class="number">20</span>, <span class="number">20</span>, :, :]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(anchors.shape[<span class="number">0</span>]):</span><br><span class="line">    plt.gca().add_patch(box_to_rect(anchors[i,:]*n, colors[i]))</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### 预测物体类别</span></span><br><span class="line"><span class="comment"># 对每一个锚框我们需要预测它是不是包含了我们感兴趣的物体，还是只是背景。这里我们使用一个3x3的卷积层来做预测，加上`pad=1`使用它的输出和输入一样。</span></span><br><span class="line"><span class="comment"># 同时输出的通道数是`num_anchors*(num_classes+1)`，每个通道对应一个锚框对某个类的置信度。假设输出是`Y`，</span></span><br><span class="line"><span class="comment"># 那么对应输入中第n个样本的第(i,j)像素的置信值是在`Y[n,:,i,j]`里。具体来说，对于以`(i,j)`为中心的第`a`个锚框，</span></span><br><span class="line"><span class="comment"># - 通道 `a*(num_class+1)` 是其只包含背景的分数</span></span><br><span class="line"><span class="comment"># - 通道 `a*(num_class+1)+1+b` 是其包含第`b`个物体的分数</span></span><br><span class="line"><span class="comment"># 我们定义个一个这样的类别分类器函数：</span></span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">class_predictor</span><span class="params">(num_anchors, num_classes)</span>:</span></span><br><span class="line">    <span class="string">"""return a layer to predict classes"""</span></span><br><span class="line">    <span class="keyword">return</span> nn.Conv2D(num_anchors * (num_classes + <span class="number">1</span>), <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">cls_pred = class_predictor(<span class="number">5</span>, <span class="number">10</span>) <span class="comment"># 5个框 10类物体</span></span><br><span class="line">cls_pred.initialize()</span><br><span class="line">x = nd.zeros((<span class="number">2</span>, <span class="number">3</span>, <span class="number">20</span>, <span class="number">20</span>))</span><br><span class="line">y = cls_pred(x)</span><br><span class="line"><span class="keyword">print</span> y.shape <span class="comment">### (2L, 55L, 20L, 20L) #55=5*(10+1)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### 预测边界框</span></span><br><span class="line"><span class="comment"># 因为真实的边界框可以是任意形状，我们需要预测如何从一个锚框变换成真正的边界框。这个变换可以由一个长为4的向量来描述。同上一样，</span></span><br><span class="line"><span class="comment"># 我们用一个有`num_anchors * 4`通道的卷积。假设输出是Y，那么对应输入中第 n 个样本的第 (i,j)</span></span><br><span class="line"><span class="comment"># 像素为中心的锚框的转换在`Y[n,:,i,j]`里。具体来说，对于第`a`个锚框，它的变换在`a*4`到`a*4+3`通道里。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">box_predictor</span><span class="params">(num_anchors)</span>:</span></span><br><span class="line">    <span class="string">"""return a layer to predict delta locations"""</span></span><br><span class="line">    <span class="keyword">return</span> nn.Conv2D(num_anchors * <span class="number">4</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">box_pred = box_predictor(<span class="number">10</span>)</span><br><span class="line">box_pred.initialize()</span><br><span class="line">x = nd.zeros((<span class="number">2</span>, <span class="number">3</span>, <span class="number">20</span>, <span class="number">20</span>))</span><br><span class="line">y = box_pred(x)</span><br><span class="line"><span class="keyword">print</span> y.shape  <span class="comment"># (2L, 40L, 20L, 20L)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### 减半模块</span></span><br><span class="line"><span class="comment"># 我们定义一个卷积块，它将输入特征的长宽减半，以此来获取多尺度的预测。它由两个`Conv-BatchNorm-Relu`组成，</span></span><br><span class="line"><span class="comment"># 我们使用填充为1的3*3卷积使得输入和输入有同样的长宽，然后再通过跨度为2的最大池化层将长宽减半。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">down_sample</span><span class="params">(num_filters)</span>:</span></span><br><span class="line">    <span class="string">"""stack two Conv-BatchNorm-Relu blocks and then a pooling layer</span><br><span class="line">    to halve the feature size"""</span></span><br><span class="line">    out = nn.HybridSequential()</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">        out.add(nn.Conv2D(num_filters, <span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="number">1</span>))</span><br><span class="line">        out.add(nn.BatchNorm(in_channels=num_filters))</span><br><span class="line">        out.add(nn.Activation(<span class="string">'relu'</span>))</span><br><span class="line">    out.add(nn.MaxPool2D(<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">blk = down_sample(<span class="number">10</span>)</span><br><span class="line">blk.initialize()</span><br><span class="line">x = nd.zeros((<span class="number">2</span>, <span class="number">3</span>, <span class="number">20</span>, <span class="number">20</span>))</span><br><span class="line">y = blk(x)</span><br><span class="line"><span class="keyword">print</span> y.shape <span class="comment">#(2L, 10L, 10L, 10L)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### 合并来自不同层的预测输出</span></span><br><span class="line"><span class="comment"># 前面我们提到过SSD的一个重要性质是它会在多个层同时做预测。每个层由于长宽和锚框选择不一样，导致输出的数据形状会不一样。这里我们用物体类别预测作为样例，边框预测是类似的。</span></span><br><span class="line"><span class="comment"># 我们首先创建一个特定大小的输入，然后对它输出类别预测。然后对输入减半，再输出类别预测。</span></span><br><span class="line">x = nd.zeros((<span class="number">2</span>, <span class="number">8</span>, <span class="number">20</span>, <span class="number">20</span>))</span><br><span class="line">print(<span class="string">'x:'</span>, x.shape)</span><br><span class="line"></span><br><span class="line">cls_pred1 = class_predictor(<span class="number">5</span>, <span class="number">10</span>)</span><br><span class="line">cls_pred1.initialize()</span><br><span class="line">y1 = cls_pred1(x)</span><br><span class="line">print(<span class="string">'Class prediction 1:'</span>, y1.shape) <span class="comment">#(2L, 55L, 20L, 20L))</span></span><br><span class="line"></span><br><span class="line">ds = down_sample(<span class="number">16</span>)</span><br><span class="line">ds.initialize()</span><br><span class="line">x = ds(x)</span><br><span class="line">print(<span class="string">'x:'</span>, x.shape) <span class="comment"># (2L, 16L, 10L, 10L))</span></span><br><span class="line"></span><br><span class="line">cls_pred2 = class_predictor(<span class="number">3</span>, <span class="number">10</span>)</span><br><span class="line">cls_pred2.initialize()</span><br><span class="line">y2 = cls_pred2(x)</span><br><span class="line">print(<span class="string">'Class prediction 2:'</span>, y2.shape) <span class="comment">#(2L, 33L, 10L, 10L))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以看到`y1`和`y2`形状不同。为了之后处理简单，我们将不同层的输入合并成一个输出。首先我们将通道移到最后的维度，然后将其展成2D数组。</span></span><br><span class="line"><span class="comment"># 因为第一个维度是样本个数，所以不同输出之间是不变，我们可以将所有输出在第二个维度上拼接起来。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatten_prediction</span><span class="params">(pred)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> pred.transpose(axes=(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>)).flatten()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">concat_predictions</span><span class="params">(preds)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> nd.concat(*preds, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">flat_y1 = flatten_prediction(y1)</span><br><span class="line">print(<span class="string">'Flatten class prediction 1'</span>, flat_y1.shape) <span class="comment"># (2L, 22000L)</span></span><br><span class="line">flat_y2 = flatten_prediction(y2)</span><br><span class="line">print(<span class="string">'Flatten class prediction 2'</span>, flat_y2.shape) <span class="comment"># (2L, 3300L)</span></span><br><span class="line">y = concat_predictions([flat_y1, flat_y2])</span><br><span class="line">print(<span class="string">'Concat class predictions'</span>, y.shape) <span class="comment"># (2L, 25300L)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### 主体网络</span></span><br><span class="line"><span class="comment"># 主体网络用来从原始像素抽取特征。通常前面介绍的用来图片分类的卷积神经网络，例如ResNet，都可以用来作为主体网络。这里为了示范，我们简单叠加几个减半模块作为主体网络。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">body</span><span class="params">()</span>:</span></span><br><span class="line">    out = nn.HybridSequential()</span><br><span class="line">    <span class="keyword">for</span> nfilters <span class="keyword">in</span> [<span class="number">16</span>, <span class="number">32</span>, <span class="number">64</span>]:</span><br><span class="line">        out.add(down_sample(nfilters))</span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">bnet = body()</span><br><span class="line">bnet.initialize()</span><br><span class="line">x = nd.random.uniform(shape=(<span class="number">2</span>,<span class="number">3</span>,<span class="number">256</span>,<span class="number">256</span>))</span><br><span class="line">y = bnet(x)</span><br><span class="line"><span class="keyword">print</span> y.shape <span class="comment"># (2L, 64L, 32L, 32L)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### 创建一个玩具SSD模型</span></span><br><span class="line"><span class="comment"># 现在我们可以创建一个玩具SSD模型了。我们称之为玩具是因为这个网络不管是层数还是锚框个数都比较小，仅仅适合之后我们使用的一个小数据集。但这个模型不会影响我们介绍SSD。</span></span><br><span class="line"><span class="comment"># 这个网络包含四块。主体网络，三个减半模块，以及五个物体类别和边框预测模块。其中预测分别应用在在主体网络输出，减半模块输入，和最后的全局池化层上。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">toy_ssd_model</span><span class="params">(num_anchors, num_classes)</span>:</span></span><br><span class="line">    downsamplers = nn.Sequential()</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        downsamplers.add(down_sample(<span class="number">128</span>))</span><br><span class="line"></span><br><span class="line">    class_predictors = nn.Sequential()</span><br><span class="line">    box_predictors = nn.Sequential()</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        class_predictors.add(class_predictor(num_anchors, num_classes))</span><br><span class="line">        box_predictors.add(box_predictor(num_anchors))</span><br><span class="line"></span><br><span class="line">    model = nn.Sequential()</span><br><span class="line">    model.add(body(), downsamplers, class_predictors, box_predictors)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="comment">### 计算预测</span></span><br><span class="line"><span class="comment"># 给定模型和每层预测输出使用的锚框大小和形状，我们可以定义前向函数。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">toy_ssd_forward</span><span class="params">(x, model, sizes, ratios, verbose=False)</span>:</span></span><br><span class="line">    body, downsamplers, class_predictors, box_predictors = model</span><br><span class="line">    anchors, class_preds, box_preds = [], [], []</span><br><span class="line">    <span class="comment"># feature extraction</span></span><br><span class="line">    x = body(x)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        <span class="comment"># predict</span></span><br><span class="line">        anchors.append(MultiBoxPrior(x, sizes=sizes[i], ratios=ratios[i]))</span><br><span class="line">        class_preds.append(flatten_prediction(class_predictors[i](x)))</span><br><span class="line">        box_preds.append(flatten_prediction(box_predictors[i](x)))</span><br><span class="line">        <span class="keyword">if</span> verbose:</span><br><span class="line">            print(<span class="string">'Predict scale'</span>, i, x.shape, <span class="string">'with'</span>,anchors[-<span class="number">1</span>].shape[<span class="number">1</span>], <span class="string">'anchors'</span>)</span><br><span class="line">        <span class="comment"># down sample</span></span><br><span class="line">        <span class="keyword">if</span> i &lt; <span class="number">3</span>:</span><br><span class="line">            x = downsamplers[i](x)</span><br><span class="line">        <span class="keyword">elif</span> i == <span class="number">3</span>:</span><br><span class="line">            x = nd.Pooling(</span><br><span class="line">                x, global_pool=<span class="keyword">True</span>, pool_type=<span class="string">'max'</span>,</span><br><span class="line">                kernel=(x.shape[<span class="number">2</span>], x.shape[<span class="number">3</span>]))</span><br><span class="line">    <span class="comment"># concat date</span></span><br><span class="line">    <span class="keyword">return</span> (concat_predictions(anchors),</span><br><span class="line">            concat_predictions(class_preds),</span><br><span class="line">            concat_predictions(box_preds))</span><br><span class="line"></span><br><span class="line"><span class="comment">### 完整的模型</span></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ToySSD</span><span class="params">(gluon.Block)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes, verbose=False, **kwargs)</span>:</span></span><br><span class="line">        super(ToySSD, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># anchor box sizes and ratios for 5 feature scales</span></span><br><span class="line">        self.sizes = [[<span class="number">.2</span>,<span class="number">.272</span>], [<span class="number">.37</span>,<span class="number">.447</span>], [<span class="number">.54</span>,<span class="number">.619</span>], [<span class="number">.71</span>,<span class="number">.79</span>], [<span class="number">.88</span>,<span class="number">.961</span>]]</span><br><span class="line">        self.ratios = [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">.5</span>]]*<span class="number">5</span>  <span class="comment">#[[1, 2, 0.5], [1, 2, 0.5], [1, 2, 0.5], [1, 2, 0.5], [1, 2, 0.5]]</span></span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.verbose = verbose</span><br><span class="line">        num_anchors = len(self.sizes[<span class="number">0</span>]) + len(self.ratios[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line">        <span class="comment"># use name_scope to guard the names</span></span><br><span class="line">        <span class="keyword">with</span> self.name_scope():</span><br><span class="line">            self.model = toy_ssd_model(num_anchors, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        anchors, class_preds, box_preds = toy_ssd_forward(</span><br><span class="line">            x, self.model, self.sizes, self.ratios,</span><br><span class="line">            verbose=self.verbose)</span><br><span class="line">        <span class="comment"># it is better to have class predictions reshaped for softmax computation</span></span><br><span class="line">        class_preds = class_preds.reshape(shape=(<span class="number">0</span>, -<span class="number">1</span>, self.num_classes+<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> anchors, class_preds, box_preds</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们看看一下输入图片的形状是如何改变的，已经输出的形状。</span></span><br><span class="line">net = ToySSD(num_classes=<span class="number">2</span>, verbose=<span class="keyword">True</span>)</span><br><span class="line">net.initialize()</span><br><span class="line">x = batch.data[<span class="number">0</span>][<span class="number">0</span>:<span class="number">1</span>]</span><br><span class="line">print(<span class="string">'Input:'</span>, x.shape)</span><br><span class="line">anchors, class_preds, box_preds = net(x)</span><br><span class="line">print(<span class="string">'Output achors:'</span>, anchors.shape)</span><br><span class="line">print(<span class="string">'Output class predictions:'</span>, class_preds.shape)</span><br><span class="line">print(<span class="string">'Output box predictions:'</span>, box_preds.shape)</span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># ('Input:', (1L, 3L, 256L, 256L))</span></span><br><span class="line"><span class="comment"># ('Predict scale', 0, (1L, 64L, 32L, 32L), 'with', 4096L, 'anchors')</span></span><br><span class="line"><span class="comment"># ('Predict scale', 1, (1L, 128L, 16L, 16L), 'with', 1024L, 'anchors')</span></span><br><span class="line"><span class="comment"># ('Predict scale', 2, (1L, 128L, 8L, 8L), 'with', 256L, 'anchors')</span></span><br><span class="line"><span class="comment"># ('Predict scale', 3, (1L, 128L, 4L, 4L), 'with', 64L, 'anchors')</span></span><br><span class="line"><span class="comment"># ('Predict scale', 4, (1L, 128L, 1L, 1L), 'with', 4L, 'anchors')</span></span><br><span class="line"><span class="comment"># ('Output achors:', (1L, 5444L, 4L))</span></span><br><span class="line"><span class="comment"># ('Output class predictions:', (1L, 5444L, 3L))</span></span><br><span class="line"><span class="comment"># ('Output box predictions:', (1L, 21776L))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### 损失函数</span></span><br><span class="line"><span class="comment"># 虽然每张图片里面通常只有几个标注的边框，但SSD会生成大量的锚框。可以想象很多锚框都不会框住感兴趣的物体，就是说跟任何对应感兴趣物体的表框的IoU都小于某个阈值。</span></span><br><span class="line"><span class="comment"># 这样就会产生大量的负类锚框，或者说对应标号为0的锚框。对于这类锚框有两点要考虑的：</span></span><br><span class="line"><span class="comment"># 1. 边框预测的损失函数不应该包括负类锚框，因为它们并没有对应的真实边框</span></span><br><span class="line"><span class="comment"># 1. 因为负类锚框数目可能远多于其他，我们可以只保留其中的一些。而且是保留那些目前预测最不确信它是负类的，就是对类0预测值排序，选取数值最小的哪一些困难的负类锚框。</span></span><br><span class="line"><span class="comment"># 我们可以使用`MultiBoxTarget`来完成上面这两个操作。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mxnet.contrib.ndarray <span class="keyword">import</span> MultiBoxTarget</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_targets</span><span class="params">(anchors, class_preds, labels)</span>:</span></span><br><span class="line">    class_preds = class_preds.transpose(axes=(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> MultiBoxTarget(anchors, labels, class_preds)</span><br><span class="line"></span><br><span class="line">out = training_targets(anchors, class_preds, batch.label[<span class="number">0</span>][<span class="number">0</span>:<span class="number">1</span>])</span><br><span class="line"><span class="keyword">print</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 它返回三个`NDArray`，分别是</span></span><br><span class="line"><span class="comment"># 1. 预测的边框跟真实边框的偏移，大小是`batch_size x (num_anchors*4)`</span></span><br><span class="line"><span class="comment"># 1. 用来遮掩不需要的负类锚框的掩码，大小跟上面一致</span></span><br><span class="line"><span class="comment"># 1. 锚框的真实的标号，大小是`batch_size x num_anchors`</span></span><br><span class="line"><span class="comment"># 我们可以计算这次只选中了多少个锚框进入损失函数：</span></span><br><span class="line"><span class="keyword">print</span> out[<span class="number">1</span>].sum()/<span class="number">4</span>  <span class="comment">#[ 36.]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后我们可以定义需要的损失函数了。</span></span><br><span class="line"><span class="comment"># 对于分类问题，最常用的损失函数是之前一直使用的交叉熵。这里我们定义一个类似于交叉熵的损失，不同于交叉熵的定义 $\log(p_j)$，</span></span><br><span class="line"><span class="comment"># 这里 $j$ 是真实的类别，且 $p_j$ 是对于的预测概率。我们使用一个被称之为关注损失的函数，给定正的$\gamma$和$\alpha$，它的定义是</span></span><br><span class="line"><span class="comment"># $$ - \alpha (1-p_j)^&#123;\gamma&#125; \log(p_j) $$</span></span><br><span class="line"><span class="comment"># 下图我们演示不同$\gamma$导致的变化。可以看到，增加$\gamma$可以使得对正类预测值比较大时损失变小。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">focal_loss</span><span class="params">(gamma, x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> - (<span class="number">1</span>-x)**gamma*np.log(x)</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">0.01</span>, <span class="number">1</span>, <span class="number">.01</span>)</span><br><span class="line">gammas = [<span class="number">0</span>,<span class="number">.25</span>,<span class="number">.5</span>,<span class="number">1</span>]</span><br><span class="line"><span class="keyword">for</span> i,g <span class="keyword">in</span> enumerate(gammas):</span><br><span class="line">    plt.plot(x, focal_loss(g,x), colors[i])</span><br><span class="line"></span><br><span class="line">plt.legend([<span class="string">'gamma='</span>+str(g) <span class="keyword">for</span> g <span class="keyword">in</span> gammas])</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个自定义的损失函数可以简单通过继承`gluon.loss.Loss`来实现。</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FocalLoss</span><span class="params">(gluon.loss.Loss)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, axis=-<span class="number">1</span>, alpha=<span class="number">0.25</span>, gamma=<span class="number">2</span>, batch_axis=<span class="number">0</span>, **kwargs)</span>:</span></span><br><span class="line">        super(FocalLoss, self).__init__(<span class="keyword">None</span>, batch_axis, **kwargs)</span><br><span class="line">        self._axis = axis</span><br><span class="line">        self._alpha = alpha</span><br><span class="line">        self._gamma = gamma</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hybrid_forward</span><span class="params">(self, F, output, label)</span>:</span></span><br><span class="line">        output = F.softmax(output)</span><br><span class="line">        pj = output.pick(label, axis=self._axis, keepdims=<span class="keyword">True</span>)</span><br><span class="line">        loss = - self._alpha * ((<span class="number">1</span> - pj) ** self._gamma) * pj.log()</span><br><span class="line">        <span class="keyword">return</span> loss.mean(axis=self._batch_axis, exclude=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cls_loss = FocalLoss()</span><br><span class="line"><span class="keyword">print</span> cls_loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于边框的预测是一个回归问题。通常可以选择平方损失函数（L2损失）$f(x) = x ^ 2$。</span></span><br><span class="line"><span class="comment"># 但这个损失对于比较大的误差的惩罚很高。我们可以采用稍微缓和一点绝对损失函数（L1损失）$f(x) = | x |$，</span></span><br><span class="line"><span class="comment"># 它是随着误差线性增长，而不是平方增长。但这个函数在0点处倒是不唯一，因此可能会影响收敛。一个通常的解决办法是在0点附近使用平方函数使得它更加平滑。</span></span><br><span class="line"><span class="comment"># 它被称之为平滑L1损失函数。它通过一个参数$\sigma$来控制平滑的区域：</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># $$</span></span><br><span class="line"><span class="comment"># f(x) =</span></span><br><span class="line"><span class="comment"># \begin</span></span><br><span class="line"><span class="comment"># &#123;cases&#125;</span></span><br><span class="line"><span class="comment"># (\sigma</span></span><br><span class="line"><span class="comment"># x) ^ 2 / 2, &amp;  \text</span></span><br><span class="line"><span class="comment"># &#123; if&#125;x &lt; 1 /\sigma ^ 2\ \</span></span><br><span class="line"><span class="comment">#     | x | -0.5 /\sigma ^ 2, &amp;  \text</span></span><br><span class="line"><span class="comment"># &#123;otherwise&#125;</span></span><br><span class="line"><span class="comment"># \end</span></span><br><span class="line"><span class="comment"># &#123;cases&#125;</span></span><br><span class="line"><span class="comment"># $$</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们图示不同的$\sigma$的平滑L1损失和L2损失的区别。</span></span><br><span class="line">scales = [<span class="number">.5</span>, <span class="number">1</span>, <span class="number">10</span>]</span><br><span class="line">x = nd.arange(-<span class="number">2</span>, <span class="number">2</span>, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, s <span class="keyword">in</span> enumerate(scales):</span><br><span class="line">    y = nd.smooth_l1(x, scalar=s)</span><br><span class="line">    plt.plot(x.asnumpy(), y.asnumpy(), color=colors[i])</span><br><span class="line">plt.plot(x.asnumpy(), (x ** <span class="number">2</span>).asnumpy(), color=colors[len(scales)])</span><br><span class="line">plt.legend([<span class="string">'scale='</span> + str(s) <span class="keyword">for</span> s <span class="keyword">in</span> scales] + [<span class="string">'Square loss'</span>])</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们同样通过继承`Loss`来定义这个损失。同时它接受一个额外参数`mask`，这是用来屏蔽掉不需要被惩罚的负例样本。</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SmoothL1Loss</span><span class="params">(gluon.loss.Loss)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, batch_axis=<span class="number">0</span>, **kwargs)</span>:</span></span><br><span class="line">        super(SmoothL1Loss, self).__init__(<span class="keyword">None</span>, batch_axis, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hybrid_forward</span><span class="params">(self, F, output, label, mask)</span>:</span></span><br><span class="line">        loss = F.smooth_l1((output - label) * mask, scalar=<span class="number">1.0</span>)</span><br><span class="line">        <span class="keyword">return</span> loss.mean(self._batch_axis, exclude=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">box_loss = SmoothL1Loss()</span><br><span class="line"><span class="keyword">print</span> box_loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### 评估测量</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 对于分类好坏我们可以沿用之前的分类精度。评估边框预测的好坏的一个常用是是平均绝对误差。记得在[线性回归]</span></span><br><span class="line"><span class="comment"># (.. / chapter_supervised - learning / linear - regression - scratch.md)我们使用了平均平方误差。</span></span><br><span class="line"><span class="comment"># 但跟上面对损失函数的讨论一样，平方误差对于大的误差给予过大的值，从而数值上过于敏感。平均绝对误差就是将二次项替换成绝对值，具体来说就是预测的边框和真实边框在4个维度上的差值的绝对值。</span></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> metric</span><br><span class="line">cls_metric = metric.Accuracy()</span><br><span class="line">box_metric = metric.MAE()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### 初始化模型和训练器</span></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gpu</span><br><span class="line">ctx = gpu(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># the CUDA implementation requres each image has at least 3 lables.</span></span><br><span class="line"><span class="comment"># Padd two -1 labels for each instance</span></span><br><span class="line">train_data.reshape(label_shape=(<span class="number">3</span>, <span class="number">5</span>))</span><br><span class="line">train_data = test_data.sync_label_shape(train_data)</span><br><span class="line"></span><br><span class="line">net = ToySSD(num_class)</span><br><span class="line">net.initialize(init.Xavier(magnitude=<span class="number">2</span>), ctx=ctx)</span><br><span class="line">trainer = gluon.Trainer(net.collect_params(),<span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.1</span>, <span class="string">'wd'</span>: <span class="number">5e-4</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">### 训练模型</span></span><br><span class="line"><span class="comment"># 训练函数跟前面的不一样在于网络会有多个输出，而且有两个损失函数。</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    <span class="comment"># reset data iterators and metrics</span></span><br><span class="line">    train_data.reset()</span><br><span class="line">    cls_metric.reset()</span><br><span class="line">    box_metric.reset()</span><br><span class="line">    tic = time.time()</span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(train_data):</span><br><span class="line">        x = batch.data[<span class="number">0</span>].as_in_context(ctx)</span><br><span class="line">        y = batch.label[<span class="number">0</span>].as_in_context(ctx)</span><br><span class="line">        <span class="keyword">with</span> autograd.record():</span><br><span class="line">            anchors, class_preds, box_preds = net(x)</span><br><span class="line">            box_target, box_mask, cls_target = training_targets(</span><br><span class="line">                anchors, class_preds, y)</span><br><span class="line">            <span class="comment"># losses</span></span><br><span class="line">            loss1 = cls_loss(class_preds, cls_target)</span><br><span class="line">            loss2 = box_loss(box_preds, box_target, box_mask)</span><br><span class="line">            loss = loss1 + loss2</span><br><span class="line">        loss.backward()</span><br><span class="line">        trainer.step(batch_size)</span><br><span class="line">        <span class="comment"># update metrics</span></span><br><span class="line">        cls_metric.update([cls_target], [class_preds.transpose((<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>))])</span><br><span class="line">        box_metric.update([box_target], [box_preds * box_mask])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># print('Epoch %2d, train %s %.2f, %s %.5f, time %.1f sec' % (epoch, cls_metric.get(), box_metric.get(), time.time() - tic))</span></span><br><span class="line">    print(<span class="string">'Epoch %2d, train %s %.2f, %s %.5f, time %.1f sec'</span> % (</span><br><span class="line">            epoch, cls_metric.get()[<span class="number">0</span>], cls_metric.get()[<span class="number">1</span>], box_metric.get()[<span class="number">0</span>], box_metric.get()[<span class="number">1</span>], time.time() - tic))</span><br><span class="line"></span><br><span class="line"><span class="comment">## 预测</span></span><br><span class="line"><span class="comment"># 在预测阶段，我们希望能把图片里面所有感兴趣的物体找出来。我们先定一个数据读取和预处理函数。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_image</span><span class="params">(fname)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(fname, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        im = image.imdecode(f.read())</span><br><span class="line">    <span class="comment"># resize to data_shape</span></span><br><span class="line">    data = image.imresize(im, data_shape, data_shape)</span><br><span class="line">    <span class="comment"># minus rgb mean</span></span><br><span class="line">    data = data.astype(<span class="string">'float32'</span>) - rgb_mean</span><br><span class="line">    <span class="comment"># convert to batch x channel x height xwidth</span></span><br><span class="line">    <span class="keyword">return</span> data.transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)).expand_dims(axis=<span class="number">0</span>), im</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后我们跟训练那样预测表框和其对应的物体。但注意到因为我们对每个像素都会生成数个锚框，这样我们可能会预测出大量相似的表框，从而导致结果非常嘈杂。</span></span><br><span class="line"><span class="comment"># 一个办法是对于IoU比较高的两个表框，我们只保留预测执行度比较高的那个。这个算法（称之为non maximum suppression）在`MultiBoxDetection`里实现了。下面我们实现预测函数：</span></span><br><span class="line"><span class="keyword">from</span> mxnet.contrib.ndarray <span class="keyword">import</span> MultiBoxDetection</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(x)</span>:</span></span><br><span class="line">    anchors, cls_preds, box_preds = net(x.as_in_context(ctx))</span><br><span class="line">    cls_probs = nd.SoftmaxActivation(</span><br><span class="line">        cls_preds.transpose((<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)), mode=<span class="string">'channel'</span>)</span><br><span class="line">    <span class="keyword">return</span> MultiBoxDetection(cls_probs, box_preds, anchors,force_suppress=<span class="keyword">True</span>, clip=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测函数会输出所有边框，每个边框由`[class_id, confidence, xmin, ymin, xmax, ymax]`表示。其中`class_id = -1`表示要么这个边框被预测只含有背景，或者被去重掉了。</span></span><br><span class="line">x, im = process_image(<span class="string">'../data/pikachu.jpg'</span>)</span><br><span class="line">out = predict(x)</span><br><span class="line"><span class="keyword">print</span> out.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后我们将预测出置信度超过某个阈值的边框画出来：</span></span><br><span class="line">mpl.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">6</span>, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display</span><span class="params">(im, out, threshold=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    plt.imshow(im.asnumpy())</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> out:</span><br><span class="line">        row = row.asnumpy()</span><br><span class="line">        class_id, score = int(row[<span class="number">0</span>]), row[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> class_id &lt; <span class="number">0</span> <span class="keyword">or</span> score &lt; threshold:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        color = colors[class_id % len(colors)]</span><br><span class="line">        box = row[<span class="number">2</span>:<span class="number">6</span>] * np.array([im.shape[<span class="number">0</span>], im.shape[<span class="number">1</span>]] * <span class="number">2</span>)</span><br><span class="line">        rect = box_to_rect(nd.array(box), color, <span class="number">2</span>)</span><br><span class="line">        plt.gca().add_patch(rect)</span><br><span class="line"></span><br><span class="line">        text = class_names[class_id]</span><br><span class="line">        plt.gca().text(box[<span class="number">0</span>], box[<span class="number">1</span>],</span><br><span class="line">                       <span class="string">'&#123;:s&#125; &#123;:.2f&#125;'</span>.format(text, score),</span><br><span class="line">                       bbox=dict(facecolor=color, alpha=<span class="number">0.5</span>),</span><br><span class="line">                       fontsize=<span class="number">10</span>, color=<span class="string">'white'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">display(im, out[<span class="number">0</span>], threshold=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 结论</span></span><br><span class="line"><span class="comment"># 物体检测比分类要困难很多。因为我们不仅要预测物体类别，还要找到它们的位置。这一章我们展示我们还是可以在合理篇幅里实现SSD算法。</span></span><br></pre></td></tr></table></figure>
</span>
      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Mxnet/" rel="tag">#Mxnet</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2016/11/17/linux_centos/" rel="next" title="CentOS">
                <i class="fa fa-chevron-left"></i> CentOS
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/11/24/tensorflowinstall/" rel="prev" title="tensorflow安装">
                tensorflow安装 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
        <div class="ds-share flat" data-thread-key="2017/10/13/deeplearn/"
     data-title="深度学习之Mxnet--李沐视频"
     data-content=""
     data-url="http://zhangjin4415.github.io/2017/10/13/deeplearn/">
  <div class="ds-share-inline">
    <ul  class="ds-share-icons-16">

      <li data-toggle="ds-share-icons-more"><a class="ds-more" href="javascript:void(0);">分享到：</a></li>
      <li><a class="ds-weibo" href="javascript:void(0);" data-service="weibo">微博</a></li>
      <li><a class="ds-qzone" href="javascript:void(0);" data-service="qzone">QQ空间</a></li>
      <li><a class="ds-qqt" href="javascript:void(0);" data-service="qqt">腾讯微博</a></li>
      <li><a class="ds-wechat" href="javascript:void(0);" data-service="wechat">微信</a></li>

    </ul>
    <div class="ds-share-icons-more">
    </div>
  </div>
</div>
      
    </div>
  </div>


        </div>

        


        
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="2017/10/13/deeplearn/"
           data-title="深度学习之Mxnet--李沐视频" data-url="http://zhangjin4415.github.io/2017/10/13/deeplearn/">
      </div>
    
  </div>


      </div>

      
        
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" src="/images/avatar.jpeg" alt="Jin Zhang" itemprop="image"/>
          <p class="site-author-name" itemprop="name">Jin Zhang</p>
        </div>
        <p class="site-description motion-element" itemprop="description">天高任鸟飞，海阔凭鱼跃。</p>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">34</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          <div class="site-state-item site-state-categories">
            <a href="/categories">
              <span class="site-state-item-count">20</span>
              <span class="site-state-item-name">分类</span>
              </a>
          </div>

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">35</span>
              <span class="site-state-item-name">标签</span>
              </a>
          </div>

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        <div class="links-of-author motion-element">
          
        </div>

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc-indicator-top post-toc-indicator">
            <i class="fa fa-angle-double-up"></i>
          </div>
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-_u524D_u8A00"><span class="nav-number">1.</span> <span class="nav-text">1.1.前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-_u4F7F_u7528NDArray_u6765_u5904_u7406_u6570_u636E"><span class="nav-number">2.</span> <span class="nav-text">1.2.使用NDArray来处理数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-_u4F7F_u7528autograd_u81EA_u52A8_u6C42_u5BFC"><span class="nav-number">3.</span> <span class="nav-text">1.3.使用autograd自动求导</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-4-_u4ECE0_u5F00_u59CB_u7EBF_u6027_u56DE_u5F52"><span class="nav-number">4.</span> <span class="nav-text">1.4.从0开始线性回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-5-_u4F7F_u7528Gluon_u5B9E_u73B0_u7EBF_u6027_u56DE_u5F52"><span class="nav-number">5.</span> <span class="nav-text">1.5.使用Gluon实现线性回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-6-_u4ECE0_u5F00_u59CB_u591A_u7C7B_u903B_u8F91_u56DE_u5F52"><span class="nav-number">6.</span> <span class="nav-text">1.6.从0开始多类逻辑回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-7-Gluon_u7248_u591A_u7C7B_u903B_u8F91_u56DE_u5F52"><span class="nav-number">7.</span> <span class="nav-text">1.7.Gluon版多类逻辑回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-_u4ECE0_u5F00_u59CB_u591A_u5C42_u611F_u77E5_u673A"><span class="nav-number">8.</span> <span class="nav-text">2.1.从0开始多层感知机</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-_u4F7F_u7528Gluon_u591A_u5C42_u611F_u77E5_u673A"><span class="nav-number">9.</span> <span class="nav-text">2.2.使用Gluon多层感知机</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-_u4ECE0_u5F00_u59CB_u6B63_u5219_u5316"><span class="nav-number">10.</span> <span class="nav-text">2.3.从0开始正则化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-_u4F7F_u7528Gluon_u6B63_u5219_u5316"><span class="nav-number">11.</span> <span class="nav-text">2.3.使用Gluon正则化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-4-_u4F7F_u7528GPU_u6765_u8BA1_u7B97"><span class="nav-number">12.</span> <span class="nav-text">2.4.使用GPU来计算</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-5-_u4ECE0_u5F00_u59CB_u5377_u79EF_u795E_u7ECF_u7F51_u7EDC"><span class="nav-number">13.</span> <span class="nav-text">2.5.从0开始卷积神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-6-_u4F7F_u7528gluon_u5377_u79EF_u795E_u7ECF_u7F51_u7EDC"><span class="nav-number">14.</span> <span class="nav-text">2.6.使用gluon卷积神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-_u521B_u5EFA_u795E_u7ECF_u7F51_u7EDC"><span class="nav-number">15.</span> <span class="nav-text">3.1.创建神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-_u521D_u59CB_u5316_u6A21_u578B_u53C2_u6570"><span class="nav-number">16.</span> <span class="nav-text">3.2.初始化模型参数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-_u5E8F_u5217_u5316_u8BFB_u5199_u6A21_u578B"><span class="nav-number">17.</span> <span class="nav-text">3.3.序列化读写模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-4-_u8BBE_u8BA1_u81EA_u5B9A_u4E49_u5C42"><span class="nav-number">18.</span> <span class="nav-text">3.4.设计自定义层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-5-dropout"><span class="nav-number">19.</span> <span class="nav-text">3.5.dropout</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-6-_u4F7F_u7528Gluon_u4E22_u5F03_u6CD5_uFF08Dropout_uFF09"><span class="nav-number">20.</span> <span class="nav-text">3.6.使用Gluon丢弃法（Dropout）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-7-_u6DF1_u5EA6_u5377_u79EF_u795E_u7ECF_u7F51_u7EDC_u548CAlexNet"><span class="nav-number">21.</span> <span class="nav-text">3.7.深度卷积神经网络和AlexNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-8-VGGNet"><span class="nav-number">22.</span> <span class="nav-text">3.8.VGGNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-__u4ECE0_u5F00_u59CB_u6279_u91CF_u5F52_u4E00_u5316BatchNorm"><span class="nav-number">23.</span> <span class="nav-text">4.1. 从0开始批量归一化BatchNorm</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-gluon_u7248batchnorm"><span class="nav-number">24.</span> <span class="nav-text">4.2.gluon版batchnorm</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-3-_u7F51_u7EDC_u4E2D_u7684_u7F51_u7EDCNIN"><span class="nav-number">25.</span> <span class="nav-text">4.3.网络中的网络NIN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-4-GoogleNet"><span class="nav-number">26.</span> <span class="nav-text">4.4.GoogleNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-5-Resnet"><span class="nav-number">27.</span> <span class="nav-text">4.5.Resnet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-6-DenseNet"><span class="nav-number">28.</span> <span class="nav-text">4.6.DenseNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-7-_u56FE_u7247_u589E_u5F3A"><span class="nav-number">29.</span> <span class="nav-text">4.7.图片增强</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-1__u4F7F_u7528Gluon_u5B9E_u73B0SSD"><span class="nav-number">30.</span> <span class="nav-text">8.1 使用Gluon实现SSD</span></a></li></ol></div>
            
          </div>
          <div class="post-toc-indicator-bottom post-toc-indicator">
            <i class="fa fa-angle-double-down"></i>
          </div>
        </section>
      

    </div>
  </aside>


      
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="icon-next-heart fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jin Zhang</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>



      </div>
    </footer>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  

  
    

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"bestzhangjin"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>
    
     


    
  

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js?v=0.4.5.2"></script>


  <script type="text/javascript" src="/js/helpers.js?v=0.4.5.2"></script>
  <script type="text/javascript" src="/lib/velocity/velocity.min.js"></script>
<script type="text/javascript" src="/lib/velocity/velocity.ui.min.js"></script>

<script type="text/javascript" src="/js/motion.js?v=0.4.5.2" id="motion.global"></script>


  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  
<script type="text/javascript" src="/js/bootstrap.scrollspy.js?v=0.4.5.2" id="bootstrap.scrollspy.custom"></script>


<script type="text/javascript" id="sidebar.toc.highlight">
  $(document).ready(function () {
    var tocSelector = '.post-toc';
    var $tocSelector = $(tocSelector);
    var activeCurrentSelector = '.active-current';

    $tocSelector
      .on('activate.bs.scrollspy', function () {
        var $currentActiveElement = $(tocSelector + ' .active').last();

        removeCurrentActiveClass();
        $currentActiveElement.addClass('active-current');

        $tocSelector[0].scrollTop = $currentActiveElement.position().top;
      })
      .on('clear.bs.scrollspy', function () {
        removeCurrentActiveClass();
      });

    function removeCurrentActiveClass () {
      $(tocSelector + ' ' + activeCurrentSelector)
        .removeClass(activeCurrentSelector.substring(1));
    }

    function processTOC () {
      getTOCMaxHeight();
      toggleTOCOverflowIndicators();
    }

    function getTOCMaxHeight () {
      var height = $('.sidebar').height() -
                   $tocSelector.position().top -
                   $('.post-toc-indicator-bottom').height();

      $tocSelector.css('height', height);

      return height;
    }

    function toggleTOCOverflowIndicators () {
      tocOverflowIndicator(
        '.post-toc-indicator-top',
        $tocSelector.scrollTop() > 0 ? 'show' : 'hide'
      );

      tocOverflowIndicator(
        '.post-toc-indicator-bottom',
        $tocSelector.scrollTop() >= $tocSelector.find('ol').height() - $tocSelector.height() ? 'hide' : 'show'
      )
    }

    $(document).on('sidebar.motion.complete', function () {
      processTOC();
    });

    $('body').scrollspy({ target: tocSelector });
    $(window).on('resize', function () {
      if ( $('.sidebar').hasClass('sidebar-active') ) {
        processTOC();
      }
    });

    onScroll($tocSelector);

    function onScroll (element) {
      element.on('mousewheel DOMMouseScroll', function (event) {
          var oe = event.originalEvent;
          var delta = oe.wheelDelta || -oe.detail;

          this.scrollTop += ( delta < 0 ? 1 : -1 ) * 30;
          event.preventDefault();

          toggleTOCOverflowIndicators();
      });
    }

    function tocOverflowIndicator (indicator, action) {
      var $indicator = $(indicator);
      var opacity = action === 'show' ? 1 : 0;
      $indicator.velocity ?
        $indicator.velocity('stop').velocity({
          opacity: opacity
        }, { duration: 100 }) :
        $indicator.stop().animate({
          opacity: opacity
        }, 100);
    }

  });
</script>

<script type="text/javascript" id="sidebar.nav">
  $(document).ready(function () {
    var html = $('html');
    var TAB_ANIMATE_DURATION = 200;
    var hasVelocity = $.isFunction(html.velocity);

    $('.sidebar-nav li').on('click', function () {
      var item = $(this);
      var activeTabClassName = 'sidebar-nav-active';
      var activePanelClassName = 'sidebar-panel-active';
      if (item.hasClass(activeTabClassName)) {
        return;
      }

      var currentTarget = $('.' + activePanelClassName);
      var target = $('.' + item.data('target'));

      hasVelocity ?
        currentTarget.velocity('transition.slideUpOut', TAB_ANIMATE_DURATION, function () {
          target
            .velocity('stop')
            .velocity('transition.slideDownIn', TAB_ANIMATE_DURATION)
            .addClass(activePanelClassName);
        }) :
        currentTarget.animate({ opacity: 0 }, TAB_ANIMATE_DURATION, function () {
          currentTarget.hide();
          target
            .stop()
            .css({'opacity': 0, 'display': 'block'})
            .animate({ opacity: 1 }, TAB_ANIMATE_DURATION, function () {
              currentTarget.removeClass(activePanelClassName);
              target.addClass(activePanelClassName);
            });
        });

      item.siblings().removeClass(activeTabClassName);
      item.addClass(activeTabClassName);
    });

    $('.post-toc a').on('click', function (e) {
      e.preventDefault();
      var targetSelector = escapeSelector(this.getAttribute('href'));
      var offset = $(targetSelector).offset().top;
      hasVelocity ?
        html.velocity('stop').velocity('scroll', {
          offset: offset  + 'px',
          mobileHA: false
        }) :
        $('html, body').stop().animate({
          scrollTop: offset
        }, 500);
    });

    // Expand sidebar on post detail page by default, when post has a toc.
    motionMiddleWares.sidebar = function () {
      var $tocContent = $('.post-toc-content');
      if (CONFIG.sidebar === 'post') {
        if ($tocContent.length > 0 && $tocContent.html().trim().length > 0) {
          displaySidebar();
        }
      }
    };
  });
</script>



  <script type="text/javascript" src="/js/bootstrap.js"></script>

  
  

  
  

</body>
</html>
